# 1. OVERVIEW

- Apache Spark is an **open-source** cluster computer framework for **real-time data processing**(meaning that there is no need to store the data in harddisk)
- is 100 times faster than hadoop, because Spark processes data in-memory while Hadoop have to write and read data with HDFS(Hadoop Distributed file system)
- Spark has comfortable APIs for Java, Scala and Python. etc, and powers a stack of libraries including SQL and DataFrames, MLlib(Machine Learning),GraphX, and Spark Streaming.

- Spark overcomes the time lag issue,which means that you can process data the day you input data. 

## 1.1 Spark cluster manager

- local：Local operation mode, non-distributed
- Standalone: using Spark's built-in clcuster manager,which can only run Spark tasks
- Yarn: 
- Mesos

# 2. Spark RDD(Resilient Distributed Data)

RDD , a fundamental data structure of Spark which is an immutable distributed collection of objects, is the key concept in Spark.

Each dataset in RDD is divided into **logical partition** to be computed on different nodes of the cluster, and can be cached in memory, which improves the calculation performance.

## 2.1 Operation on RDDs

- Creating new RDDs
- Transforming existing RDDs to new RDDs(Transformation)   is Lazy operation because it will not be executed immediately
- calling operations on RDDs to compute a result(Action)

each RDD is split into multiple partition, which can be computed on different nodes of the cluster

**Transformation**

lazy operation

when apply the transformation on any RDD it will not perform the operation. It will create a DAG(Directed Acyclic Graph)

# 3. Spark API

## 3.1 Spark Architecture Design

- Application: spark application written by the user, one Application contains multiple Jobs
- Job: a Job contains multiple RDDs

Application --> Jobs --> Stages --> Tasks

Stage is the basic unit 

# 4. 在Python中的使用

## 4.1 Creating RDDs

RDDs can be created by **loading an external file**(from local or HDFS)

1. import Spark library

   ```python
   from pyspark import SparkConf, SparkContext
   ```

2. configue the environment

   ```python
   appName = "demo_rdd_creation"
   conf = SparkConf().setAppName(appName)
   sc = SparkContext(conf=conf)
   ```

3. Read file into RDD from HDFS

   ```python
   rdd = sc.textFile("hdfs://(filepath)")
   ```

RDDs can be created by a **parallelize a collection of data**

```python
data = ['tom', 'mike', 'helen', 'mary']
rdd1 = sc.parallelize(data)
```

## 4.2 Action on RDDs

Actions compute a result based on an RDD, and either return it to the driver program or save it to an external storage system such as HDFS

the example rdd we use is below

`rdd = sc.parallelize(range(10), 5)`  rdd = {0,1,2,3,4,5,6,7,8,9}

- collect() : returns the RDD as a Python list

  ```python
  all_data = rdd.collect()
  #all_data = {0,1,2,3,4,5,6,7,8,9}
  ```

- take(n) : return the first n element in RDDs

  ```python
  part_data = rdd.take(4)
  # part_data = {0,1,2,3}
  ```

- first() : return the first element in RDD

  ```python
  first_data = rdd.first()
  # first_data = 0
  ```

- count(): return the number of element in RDD

  ```python
  count = rdd.count()
  # count = 10
  ```

- reduce(function) : 

## 4.3 Transformation

