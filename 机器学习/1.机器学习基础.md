#  1. 数据操作

## 1.1 数组

### 1.1.1 N维数组样例

- 0-d 标量

  0维数组就是一个数字

  用来表示一个类别

- 1-d 向量

  就是一个一维数组

  用来表示一个特征向量

- 2-d 矩阵

  二维数组

  表示一个样本——特征矩阵

- 3-d

  三维数组

  可以用于表示RGB图片（宽×高×通道）

- 4-d

  四维数组

  可以用于表示一个RGB图片批量（即多张图片）

- 5-d

  五维数组

  可以用于表示一个视频批量（批量大小×时间×宽×高×通道）

### 1.1.2 创建数组

创建数组需要以下信息

- 形状

  比如3x4的二维数组

- 每个元素的数据类型

  比如32位浮点数

- 每个元素的值

  比如全是0，或者服从正态分布的随机数

### 1.1.3 访问数组元素

以下操作只能在torch数据ndArray中实现

比如一个四行四列的二维数组

```python
1  2  3  4
5  6  7  8
9 10 11 12
13 14 15 16
```

- 访问一个元素

  [1,2] —— 7

- 访问一行

  [1, :] —— [5,6,7,8]

- 访问一列

  [ : , 1] —— [2,6,10,14]的转置

- 访问连续子区域

  [1:3, 1:] 

  ```python
  6  7  8
  10 11 12
  ```

- 访问分散子区域

  每隔3个选一个数据
  
  ```python
  [::3, ::2]
  1   3
  13  15
  ```
  
  指定下标选取一些数据
  
  ```python
  [[0, 2], [1, 2]] # 选取0，2行 中的 1，2列
   2   3
  10  11
  
  [[0, 2]] # 选取第0行和第2行
  [[0, 2], :] # 与这种写法等价
  1  2  3  4
  9 10 11 12
  ```

## 1.2 数据操作实现

### 1.2.1 创建张量

张量是数学中的概念，在计算机中对张量和数组没有区分。

表示一个数值组成的数组，这个数组可能有多个维度。

**指定填充元素创建张量**

- **arange** ：从0开始依次往下填充

  只能创建一维张量，可以通过reshape来更改为多维张量

  ```python
  x = torch.arange(6, dtype=torch.float32)
  x
  Out: tensor([0,1,2,3,4,5])
  ```

- **zeros** ：全部用0填充

- **empty**：不赋初值（用垃圾值填充）

- **ones**：全部用1填充

- **randn**：用随机数填充

  上述函数可以直接在参数中指定 创建多维张量

  ```python
  x = torch.zeros(3, 4)
  x
  Out:tensor(
  [0, 0, 0, 0],
  [0, 0, 0, 0],
  [0, 0, 0, 0]
  )
  ```

**通过Python列表创建**

tensor

```python
torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])
```

### 1.2.2 张量的属性

- **reshape**：修改张量的形状（修改前后元素个数必须相同）

需要传入一个元组作为参数

```python
A = torch.arrange(12)
A = A.respape((3, 4))
```

- **shape** ：获取数组的形状

- **numel** ： 获取数组中元素的个数

### 1.2.3 张量运算

**标准运算**

可以对两个张量进行 `+ - * /  **` 运算，会将张量中的对应元素依次做对应运算

 ```python
 x = torch.tensor([1.0, 2, 4, 8])
 y = torch.tensor([2, 3, 2, 2])
 x ** y
 Out: tensor([ 1.,  8., 16., 64.]) # 分别计算 1^2 2^3 4^2 8^2
 ```

**逻辑运算**

\>  ==  < 

x == y ，将x和y中的每个对应位置的元素做比较，得到一个有True或False组成的张量

**张量连结cat**

```python
X = [[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.]]
Y = [[ 2.,  1.,  4.,  3.],
         [ 1.,  2.,  3.,  4.],
         [ 4.,  3.,  2.,  1.]]

torch.cat((X, Y), dim=0) # 在第0维合并，把行堆叠起来
tensor([[ 0.,  1.,  2.,  3.],
         [ 4.,  5.,  6.,  7.],
         [ 8.,  9., 10., 11.],
         [ 2.,  1.,  4.,  3.],
         [ 1.,  2.,  3.,  4.],
         [ 4.,  3.,  2.,  1.]]),

torch.cat((X, Y), dim=1) # 在第1维合并，把行拼接起来
tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],
         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],
         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])
```

**求和运算sum**

将张量中的所有元素求和，得到一个标量

### 1.2.4 广播机制

广播机制有两个要求：

1. 每个参与计算的Tensor至少有一个维度（即维度不能全是0）

   反例：

   ```python
   x = torch.zeros(0, 0)
   ```

2. 从两个Tensor的最后一个维度开始向前遍历，倒数对应的位置的维度需要满足以下三个条件的其中一个

   - 维度相等
   - 其中一个维度是1
   - 其中一个维度不存在

满足要求之后，会从两个Tensor的最后一个维度开始向前遍历，结果中倒数对应位置的维度增加到两个Tensor中的较大值（直接复制）

(当使用一个标量和一个张量做运算时，实际上是标量和矩阵中的每个元素分别做运算，这其实就是一种广播机制)

- 例1

```python
有两个二维张量：
a = torch.arange(3).reshape((3, 1))
b = torch.arange(2).reshape((1, 2))

[[0],
 [1],
 [2]]

[[0, 1]]

首先比较 1 和 2，满足要求其中一个维度是1，因此都扩大到2
然后比较 3 和 1，满足要求，因此都扩大到3
最终结果为 3 x 2
a + b
Out:tensor([[0, 1],
        [1, 2],
        [2, 3]])
```

- 例2

```python
x = torch.zeros(5, 3, 4, 1)
y = torch.zeros(3, 1, 1)

首先比较 1 和 1，相等满足要求
比较 1 和 4，其中一个是1满足要求，扩大到4
比较 3 和 3， 相等满足要求
比较 5 和 0， 其中一个维度不存在满足要求， 扩大到5

(x + y).shape
Out: torch.Size([5, 3, 4, 3])
```

- 例3

```python
x = torch.zeros(5, 3, 4, 1)
y = torch.zeros(3, 2, 1)

将上例中y的第1个维度换成2，不满足要求，会报错
The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 2

最后的non-singleton dimension的下标是：出现不符合要求的维度的下标，该下标是维度更多的那个数组，从前往后数，下标从0开始计算得到的（实际上是轴axis的下标，即x的第二个轴的维度不符合要求）
```

### 1.2.5 存取数据（数据预处理）

数据集一般存储在csv文件中

csv文件的特点是，数据之间以逗号，分割

**读取csv文件**

读取csv文件使用pandas库

```python
pip install pandas
import pandas as pd

data = pd.read_csv(data_file) # 参数是文件路径

# 如果最后一列是输出y，前面的所有列是输入x
inputs = data.iloc[:, 0:-1] # 取出前面的所有列
outputs = data.iloc[:, -1] # 取出最后一列

# 如果结果列是字符串，要映射为数值
species_mapping = {'setosa': 0, 'versicolor': 1, 'virginica': 2} # 创建一个字符串和数值的映射字典
outputs = outputs.map(species_mapping) # 映射

# 转化为tensor
X, y = torch.tensor(inputs.values), torch.tensor(outputs.values).reshape(-1, 1)
```

## 1.3 线性代数操作

### 1.3.1 创建矩阵

矩阵即二维数组，和上面创建张量的操作相同

比如创建5×4的矩阵

- arange

  ```python
  A = torch.arange(20).reshape((5, 4))
  ```

- zeros

  ```python
  A = torch.zeros(5, 4)
  ```

- ones

  ```python
  A = torch.ones(5, 4)
  ```

### 1.3.2 矩阵转置

```python
A_T = A.T
```

可以通过判断矩阵与其转置是否相同来判断是否为对阵矩阵

```python
A == A.T
```

### 1.3.3 矩阵乘法

**对应元素相乘**

称为哈达玛积，即对应位置元素直接相乘，在数学中使用并不多

两个矩阵直接使用 * 相乘是计算哈达玛积

**dot向量点积(内积)**

dot只能对一维向量做乘积，即每个元素相乘再相加，得到一个标量

```python
a = torch.tensor([1, 2, 3, 4])
b = torch.tensor([4, 3, 2, 1])
torch.dot(a, b)
Out:20
    
torch.sum(a * b) # 与上面的dot操作等价
```

**mv矩阵向量积**

即Ax，一个矩阵乘一个向量

```python
A = torch.ones(3, 4)
x = torch.tensor([1., 2., 3., 4.])

1 1 1 1   1
1 1 1 1   2
1 1 1 1   3
          4
torch.mv(A, x)
	即 每行依次与向量相乘相加
	10 10 10
```

**mm矩阵乘法**

矩阵叉乘，即对应行和列相乘再相加

```python
A = torch.arange(20, dtype=torch.float32).reshape((5, 4))
B = torch.ones((4, 3))
torch.mm(A, B).shape
Out: torch.Size([5, 3])
```

### 1.3.4 矩阵按维度求和

```python
a = torch.ones(3, 4)
a

    1 1 1 1
    1 1 1 1
    1 1 1 1
```

**直接求和**

所有元素求和，得到一个标量

```python
a.sum()
Out: 12
```

**axis指定轴求和**

指定哪个轴求和，在结果矩阵中哪个维度就会消失（下标从0开始）

```python
a.sum(axis=0).shape # 输出torch.Size([4]), 第0轴 3 会消失，相当于按列相加

    3 3 3 3

a.sum(axis=1).shape # 输出torch.Size([3])， 第1轴 4 会消失，相当于按行相加

	4
    4
    4
```

可以同时按多个轴进行求和

```python
a = torch.ones((2, 5, 4))
a.sum(axis=[0,2]).shape 
Outs: torch.Size([5])
    
    8 8 8 8 8
```

**keepdims**

将keepdims置为True时会在指定轴求和时保留相应的维度，对应的维度会变成1

```python
a.sum(axis=0, keepdims=True).shape 
Outs: torch.Size([1, 4])
    tensor([[3, 3, 3, 3]])
```

### 1.3.5 求平均值

平均值和求和类似，只不过求和之后再除以相应的元素个数即可。也有按维度的概念

```python
A = torch.arange(6).reshape(3, 2)

	0 1
    2 3
    4 5
```

**求平均值**

```python
A = torch.arange(6, dtype=torch.float32).reshape((3, 2))
A.mean(), A.sum() / A.numel()
Outs: (tensor(2.5000), tensor(2.5000))
```

**按维度求平均值**

```python
A.mean(axis=0), A.sum(axis=0) / A.shape[0] # 相当于按列求
Out: (tensor([2., 3.]), tensor([2., 3.]))
```

### 1.3.6 矩阵范数

L范数针对一维向量，F范数针对矩阵

```python
u = torch.tensor([-3, 4])
```

**L1范数**

向量中的每个元素的绝对值相加

```python
torch.abs(u).sum()
Outs: tensor(7.)
```

**L2范数**

向量中的元素的平方和再开根号

```python
torch.norm(u)
Outs:tensor(5.)
```

**F范数**

其实和L2范数相同，只不过是相对于矩阵而言的

矩阵的所有元素的平方和开根号（相当于把矩阵拉成一个一维向量再求L2范式）

```python
torch.norm(torch.ones((4, 9)))
Outs: tensor(6.)
```

## 1.4 矩阵求导

### 1.4.1 矩阵（向量）求导规则

![](./1/1.4.1向量求导规则.png)

y对x求导时

标量表示只有一个变量比如 y，向量表示有多个变量，比如[x1, x2, x3, x4]

- 如果y是标量，x也是标量，求导也得到一个标量
- 如果y是标量，x是向量，求导得到一个向量
- 如果y是向量，x是标量，求导得到一个向量
- 如果y是向量，x也是向量，求导得到一个矩阵



**y是标量x是向量 （标量关于向量求导）**

这个事实上求的是梯度

运算规则中注意，X本来是列向量，求导之后的结果是行向量

![](./1/1.4.1y标x向.png)

例子：

```python
假设 y = x1^2 + 2 * x2^2 （这是一个椭圆，进而延申为一组等高线）
则y对x求导为  [2x1, 4x2] （第一个位置对x1求导，第二个位置对x2求导）

求出的(2x1, 4x2)就是梯度方向（即变化最大的方向）
比如在点（1，1），方向（2，4）就是梯度，跟等高线正交
```



**y是向量x是标量 （向量关于标量求导）**

y是列向量，求导之后的结果还是列向量。求导就是y1，y2... ym分别对x求导

![](./1/1.4.1y向x标.png)



**x是向量y是向量  （向量关于向量求导）**

相当于把y拆成m个标量，分别与x向量求导，每次求导都会求出一个行向量，最终求出m个行向量组成矩阵

y是m维，x是n维，结果是m×n

![](./1/1.4.1y向x向.png)

### 1.4.2 向量链式法则

![](./1/1.4.2向量链式法则.png)

记住向量求导之后结果的形状，就能大概猜出答案



**例子**

![](./1/1.4.2例1.png)

x,w是n维向量，y是标量。其中<x,w> 是x和w的内积（点积），就等于x.T × w

可以通过设中间变量a、b、z，再借用链式求导法则

最后 <x,w>对w求导，就是 x.T × w对w求导，结果是x.T。属于标量对向量求导

### 1.4.3 自动求导

自动求导是计算一个函数在指定值上的导数

有别于符号求导和数值求导

**计算图**

计算图是指将代码分解成操作子，进而将计算表示成一个无环图

![](./1/1.4.3计算图.png)

计算图可以显示构造也可以隐式构造

- 显示构造

  Tensorflow/Theano/MXNet

- 隐式构造

  PyTorch/MXNet



**自动求导**

有了计算图之后，就可以根据这些操作子来使用链式法则一步一步求导

![](./1/1.4.3自动求导.png)

根据求导顺序的不同又可以分为正向累积和反向累积（反向传递）

### 1.4.4 自动求导代码

以对`y = 2*(x.T)*x`求导，不难看出y是一个标量

1. 在计算y关于x的梯度之前，需要一个地方来存储梯度

   ```python
   x = torch.arange(4.0) # tensor([0., 1., 2., 3.])
   x.requires_grad_(True) 
   x.grad # 计算出关于x的梯度之后就存在这个地方，默认值是None
   
   # 也可以用一行代码搞定
   x = torch.arange(4.0, requires_grad=True)
   ```

2. 计算y

   ```python
   y = 2 * torch.dot(x, x) # dot运算就是xTx运算
   y # y是一个标量
   Out: tensor(28., grad_fn=<MulBackward0>) # grad_fn是求梯度的函数，也就是隐式的计算图
   ```

3. 调用反向传播函数自动计算y关于x每个分量的梯度

   ```python
   y.backward() # 求导
   x.grad # 梯度的结果存在这里
   Out: tensor([ 0.,  4.,  8., 12.])
   ```

   ```
   计算过程：
   y = 2 * x^2,所以求导之后就是 y‘ = x.grad = 4 * x
   而x = [0, 1, 2, 3]
   所以带入之后就是 x.grad = [0, 4, 8, 12]
   
   x.grad == 4 * x
   Out: tensor([True, True, True, True])
   ```

4. 计算另一个关于x的函数的梯度

   在默认情况下，PyTorch会累积梯度，所以计算下一个之前要先清除之前的值

   ```python
   x.grad.zero_() # 清除梯度中保存的内容
   y = x.sum() # 此时y=x1 + x2 + ... + xn  y=tensor(6., grad_fn=<SumBackward0>)
   y.backward()
   x.grad
   Out: tensor([1., 1., 1., 1.])
       
   由于y=x1 + x2 + x3 ... + xn，所以y关于任何一个x求导都是1
   ```

**求偏导**

将某些计算移动到记录的计算图之外，我理解的就是用于求偏导

```python
x = torch.arange(4.0, requires_grad=True)
y = x * x # 这里y是关于x的一个向量，是x的平方
u = y.detach() # 这里u本来也应该是关于x的向量，但是使用detach之后可以让系统将u视为和x没关系的一个变量
z = u * x # 由于u已经和x没关系了，因此在z中u是一个常数（系数）

z.sum().backward() # z.sum() 就是 z = u*x1 + u*x2 + u*x3 + ... + u*xn
# 在求导之后就是 [u, u, u..., u]
x.grad == u 
Out: tensor([True, True, True, True])
    
x.grad # 由于u=x*x，因此将x=[0, 1, 2, 3]代入就是 [u, u, u, u] = [0, 1, 4, 9]
Out: tensor([0, 1, 4, 9])
```

# 2. 线性回归

线性回归是对n维输入的加权，外加偏差。

使用平方损失来衡量预测值和真实值的差异。

## 2.1 理论

### 2.1.1 实际案例

假设一个简化模型：房价预测

假设影响房价的因素有：卧室个数 x1， 卫生间个数 x2， 居住面积 x3

则成交价是关键因素的加权和 `y = w1x1 + w2x2 + w3x3 + b`

其中`W = [w1, w2, w3]`是权重，b是偏差，权重和偏差将在后面决定

### 2.1.2 线性模型

给定n维输入 `X = [x1, x2, ..., xn].T`

线性模型有一个n维权重 `W = [w1, w2, ..., wn].T` 和一个标量偏差 b

则输出是输入的加权和 `y = w1*x1 + w2*x2 + ... + wn*xn + b`，向量版本就是 `<W, X> + b` 即W和X的内积 加 标量b

线性模型可以看作是单层神经网络

### 2.1.3 衡量预估质量（损失）

通过房屋的真实值y 和 模型的估计值y.hat 来计算损失

下面是一种计算损失的方法，称为平方损失

![](./1/2.1平方损失.png)

### 2.1.4 参数学习（损失函数）

参数学习就是找到参数 W和b，使得损失最小，其中W是一个向量，b是一个标量

定义训练损失：

1. 由线性模型可知，对于i号房子，有 `yi.hat = <xi, W> + b`

   其中yi.hat是一个标量，表示对i号房子的预测价格

   xi 是一个向量，表示i号房子的 卧室个数、卫生间个数、居住面积组成的向量 `[xi1, xi2, xi3]`

2. 代入上面的平方损失公式，得到 i 号房子的房价损失模型为

   `1/2 * (yi - <xi, W> - b)^2`

3. 再将每个房子的平方损失 求平均值

   就是所有平方损失求和再除以n

   X是矩阵，xi是向量，w是向量

   ![](./1/2.1平方损失公式1.png)

4. 注意到后面求和的部分可以写作L2范数

   ![](./1/2.1平方损失公式2.png)即为最终的训练损失函数

机器学习的目标即为确定 W和b 的值，来使得训练损失达到最小

### 2.1.5 显示解

由于线性模型是凸函数，因此有显示解（在后面的模型中大都没有显示解）

显示解就是梯度为0的地方，即求导之后令导数为0

![](./1/2.1.5显示解.png)

## 2.2 优化算法

当没有显示解时，只能通过优化算法来求得最优解

这里先只讲基础优化算法，最简单的优化算法为——梯度下降

### 2.2.1 梯度下降

先随便选一个w0，然后通过不断迭代来获取最优解

![](./1/2.2.1梯度下降.png)

每一次的wt 等于 上一次迭代的结果 wt-1 减去 （η 乘  损失函数 L 关于 wt-1处的梯度）。因为梯度是下降最快的方向，所以每次都沿着下降最快的方向下降

![](./1/2.2.1梯度下降图.png)

其中η是一个标量，表示学习率，即步长，也就是说每次在 （损失函数L关于wt-1处的梯度）的方向走多远

η是一个超参数，也就是说这个值需要人为指定

**学习率的选择**

- 学习率的选择不能太小

  因为这样会导致计算梯度的次数过多。计算梯度在机器学习中是最消耗资源的部分

- 不能太大

  如果太大的话会导致震荡，而不是每次都在下降

![](./1/2.2.1学习率选取.png)

### 2.2.2 小批量随机梯度下降

之前的损失函数 L 是基于所有数据（即整个训练集）得到的，由于在整个训练集上算梯度太贵了，所以我们可以从全部样本中采样几个样本，来近似训练损失。

采样的样本数b称为批量大小，也是一个超参数 

从全部样本 `[i1, i2, i3, ..., in]` 中采样 b个 `[i1, i2, ..., ib]` ,来近似损失

![](./1/2.2.2近似损失.png)

小批量随机梯度下降是深度学习默认的求解算法

其中有两个重要的超参数 学习率η 和 批量大小 b

## 2.3 从0实现算法

### 2.3.0 基本步骤

1. 根据现在的w和b，代入线性模型 求出预测值y_hat

   `y_hat = <X, w> + b`

2. 计算 计算出的y_hat 与 真实值y 的区别（即损失）

   <img src="./1/2.1平方损失.png" style="zoom:33%;" />

3. 损失函数 l 对w和b求梯度

4. 使用梯度下降算法更新参数w和b，完成一次迭代

   ![](./1/2.2.1梯度下降.png)

```python
lr = 0.03 # 指定学习步长，即η
num_epochs = 20 # 指定学习轮数
batch_size = 10 # 指定批量大小

# 随便指定一个模型参数
true_w = torch.tensor([-2, 3.1])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000) # 生成人工数据集

# 随机初始化一个w和b的值
w = torch.normal(0, 0.01, size=true_w.reshape(-1, 1).shape, requires_grad=True)
b = torch.zeros(1, requires_grad=True)

for _ in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        y_hat = torch.mm(X, w) + b # 1. 根据线性模型计算 y_hat
        l = (y - y_hat) ** 2 / 2 / batch_size # 2. 计算损失函数
        l.sum().backward() # 3. 损失函数对w求梯度
        with torch.no_grad(): # 4. 更新w和b的值
            w -= lr * w.grad
            b -= lr * b.grad
            w.grad.zero_()
            b.grad.zero_()
    with torch.no_grad():
        print(f'w:{w}, w的估计误差：{true_w - w.reshape(true_w.shape)}')
        print(f'b:{b}, b的估计误差{true_b - b}')
        print()
```

### 2.3.1 构造数据集

构造一个带有噪声的人造数据集

即通过正态分布生成的随机 x 生成一个数据集

再认为指定w和b，引入噪声ε，通过 `y = Xw + b + ε`  计算出y，得到一个数据集

```python
# w = [2, -3.4] b = 4.2
def synthetic_data(w, b, num_examples) : # 指定w、b和样本数量num_examples生成数据
    # 生成服从均值为0，方差为1的正态分布的随机x值 
    # X矩阵中 每行有len(w)个数据，代表一个样本中有len(w)个数据；
    # 共 num_examples行，表示生成num_examples个样本
    X = torch.normal(0, 1, (num_examples, len(w))) 
    y = torch.matmul(X, w) + b # 生成y ，y = Xw + b
    y += torch.normal(0, 0.01, y.shape) # 给生成的y添加噪声 噪声是服从（0，1）的正态分布的随机数
    return X, y.reshape((-1, 1)) # 返回时将y变为列向量

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000) # 生成大小为1000的数据集
```

### 2.3.2 定义采样批量函数

该函数每次从所有数据集中随机抽取batch_size个数据

```python
def data_iter(batch_size, features, labels) : # features为X矩阵，labels为y向量
    num_examples = len(features)
    indices = list(range(num_examples)) # 给每个数据进行编号
    random.shuffle(indices) # 打乱所有下标，随机从中抽取数据
    
    for i in range(0, num_expamles, batch_size):
        # 这一步是 取出打乱后的下标数组中 [i:i+batch_size]部分，即随机取出batch_size个下标
        batch_indices = torch.tensor(indices[i:min(i + batch_size, num_examples)])
        # 从原有的数据集中，根据上面随机取出的batch_size个下标，随机抽取出batch_size个数据
        # 最后返回的是 (batch_size, len(w))维矩阵X 和 batch_size维向量y
        yield features[batch_indices], labels[batch_indices]
        

# 从数据集中随机取出10个数据，打印一组试试效果
for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
```

### 2.3.3 定义线性回归模型

```python
def linreg(X, w, b):
    # 输入X，w和b，返回预测值y_hat
    return torch.matmul(X, w) + b 
```

### 2.3.4 定义损失函数

使用均方损失

![](./1/2.1平方损失.png)

```python
def squared_loss(y_hat, y):
    # 在这里先除以n，后面直接相加就能求出最后的 损失l
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2 / batch_size
```

### 2.3.5 定义优化算法

使用小批量随机梯度下降

![](./1/2.2.1梯度下降.png)

```python
def sgd(params, lr, batch_size):  
    """小批量随机梯度下降。"""
    with torch.no_grad():
        for param in params:
            # 在调用sgd函数之前已经完成梯度计算了，所以可以直接调用param.grad，即为 l对wt-1的梯度
            param -= lr * param.grad # lr即为η
            param.grad.zero_()
```

### 2.3.6 训练过程

**定义训练过程中用到的函数，生成数据**

```python
lr = 0.03 # 指定学习步长
num_epochs = 3 # 指定学习轮数
net = linreg # 指定用到的模型，本次使用线性模型
loss = squared_loss # 指定计算损失的函数，本次使用均方损失

features, labels = synthetic_data(true_w, true_b, 1000) # 人工构造此次使用的数据集
# 随机初始化w和b，迭代过程中会不断计算二者梯度，所以requires_grad都置为True
w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

**训练过程**

所有模型的训练过程都大致相同

假设批量大小为b

1. 根据训练模型计算y_hat

   y_hat是大小为b的向量，因为对于每个xi 都能 计算出一个 y_hat_i

   由于初始化w 和 b时都指定了 requires_grad=True，所以此时y_hat中保存了关于w和b的计算图`y_hat = <X, w> + b`，且y_hat的requires_grad为True 

2. 根据损失函数计算本次使用 w 和 b 造成的损失 l

   损失 l 此时是 大小为b的向量，因为对于每一组 xi 和 yi 计算出一个损失 l_i

   l_i 由 y_hat_i 计算而来，因此 l 中保存了关于y_hat的计算图`l_i = 1/2 * (y_hat_i - y_i) ^ 2`，进一步即为关于w和b的计算图 

3. 计算真正的损失值 l 

   上一步计算出了所有的损失值，得到了向量 l ，现在需要求均值，得到真正的标量  l  

   因此我们进行 l.sum()操作，计算出总和

   ![](./1/2.1平方损失公式1.png)

```python
for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        y_hat = net(X, w, b) # y_hat此时是一个向量
        l = loss(y_hat, y) # l此时是一个向量
        l.sum().backward() # l.sum()变成标量，即最终要求的损失函数，再对w-1求梯度
        sgd([w, b], lr, batch_size) # 求完梯度之后，将wt-1更新为wt
    with torch.no_grad(): # 这句话是说下面的计算无需计算梯度，因此也不用保存计算图
        # 查看以下这轮计算完之后 均方差 d
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

## 2.4 调库实现算法

```python
import numpy as np
import torch
from torch.utils import data # 机器学习用到的东西都在这个包里
from torch import nn # 这个包中有定义好的层（模型）
from d2l import torch as d2l # 这是之前自己写的函数，封装到这个包里

# 生成数据
true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

# 调用API来分批读取数据
def load_array(data_arrays, batch_size, is_train=True):  
    """构造一个PyTorch数据迭代器。"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)

# 使用框架中预定义好的层
net = nn.Sequential(nn.Linear(2, 1)) # 这里实际上是一个list，可以传入多个模型表示神经网络的多层结构
# 这里只有一层线性层，创建层需要的参数是 w和b的维度

# 初始化w和b为随机值
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

# 计算均方误差的函数
loss = nn.MSELoss()

# 优化算法（小批量随机梯度下降）
trainer = torch.optim.SGD(net.parameters(), lr=0.03)

# 训练过程
num_epochs =  3
for _ in range(num_epochs):
    for X, y in data_iter:
        y_hat = net(X)
        l = loss(net(X), y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
        
    w = net[0].weight.data
    print(f'w的误差：{true_w - w.reshape(true_w.shape)}')
    b = net[0].bias.data
    print(f'b的误差：{}')
```

# 3. softmax分类

## 3.1 模型介绍

softmax是一个分类模型，可以用于比如识别一个图片是猫还是狗，识别手写的0~9的数字（十类问题）

### 3.1.1 分类模型

**分类和回归的区别**

回归：

- 单连续数值输出
- 输出的区间是自然区间R
- 用输出数值和真实值的区别作为损失

分类：

- 通常多个输出（要分为几个类就有几个输出）
- 每个输出表示：当前样例属于这一类的置信度有多少（概率有多大）

### 3.1.2 使用回归模型解决分类问题

**实现思路**

1. 首先我们对类别进行一位有效编码（one-hot encoding）

   一位有效编码是一个向量，向量大小和分类的类别一样多。类别对应的分量设置为1，其他所有分量均设置为0

   ```
   比如我们使用一个2x2的灰度图像表示一个动物，这个动物只有三种：猫、狗、鸡
   
   那么可以使用 
   [1,0,0] —— 猫
   [0,1,0] —— 鸡
   [0,0,1] —— 狗
   ```

2. 定义一个和输出一样多的仿射函数（多维到多维的映射）

   ![](./1/3.1.2softmax回归.png)

   ```
   这里我们输入的是一个2x2灰度图像，也就是说有4个输入。对应三个输出，因为有三个类别
   
   o1 = x1w11 + x2w12 + x3w13 + x4w14 + b1
   o2 = x1w21 + x2w22 + x3w23 + x4w24 + b2
   o3 = x1w31 + x2w32 + x3w33 + x4w34 + b3
   
   这样的话我们输入 [x1, x2, x3, x4] ，就应该会得到一个 [o1, o2, o3]
   ```

3. 使用线性回归的均方损失来训练模型

   最终得到的结果为，输入一个[x1, x2, x3, x4]得到一个[o1, o2, o3] 

   我们根据o1,o2,o3的值中最大的来预测这张图片是哪个动物的概率最大

### 3.1.3 回归模型优化为softmax

注意，优化完之后使用的依然是线性回归的思路

- 现在我们得到的是一个向量，但是我们更希望得到的是一个概率向量，里面的每个值是样本属于该类的概率（要求就是向量中所有值都大于零，且相加为1）

  可以通过softmax函数来把一个向量转化为一个概率向量（这个概率依然是一个向量）

  ![](./1/softmax函数.png)

- 实际上在分类问题中，我们并不关心这个样例属于每个类别的概率oi，我们更需要这个样例属于某个类别的概率显著高于其他所有类别，这样的话才能进行有效分类

  可以使用交叉熵损失来代替均方损失，交叉熵损失可以用来衡量两个概率向量的区别

  ![](./1/3.1.2交叉熵损失.png)

### 3.1.4 交叉熵损失

交叉熵用于衡量两个两个概率向量的区别

**基本公式**

![](./1/3.1.4交叉熵.png)

p和q都是概率向量，对于p和q中的每个元素，我们计算 `- pi * log(qi)` ，最后将所有的结果求和

**作为损失函数**

![](./1/3.1.2交叉熵损失函数.png)

损失函数就是实际概率向量 y（向量中只有真实种类对应的1，其他都是0） 和 预测概率向量y_hat （样本是每个种类的概率） 的差距，就直接将 y 和 y_hat代入H(y, y_hat)，即可得到第二个等号

注意到实际概率向量y 实际上只有一个1，其他都是0。所以对于预测概率向量y_hat来说，对于非正确种类的概率的预测乘完都是0，也就是说我们不关心对于非正确类的预测值，最终结果就是 对于正确种类的预测概率 求log再取负数

**梯度**

上述损失函数的梯度是真实概率和预测概率的差值

![](./1/3.1.4梯度.png)

求导过程：

![](./1/3.1.4梯度过程.png)

## 3.2 模型从0实现

### 3.2.0 读取图片分类数据集

此次使用的数据集为 Fashion-MNIST数据集，里面是一些不同衣服种类的黑白图片

1. 下载数据集

   ```python
   trans = transforms.ToTensor() # 将读入的图片转化为一个Tensor（保存了每个像素信息的矩阵）
   mnist_train = torchvision.datasets.FashionMNIST(root="../data", train=True,
                                              transform=trans, download=False)
   mnist_test = torchvision.datasets.FashionMNIST(root="../data", train=False,
                                                transform=trans, download=False)
   
   len(mnist_train), len(mnist_test)
   Out: (60000, 10000) # 训练数据集有6w张图片，验证数据集有1w张图片
       
   mnist_train[0][0].data # 从训练数据集的第0个example中取出第一个图像，查看形状
   Out: torch.Size([1, 28, 28]) 
       # 1表示通道数，由于是黑白图片所有只有一个通道
       # 32x32表示图片大小，32x32像素
   ```
   
2. 放入DataLoader中，来按批量获取

   ```python
   from torch.utils import data
   
   batch_size = 256
   
   # 最后的参数num_workers表示使用的线程数
   train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True, num_workers=1)
   test_iter = data.DataLoader(mnist_test, batch_size, shuffle=True, num_workers)
   
   X, y = next(train_iter) # 取一组数据出来
   ```

3. 显示衣服图像和对应的衣服类别标签

   ```python 
   def get_fashion_mnist_labels(labels):  
       """返回Fashion-MNIST数据集的文本标签。"""
       text_labels = [
           't-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt',
           'sneaker', 'bag', 'ankle boot']
       return [text_labels[int(i)] for i in labels]
   
   def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  
       """Plot a list of images."""
       figsize = (num_cols * scale, num_rows * scale)
       _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
       axes = axes.flatten()
       for i, (ax, img) in enumerate(zip(axes, imgs)):
           if torch.is_tensor(img):
               ax.imshow(img.numpy())
           else:
               ax.imshow(img)
           ax.axes.get_xaxis().set_visible(False)
           ax.axes.get_yaxis().set_visible(False)
           if titles:
               ax.set_title(titles[i])
       return axes
   
   X, y = next(iter(data.DataLoader(mnist_train, batch_size=9)))
   show_images(X.reshape(9, 28, 28), 1, 9, titles=get_fashion_mnist_labels(y))
   ```

### 3.2.1 定义softmax函数

函数参数是一个X矩阵，我们对其中的每一行分别做softmax操作

![](./1/3.2.1softmax函数.png)

```python
def softmax(X):
    X_exp = torch.exp(X) # 对X矩阵中每一个元素做指数操作，即将元素Xij变为e^Xij。X是一个矩阵
    partition = X_exp.sum(1, keepdim=True) # 对每一行分别进行求和，得到分母，partition是一个矩阵，不过每行只有一列
    return X_exp / partition # 将每一个元素除以对应行的和，即为softmax的结果。这里用到了广播机制
```

### 3.2.2 定义回归模型

**首先定义参数W和b**

```python
num_inputs = 32*32 # 由于softmax分类回归需要的输入是一个向量，因此要把图片像素矩阵拉伸为一个向量
num_outputs = 10 # 输出的个数就是分类的种类数

# 初始化W和b
W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)
```

**定义回归模型**

这里的回归模型指这个

```
o1 = x1w11 + x2w12 + x3w13 + x4w14 + b1
o2 = x1w21 + x2w22 + x3w23 + x4w24 + b2
o3 = x1w31 + x2w32 + x3w33 + x4w34 + b3
```

```python
def net(X): # X是一个四维矩阵，即batch_size个三维向量（一张图片对应一个[1,32,32]的三维向量）
    X_reshape = X.reshape(batch_size, W.shape[0]) # 对输入矩阵进行拉伸，一次应该读取一个批量大小的图片，再将每张图片拉伸为一维向量
    O = torch.matmul(X_reshape, W) + b # X和W做矩阵乘法再加b，计算出O矩阵
    # 这里X_reshape形状为(batch_size * num_inputs)，W形状为(num_inputs * num_outputs)所以最终乘完之后结果为(batch_size * num_outputs)
    
    return softmax(O) # 对O矩阵做softmax，将结果转换为概率矩阵
```

### 3.2.3 定义损失函数（交叉熵损失）

![](./1/3.1.2交叉熵损失函数.png)

我们只需要把每个预测的概率向量中正确分类对应的分量取出，然后取对数再取负数即可

**注意：** 这里只是一个样例的损失值，对于整个模型来说，最后还要把所有样例的损失相加求平均值

```python
def cross_entrypy(y_hat, y):
    return -torch.log(y_hat[range(len(y_hat)), y])
```

y_hat是对于每个样例预测的概率向量，是一个二维矩阵，y是一个保存了每个样例的实际分类的一维向量。

**代码解释**

```python
y = torch.tensor([1, 2])
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y_hat[range(len(y_hat)), y] # 这个操作就表示，分别从y_hat[i]中取出y[i]下标的元素，即y_hat[i][y[i]]
Out:tensor([0.3000, 0.5000])

# 比如：从y_hat[0]中取出y[0]（即1）下标的元素，即y_hat[0][1]
# 从y_hat[1]中取出y[1]（即2）下标的元素，即y_hat[1][2]
```

### 3.2.4 训练过程

**精度评估函数**

```python
# 创建精度评估函数
def accuracy(y_hat, y):
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1) # 取概率向量中的最大值，即为对当前样本种类的预测结果
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum()) # 返回这一批样例中预测正确的数量
```

**训练过程**

```python
train_epochs = 3 # 训练轮数
lr = 0.1 # 学习率

num_inputs = mnist_train[0][0].shape[1] * mnist_train[0][0].shape[2] # 输入的数据形状
num_outputs = 10 # 输出数据形状，即结果总共有几个分类

# 随机初始化参数W 和 b
W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)

for _ in range(train_epochs):
    for X,y in train_iter:
        y_hat = net(X) # 计算预测向量，并使用softmax处理为概率向量
        # print(y_hat.sum(axis=1))
        l = cross_entrypy(y_hat, y) # 计算交叉熵损失
        l.sum().backward() # 计算梯度
        with torch.no_grad(): # 迭代
            W -= lr * W.grad / batch_size # 这里除以batch_size是为了求损失的平均值
            W.grad.zero_()
            b -= lr * b.grad / batch_size
            b.grad.zero_()
        with torch.no_grad(): # 打印本次训练结果的正确率
            print(accuracy(y_hat, y) / y.numel())
```

### 3.2.5 使用训练好的模型进行预测

```python
def predict_ch3(net, test_iter, n=10):  
    for X, y in test_iter:
        break
    trues = get_fashion_mnist_labels(y)
    preds = get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true + '\n' + pred for true, pred in zip(trues, preds)]
    show_images(X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])

predict_ch3(net, test_iter)
```
