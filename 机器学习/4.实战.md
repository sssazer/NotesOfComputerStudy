#  0. è®­ç»ƒæ³¨æ„äº‹é¡¹

## 0.1 æœåŠ¡å™¨ç½‘ç»œç¯å¢ƒ

ğŸ¤—åº“ä½¿ç”¨æ—¶éœ€è¦è¿æ¥huggingfaceæœåŠ¡å™¨åœ¨çº¿è¿è¡Œï¼ˆä¹Ÿå¯ä»¥æŠŠè¦ç”¨åˆ°çš„æ¨¡å‹æå‰åŠ è½½å¥½ä¿å­˜åœ¨æœ¬åœ°ï¼‰

huggingfaceæœåŠ¡å™¨åœ¨å¤–ç½‘ï¼Œè€Œè®­ç»ƒç”¨çš„æœåŠ¡å™¨å¯èƒ½æ— æ³•è¿æ¥è‡³å¤–ç½‘ï¼Œæ­¤æ—¶å¯ä»¥è®©è¿œç¨‹æœåŠ¡å™¨ä½¿ç”¨æœ¬æœºä»£ç†ï¼ˆä»…é™æ ¡å›­ç½‘ï¼‰

1. é¦–å…ˆåœ¨æœ¬æœºClash for Windowsä¸­æ‰“å¼€ Allow LAN

2. ä½¿ç”¨ifconfigå‘½ä»¤ï¼ˆwindowsä¸ºipconfigï¼‰æŸ¥çœ‹æœ¬æœºipåœ°å€

3. ä¹‹åå°±å¯ä»¥åœ¨ä½¿ç”¨å‘½ä»¤æ—¶æ·»åŠ ä»£ç†

   ```shell
   pip install xxx --proxy=ip:7890
   ```

4. ä¹Ÿå¯ä»¥è®¾ç½®å…¨å±€ä»£ç†

   åœ¨å½“å‰terminalä¸­è®¾ç½®å…¨å±€ä»£ç†

   ```shell
   export all_proxy="ip:7890"
   ```

## 0.2 åå°è¿è¡Œä»»åŠ¡



# 1. PyTorchä½¿ç”¨

å¯ä»¥é€šè¿‡`dir()`æ¥æŸ¥çœ‹pythonçš„æŸä¸ªåº“çš„ç»„æˆï¼Œæˆ–è€…è¯´åˆ—ä¸¾æŸä¸ªåº“çš„æ‰€æœ‰å­åº“

é€šè¿‡`help()`æ¥æŸ¥çœ‹æŸä¸ªåº“çš„ä½¿ç”¨å¸®åŠ©

```python
dir(torch)
dir(torch.cuda)
help(torch.cuda)
```

## 1.1 æ“ä½œæ•°æ®

PyTorchä¸­æä¾›çš„æ•°æ®æ“ä½œç±»æœ‰ä¸¤ä¸ªï¼š

**Dataset**ï¼šç”¨äºè¯»å–æ•°æ®æ–‡ä»¶ï¼Œä»åŸå§‹æ•°æ®æ–‡ä»¶ä¸­è·å–æ•°æ®åŠå…¶label

**Dataloader**ï¼šå°†Datasetä¸­çš„æ•°æ®è¿›è¡Œæ‰“åŒ…åˆ†ç»„ç­‰æ“ä½œï¼Œä»¥æ–¹ä¾¿åœ°æä¾›ç»™åé¢çš„ç¥ç»ç½‘ç»œä½¿ç”¨

### 1.1.1 Dataset

è‡ªå®šä¹‰çš„åŠ è½½æ•°æ®é›†ç±»éƒ½éœ€è¦ç»§æ‰¿Datasetç±»

éœ€è¦é‡å†™çš„å‡½æ•°

- \__getitem__ï¼šè·å–æ•°æ®é›†ä¸­æŒ‡å®šä¸‹æ ‡indexçš„ä¸€æ¡æ•°æ®åŠå…¶å¯¹åº”label

  ```python
  def __getitem__(self, index):
   Â  Â ###
   Â  Â return data, label
  ```

- \__len__ï¼šè·å–æ•°æ®é›†å¤§å°

  ```python
  def __len__(self):
  ```

- \__add__ï¼šæ·»åŠ æ•°æ®

  ```python
  def __add__(self, )
  ```

### 1.1.2 Dataloader

DataLoaderç±»æ˜¯æ•°æ®è¯»å–å™¨ï¼Œæ¯æ¬¡è¯»å–æŒ‡å®šbatch_sizeå¤§å°çš„æ•°æ®ä½œä¸ºæ¨¡å‹è®­ç»ƒä¸€æ¬¡çš„è¾“å…¥ 

**åˆ›å»ºDataLoader**

```python
data_loader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=0, drop_last=True)
```

**ä½¿ç”¨DataLoader**

```python
for _ in range(epoch):
 Â  Â for data in data_loader:
 Â  Â  Â  Â datas, labels = data
```

ä½¿ç”¨forå¾ªç¯ä»data_loaderä¸­è¯»å–æ•°æ®ï¼Œæ¯æ¬¡å–å‡ºbatch_sizeæ¡æ•°æ®ï¼Œç›¸å½“äºè°ƒç”¨batch_sizeæ¬¡\__getitem__æ–¹æ³•

å–å‡ºçš„å››æ¡æ•°æ®å’Œæ ‡ç­¾ï¼Œä¼šå°†4æ¡æ•°æ®æœ¬èº«æ‰“åŒ…æˆæ•°ç»„ï¼Œä½œä¸ºdatasè¿”å›ï¼Œå°†4ä¸ªæ ‡ç­¾æ‰“åŒ…æˆä¸€ä¸ªæ•°ç»„ï¼Œä½œä¸ºlabelsè¿”å›ã€‚

æ¯æ¬¡epochéƒ½ä¼šå°†æ‰€æœ‰æ•°æ®è¯»å–ä¸€éåç»“æŸ

**å‚æ•°**

- datasetï¼šä»å“ªä¸ªdatasetå¯¹è±¡ä¸­è¯»å–æ•°æ®

- batch_sizeï¼šæ¯æ¬¡è¯»å–å‡ æ¡æ•°æ®
- shuffleï¼šæ¯ä¸ªepochä¸­ï¼Œæ¯ä¸ªbatch_sizeå¾—åˆ°çš„æ•°æ®æ˜¯å¦ç›¸åŒ

- drop_lastï¼šå½“æ•°æ®é›†ä¸­æœ€åå‰©ä¸‹çš„æ•°æ®ä¸è¶³batch_sizeæ—¶ï¼Œæ˜¯å¦ä¸¢æ‰

## 1.2 æ„å»ºæ¨¡å‹

æ¨¡å‹ç›¸å…³å†…å®¹åœ¨torch.nnåŒ…ä¸­

### 1.2.1 nn.Module

æ‰€æœ‰ç¥ç»ç½‘ç»œçš„base classï¼Œä¹Ÿå°±æ˜¯è¯´æ‰€æœ‰ç¥ç»ç½‘ç»œéƒ½è¦ç»§æ‰¿nn.Moduleç±»

åœ¨è‡ªå®šä¹‰Modelç±»ä¸­ï¼Œåº”è¯¥é‡å†™\__init__æ–¹æ³•å’Œforwardæ–¹æ³•

- \__init__ï¼šç”¨äºåˆå§‹åŒ–æ¨¡å‹å¯¹è±¡ï¼Œåº”è¯¥è°ƒç”¨`super().__init__`

- forwardï¼šç»™å®šæ¨¡å‹è¾“å…¥xï¼Œå¾—åˆ°æ¨¡å‹çš„è¾“å‡ºoutputs

```python
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
 Â  Â def __init__(self):
 Â  Â  Â  Â super().__init__()
 Â  Â  Â  Â self.conv1 = nn.Conv2d(1, 20, 5)
 Â  Â  Â  Â self.conv2 = nn.Conv2d(20, 20, 5)

 Â  Â def forward(self, x):
 Â  Â  Â  Â x = F.relu(self.conv1(x))
 Â  Â  Â  Â return F.relu(self.conv2(x))
 Â  Â 
# ä½¿ç”¨ï¼š
model = Model()
input = torch.ones((10, 3, 32, 32)) # åˆ›å»ºå¥½æ¨¡å‹ä¹‹åä¸€èˆ¬ä½¿ç”¨ä¸€ä¸ªè™šæ‹Ÿæ•°æ®æ¥æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®ï¼Œæ¯”å¦‚m10å¼  3é€šé“ 32x32 å¤§å°çš„å›¾ç‰‡
output = model(input)
```



### 1.2.2 ä¿®æ”¹å·²æœ‰æ¨¡å‹

ä¿®æ”¹æ¨¡å‹ç»“æ„ï¼Œæ¯”å¦‚æ·»åŠ æˆ–è€…åˆ é™¤ä¸€äº›å±‚ã€‚å¯ä»¥ä½¿ç”¨add_moduleä¿®æ”¹å·²æœ‰æ¨¡å‹

å…ˆç›´æ¥ä½¿ç”¨printæ‰“å°æ¨¡å‹ä¿¡æ¯ï¼Œç„¶ååœ¨æŒ‡å®šä½ç½®è¿›è¡Œæ·»åŠ æˆ–è€…ä¿®æ”¹

- æ·»åŠ ä¸€ä¸ªå…¨é“¾æ¥å±‚

```python
vgg_16 = torchvision.models.vgg16(pretrained=True)

# åœ¨vgg16æ¨¡å‹æœ€åæ·»åŠ ä¸€ä¸ªåä¸ºadd_linearçš„çº¿æ€§å±‚
vgg16.add_module('add_linear', nn.Linear(1000, 10))

# ä¹Ÿå¯ä»¥åœ¨æ¨¡å‹çš„æŸä¸ªæ¨¡å—æœ€ååŠ ä¸€ä¸ªçº¿æ€§å±‚
vgg_16.classifier.add_module('add_linear', nn.Linear(1000, 10)) # åœ¨classifieræ¨¡å—æœ€ååŠ ä¸€ä¸ªçº¿æ€§å±‚
```

- ä¿®æ”¹æŸä¸€å±‚

```python
vgg16.classifier[6] = nn.Linear(4096. 10) # ä¿®æ”¹æ¨¡å‹classifieræ¨¡å—ç¬¬7å±‚çš„å¤§å°
```



### 1.2.3 æ¨¡å‹ä¿å­˜

æœ‰ä¸¤ç§æ–¹å¼ä¿å­˜æ¨¡å‹

1. ä¿å­˜å®Œæ•´æ¨¡å‹+å‚æ•°

    modelæ˜¯å·²ç»åˆ›å»ºå¥½çš„æ¨¡å‹ï¼Œåé¢æ˜¯è¦ä¿å­˜çš„æ–‡ä»¶åç§°

    ```python
    torch.save(model, "model.pth")
    ```

    è¿™ç§æ–¹å¼ä¿å­˜å¯ä»¥ç›´æ¥é€šè¿‡æ–‡ä»¶è·å–æ¨¡å‹

    é™·é˜±ï¼šå…¶å®åŠ è½½æ—¶è¿˜éœ€è¦æœ‰æ¨¡å‹çš„å®šä¹‰ç±»ï¼Œåªæ˜¯ä¸éœ€è¦åˆ›å»ºæ¨¡å‹å¯¹è±¡äº†

    ```python
    model = torch.load("model.pth")
    ```

2. åªä¿å­˜æ¨¡å‹å‚æ•°ï¼ˆå®˜æ–¹æ¨èï¼‰

    å‚æ•°ä»¥å­—å…¸çš„å½¢å¼ä¿å­˜ï¼Œæ‰€ä»¥å«state_dict

    ```python
    torch.save(model.state_dict(), "model.pth")
    ```

    è¿™ç§æ–¹å¼åŠ è½½æ¨¡å‹éœ€è¦æå‰å‡†å¤‡å¥½æ¨¡å‹ç»“æ„ï¼ˆå³åˆ›å»ºå¥½æ¨¡å‹å¯¹è±¡ï¼‰ï¼Œç„¶åå†ä½¿ç”¨æ¨¡å‹å¯¹è±¡ä»æ–‡ä»¶ä¸­åŠ è½½å‚æ•°

    ```python
    model = my_model()
    model.load_state_dict(torch.load("model.pth"))
    ```



## 1.3 è®­ç»ƒè¿‡ç¨‹

è®­ç»ƒè¿‡ç¨‹

1. ä»æ•°æ®é›†ä¸­å–å‡ºä¸€æ¡æ•°æ®æ”¾å…¥æ¨¡å‹ä¸­å¾—åˆ°è¾“å‡º

2. æ ¹æ® æ¨¡å‹å¾—åˆ°çš„è¾“å‡º å’Œ æ ‡ç­¾ï¼ˆçœŸå®å€¼ï¼‰ï¼Œä½¿ç”¨æŸå¤±å‡½æ•°è®¡ç®—æŸå¤±

3. æ ¹æ®æŸå¤±è®¡ç®—åå‘ä¼ æ’­
4. ä½¿ç”¨ä¼˜åŒ–å™¨æ ¹æ®åå‘ä¼ æ’­ç»“æœæ›´æ–°æ¨¡å‹å‚æ•°

### 1.3.1 æŸå¤±å‡½æ•°åŠåå‘ä¼ æ’­

æŸå¤±å‡½æ•°ç±»å®šä¹‰åœ¨torch.nnåº“ä¸­

æŸå¤±å‡½æ•°ä¸­ä¸€èˆ¬ä¼ å…¥outputå’Œtargetä¸¤ä¸ªå‚æ•°ï¼Œæ¥è®¡ç®— è®¡ç®—å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„å·®è·ï¼Œä¸ºåå‘ä¼ æ’­çš„è®¡ç®—æä¾›ä¾æ®

```python
# åˆ›å»ºæŸå¤±å‡½æ•°
loss = nn.CrossEntropyLoss()
for data,label in dataloader: # å–å‡ºä¸€æ¡æ•°æ®åŠå…¶å¯¹åº”æ ‡ç­¾ï¼ˆç›®æ ‡ï¼‰
    outputs = model(data) # å°†æ•°æ®æ”¾å…¥æ¨¡å‹è®¡ç®—è¾“å‡º
    result_loss = loss(inputs, targets) # è®¡ç®—æŸå¤±
    result_loss.backward()
```

### 1.3.2 ä¼˜åŒ–å™¨

ä¼˜åŒ–å™¨åœ¨torch.optimåº“ä¸­

åœ¨è®¡ç®—å®ŒæŸå¤±å‡½æ•°ï¼Œå¹¶è®¡ç®—å®Œåå‘ä¼ æ’­å¾—åˆ°ä¼˜åŒ–æ–¹å‘ä¹‹åï¼Œå°±éœ€è¦è°ƒç”¨ä¼˜åŒ–å™¨å¯¹æ¨¡å‹å‚æ•°è¿›è¡Œä¼˜åŒ–

ä¼˜åŒ–å™¨éœ€è¦çš„å‚æ•°ï¼š

- paramsï¼šæ¨¡å‹å‚æ•°ï¼Œå‘Šè¯‰ä¼˜åŒ–å™¨æ›´æ–°ä»€ä¹ˆ
- lrï¼šå­¦ä¹ ç‡

- ä¸åŒä¼˜åŒ–å™¨ç‰¹å®šå‚æ•°

```python
optim = torch.optim.SGD(model.parameters(), lr=0.01) # æŒ‡å®šä¼˜åŒ–å™¨

for input, target in dataset:
    optimizer.zero_grad() # é˜²æ­¢ä¸Šä¸€è½®çš„æ¢¯åº¦å¯¹æœ¬è½®è®¡ç®—é€ æˆå½±å“
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optim.step()
```

### 1.3.3 æ¨¡å‹è¯„ä¼°

ä¸€èˆ¬åœ¨æ¯ä¸ªepochè®­ç»ƒç»“æŸä¹‹åï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®é›†å¯¹æ¨¡å‹è¿›è¡Œä¸€ä¸ªè¯„ä¼°ï¼Œæ¥å¾—åˆ°ä¸€ä¸ªæŸå¤±å€¼ï¼Œå¸®åŠ©åˆ¤æ–­æ¨¡å‹æ˜¯å¦åœ¨æ­£å¸¸è®­ç»ƒ

**æŸå¤±å€¼**

```python
total_test_loss = 0 # å°†æ•´ä¸ªæµ‹è¯•æ•°æ®é›†ä¸Šçš„æ‰€æœ‰lossç´¯åŠ èµ·æ¥
with torch.no_grad():
    for data,label in test_dataloader:
        outputs = model(data)
        loss = loss(outputs, label)
        total_test_loss = total_test_loss + loss.item()
print("lossï¼š{}".format(total_test_loss))
```

**å‡†ç¡®ç‡**

å¯¹äºåˆ†ç±»é—®é¢˜ç‰¹æœ‰çš„è¯„ä»·æŒ‡æ ‡

å‡†ç¡®ç‡ = é¢„æµ‹æ­£ç¡®çš„ä¸ªæ•° / è®­ç»ƒé›†æ€»æ•°æ®æ¡æ•°

```python
total_accuracy = 0
with torch.no_grad():
    for data, label in test_dataloader:
        outputs = model(data)
        outputs = outputs.argmax(1) # å–å‡ºé¢„æµ‹æ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªç±»åˆ«ä½œä¸ºæœ¬æ¬¡é¢„æµ‹çš„ç»“æœ
        # å‡†ç¡®ç‡ç­‰äºæ­£ç¡®çš„ç»“æœå’Œæ€»æ•°æ®æ¡æ•°çš„æ¯”å€¼
        accuracy = (outputs == target).sum()
        total_accuracy = total_accuracy + accuracy
    print("æ•´ä½“æµ‹è¯•é›†ä¸Šçš„æ­£ç¡®ç‡ï¼š{}".format(total_accuracy / len(test_dataloader)))
```

### 1.3.4 ä½¿ç”¨GPUè®­ç»ƒ

1. è·å–GPU

   ```python
   device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
   ```

2. å°†æ¨¡å‹å’Œæ•°æ®ç­‰è½¬ç§»åˆ°GPUä¸Š

   ```python
   model.to(device) # è½¬ç§»æ¨¡å‹
   loss.to(device) # è½¬ç§»æŸå¤±å‡½æ•°
   
   # æ³¨æ„è½¬ç§»æ•°æ®æ—¶éœ€è¦é‡æ–°èµ‹å€¼
   data = data.to(device) # è½¬ç§»è®­ç»ƒæ•°æ®
   label = label.to(device) # è½¬ç§»æ ‡ç­¾
   ```

### 1.3.5 å®Œæ•´è®­ç»ƒè¿‡ç¨‹

1. åŠ è½½å’Œå¤„ç†æ•°æ®é›†
   1. åŠ è½½æ•°æ®é›†
   2. tokenizeæ•°æ®é›†
   3. å°†æ•°æ®é›†æ”¾å…¥DataLoaderä¸­
   4. æµ‹è¯•ï¼šä»DataLoaderä¸­å–å‡ºä¸€ç»„æ•°æ®æŸ¥çœ‹æ•°æ®ç»´åº¦

2. æ­å»ºç¥ç»ç½‘ç»œ
   1. åˆ›å»ºModelç±»ï¼ˆç»§æ‰¿nn.Moduleï¼‰
   2. åˆ›å»ºmodelå¯¹è±¡
   3. æµ‹è¯•ï¼šå°†DataLoaderä¸­çš„ä¸€ç»„æ•°æ®æ”¾å…¥æ¨¡å‹ä¸­ï¼Œçœ‹èƒ½å¦å¾—åˆ°æƒ³è¦æ ¼å¼çš„è¾“å‡º

3. åˆ›å»ºæŸå¤±å‡½æ•°
4. åˆ›å»ºä¼˜åŒ–å™¨
5. è®¾ç½®è®­ç»ƒå‚æ•° 
6. å¼€å§‹è®­ç»ƒ
   1. è®­ç»ƒ
   2. è¯„ä¼°

7. æ¨¡å‹è¯„ä¼°ï¼ˆä½¿ç”¨æµ‹è¯•æ•°æ®é›†ï¼‰

```python
from transformers import AutoTokenizer, AutoModel,DataCollatorWithPadding
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd

# 1. åŠ è½½å’Œå¤„ç†æ•°æ®é›†
## 1.1 åŠ è½½æ•°æ®é›†
from datasets import load_dataset

path = "/root/autodl-tmp/CodeXGLUE/tackled_{}.csv"
data_files = {"train":path.format("train"), "valid":path.format("valid"), "test":path.format("test")}
raw_dataset = load_dataset("csv", data_files=data_files)
## 1.2 tokenizeæ•°æ®é›†
tokenizer = AutoTokenizer.from_pretrained("./codebert_tokenizer")

def tokenize_function(example):
    return tokenizer(example["funcSource"], truncation=True)

tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(["funcSource"])
tokenized_datasets.set_format("torch")
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
## 1.3 å°†æ•°æ®é›†æ”¾å…¥DataLoaderä¸­
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    batch_size=8,
    collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["valid"],
    batch_size=8,
    collate_fn=data_collator
)
test_dataloader = DataLoader(
    tokenized_datasets["test"],
    batch_size=8,
    collate_fn=data_collator
)
## 1.4 æµ‹è¯•ï¼šä»DataLoaderä¸­å–å‡ºä¸€ç»„æ•°æ®æŸ¥çœ‹æ•°æ®ç»´åº¦
for batch in train_dataloader:
    break
{k:v.shape for k, v in batch.items()}

# 2.æ­å»ºç¥ç»ç½‘ç»œ
## 2.1 åˆ›å»ºModelç±»ï¼ˆç»§æ‰¿nn.Moduleï¼‰
codebert_model = AutoModel.from_pretrained("./codebert_model")
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.codebert = codebert_model
        self.linear = nn.Linear(768, 2)

    def forward(self, input_ids, attention_mask):
        output = self.codebert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        return self.linear(output[:,0,:])
## 2.2 åˆ›å»ºmodelå¯¹è±¡
model = Model()
## 2.3 æµ‹è¯•ï¼šå°†DataLoaderä¸­çš„ä¸€ç»„æ•°æ®æ”¾å…¥æ¨¡å‹ä¸­ï¼Œçœ‹èƒ½å¦å¾—åˆ°æƒ³è¦æ ¼å¼çš„è¾“å‡º
for batch in train_dataloader:
    break
with torch.no_grad():
    output = model(batch['input_ids'], batch['attention_mask'])
output
# 3. åˆ›å»ºæŸå¤±å‡½æ•°
loss_fn = nn.CrossEntropyLoss()
# 4. åˆ›å»ºä¼˜åŒ–å™¨
from torch.optim import Adam,AdamW,SGD
optimizer = SGD(model.parameters(), lr=1e-4)
# 5. è®¾ç½®è®­ç»ƒå‚æ•°
num_epochs = 10
num_training_steps = num_epochs * len(train_dataloader)
from tqdm.auto import tqdm # è®­ç»ƒè¿›åº¦æ¡
progress_bar = tqdm(range(num_training_steps))

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# 6. å¼€å§‹è®­ç»ƒ
for epoch in range(num_epochs):
## 6.1 è®­ç»ƒ
    model.train()

    total_train_loss = 0
    for batch in train_dataloader:
        batch = {k:v.to(device) for k, v in batch.items()}
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']

        optimizer.zero_grad()
        output = model(input_ids, attention_mask)
        loss = loss_fn(output, labels)
        loss.backward()

        optimizer.step()

        total_train_loss += loss.item()
        progress_bar.update(1)
## 6.2 è¯„ä¼°ï¼ˆevalæ•°æ®é›†ï¼‰
    model.eval()
    with torch.no_grad():
        for batch in eval_dataloader:
            batch = {k:v.to(device) for k, v in batch.items()}
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            outputs = model(input_ids, attention_mask)
            loss = loss_fn(outputs, labels)
            total_eval_loss += loss.item()
            
            predictions = outputs.argmax(dim=1)
            accuracy = (predictions == labels).sum().item()
            total_eval_accuracy += accuracy
            total_samples += labels.size(0)
            
    avg_eval_loss = total_eval_loss / len(eval_dataloader)
    avg_training_loss = total_train_loss / len(train_dataloader)
    
    avg_accuracy = total_eval_accuracy / total_samples
    print("----------------------")
    print(f"training_loss:{avg_training_loss},  validation_loss:{avg_eval_loss}")
    print(f"epoch_accuracy:{avg_accuracy}")
    
    torch.save(model.state_dict(), f"codebert_SGD_epoch{epoch + 10}")
    
# 7. è®­ç»ƒå®Œæˆåè¿›è¡Œæ¨¡å‹è¯„ä¼°ï¼ˆtestæ•°æ®é›†ï¼‰
total_test_loss = 0
total_test_accuracy = 0
total_samples = 0
with torch.no_grad():
    model.eval()
    for batch in test_dataloader:
        batch = {k:v.to(device) for k, v in batch.items()}
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']
        
        outputs = model(input_ids, attention_mask)
        loss = loss_fn(outputs, labels)
        total_test_loss += loss.item()
        
        predictions = outputs.argmax(1)
        accuracy = (predictions == labels).sum().item()
        total_test_accuracy += accuracy
        total_samples += labels.size(0)
        
    avg_accuracy = total_test_accuracy / total_samples
    print(f"accuracy:{avg_accuracy}")
        
```

# 1. Hugging faceåº“Transformerä½¿ç”¨

è¦ä½¿ç”¨ğŸ¤—Transformeråº“ï¼Œç¬¬ä¸€æ­¥éœ€è¦å®‰è£…

ç›´æ¥å®‰è£…ï¼š`pip install transormers`

å®‰è£…å¼€å‘ç‰ˆæœ¬ï¼ˆå¸¦æœ‰å‡ ä¹æ‰€æœ‰æ‰€éœ€çš„ä¾èµ–é¡¹ï¼‰:`pip install transformers[sentencepiece]`

## 1.1 åŸºæœ¬ä½¿ç”¨

Huggingfaceä¸­æœ‰å¾ˆå¤šåŸºäºTransformeræ¶æ„çš„ç°æˆæ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥æ‹¿æ¥å®Œæˆä¸€äº›NLPä»»åŠ¡

### 1.1.1 ç›´æ¥ä½¿ç”¨pipeline

Transformersåº“ä¸­æœ€åŸºæœ¬çš„å¯¹è±¡æ˜¯pipeline()å‡½æ•°ï¼Œé€šè¿‡pipelineå¯ä»¥ç›´æ¥åŠ è½½å¹¶ä½¿ç”¨ä¸€ä¸ªTrasformerå·²æœ‰çš„è®­ç»ƒå¥½çš„æ¨¡å‹

åŠ è½½ä»£ç ä¼šåœ¨æ¯ä¸ªåº“çš„Use this modelä¸­æ ‡æ˜

**pipelineä½¿ç”¨**

æ¯”å¦‚ä½¿ç”¨BERTæ¨¡å‹

```python
# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("fill-mask", model="google-bert/bert-base-cased")
pipe("Hello I'm a [MASK] model.")
outputs:
    [{'score': 0.17099590599536896,
  'token': 2959,
  'token_str': 'sorry',
  'sequence': "I'm sorry, who are you."},
 {'score': 0.12543228268623352,
  'token': 1303,
  'token_str': 'here',
  'sequence': "I'm here, who are you."},
 {'score': 0.03565218672156334,
  'token': 4853,
  'token_str': 'confused',
  'sequence': "I'm confused, who are you."},
 {'score': 0.018463322892785072,
  'token': 4107,
  'token_str': 'asking',
  'sequence': "I'm asking, who are you."},
 {'score': 0.01268229354172945,
  'token': 1128,
  'token_str': 'you',
  'sequence': "I'm you, who are you."}]
```

pipelineç¬¬ä¸€ä¸ªå‚æ•°ä¸­æŒ‡å®šå®Œæˆçš„ä»»åŠ¡ï¼ˆå®Œå½¢å¡«ç©ºï¼‰ï¼Œmodelå‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹ï¼ˆBERTï¼‰ï¼Œä¼šè¿”å›ä¸€ä¸ªæ¨¡å‹å¯¹è±¡

ä¹‹åç›´æ¥å°†è¦é¢„æµ‹å•è¯çš„å¥å­æ”¾å…¥pipeä¸­ï¼Œå³å¯å¾—åˆ°é¢„æµ‹ç»“æœã€‚

**pipelineå†…éƒ¨**

pipelineå®é™…ä¸Šæ˜¯å°†ä¸‰ä¸ªæ­¥éª¤ç»„åˆåœ¨äº†ä¸€èµ·ï¼š

1. å°†ä¼ å…¥çš„å­—ç¬¦ä¸²è¿›è¡ŒTokenizerï¼Œå³åˆ†è§£ä¸ºtokenï¼Œå¹¶è½¬åŒ–ä¸ºtensor

2. tokenizedä¹‹åçš„ç»“æœä¼ å…¥Modelä¸­è¿›è¡Œé¢„æµ‹

3. å°†Modelçš„è¾“å‡ºè¿›è¡ŒPost Porcessingåå¤„ç†ï¼Œä»¥äººç±»èƒ½ç†è§£çš„æ–¹å¼è¾“å‡º

è¿™ç§æ–¹å¼æ˜“äºä½¿ç”¨ï¼Œä½†æ˜¯åªèƒ½å®Œæˆç‰¹å®šä»»åŠ¡ï¼Œå¦‚æœæƒ³æ›´çµæ´»çš„ä½¿ç”¨æ¨¡å‹ï¼ŒTransformerä¹Ÿæä¾›äº†å¯¹Tokenizerå’Œmodelçš„è·å–

### 1.1.2 tokenizer

**åŸºæœ¬ç”¨æ³•tokenizers**

- åŠ è½½

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

- ä½¿ç”¨

```python
tokenizer("Using a Transformer network is simple")
outputs:
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

- ä¿å­˜

```python
tokenizer.save_pretrained("directory_on_my_computer")
```

**å°†å¥å­æ‹†åˆ†ä¸ºtoken**

ä¼šæ ¹æ®åŠ è½½çš„æ¨¡å‹çš„tokenizerè§„åˆ™è¿›è¡Œæ‹†åˆ†

```python
sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
outputs:
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

**ç¼–ç ï¼šå°†tokenè½¬åŒ–ä¸ºtensor**

è¿™é‡Œçš„ç¼–ç æ˜¯æ ¹æ®æ¨¡å‹ä¸­tokenizeré¢„å¤„ç†ç”Ÿæˆçš„è¯æ±‡è¡¨è½¬åŒ–çš„

```python
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
outputs:
    [7993, 170, 11303, 1200, 2443, 1110, 3014]
```

ä¹Ÿå¯ä»¥è¿›è¡Œè§£ç ï¼Œå³å°†è¯æ±‡è¡¨ç¼–å·è½¬åŒ–ä¸ºå­—ç¬¦ä¸²

```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
outputs:
    'Using a Transformer network is simple'
```

### 1.1.3 Model

**åŠ è½½æ¨¡å‹**

ä»Huggingfaceä¸­åŠ è½½åˆ«äººä¸Šä¼ çš„ä½¿ç”¨Transformeræ¶æ„çš„æ¨¡å‹

- ä½¿ç”¨é»˜è®¤é…ç½®åŠ è½½

  è¿™ç§æ–¹å¼åŠ è½½çš„æ¨¡å‹å‚æ•°æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ

  ```python
  from transformers import BertConfig, BertModel
  
  # Building the config
  config = BertConfig()
  
  # Building the model from the config
  model = BertModel(config)
  ```

- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

  ``` python
  from transformers import BertModel
  
  model = BertModel.from_pretrained("bert-base-cased")
  ```

  ä¹Ÿå¯ä»¥ä½¿ç”¨AutoModelæ›¿æ¢BertModelï¼ŒAutoModelä¼šæ ¹æ®åé¢çš„æ£€æŸ¥ç‚¹ï¼ˆå³bert-base-casedï¼‰è‡ªåŠ¨æ¨æ–­æ¨¡å‹ç±»å‹

  ```python
  from transformers import AutoModel
  
  model = AutoModel.from_pretrained("bert-base-cased")
  ```

**ä¿å­˜æ¨¡å‹**

å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜åœ¨æœ¬åœ°ç£ç›˜

```python
model.save_pretrained("directory_on_my_computer")
```

è¿™ä¼šä¿å­˜ä¸¤ä¸ªæ–‡ä»¶ï¼š

- config.jsonï¼šæ¨¡å‹é…ç½®ï¼Œä¿å­˜äº†æ„å»ºæ¨¡å‹ä½“ç³»ç»“æ„éœ€è¦çš„æ‰€æœ‰å±æ€§

- pytorch_model.binï¼šæ¨¡å‹çš„æ‰€æœ‰æƒé‡

**ä½¿ç”¨æ¨¡å‹**

å°†tokenizerçš„è¾“å‡ºä½œä¸ºè¾“å‡ºæ”¾å…¥æ¨¡å‹ä¸­ï¼Œå³å¯å¾—åˆ°ç»“æœ

- ç›´æ¥ä½¿ç”¨tokenizer

    å‚æ•°ï¼š

    - sequenceï¼šè¦å¤„ç†çš„å­—ç¬¦ä¸²ï¼ˆåºåˆ—ï¼‰ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æ•°ç»„

    - paddingï¼šå¼€å¯å¡«å……åŠŸèƒ½

      å¯é€‰å€¼ï¼š"longest"ï¼Œå¡«å……è‡³å­—ç¬¦åºåˆ—ä¸­æœ€é•¿å¥å­çš„é•¿åº¦

      "max_length"æˆ–è€…Trueï¼Œå¡«å……è‡³max_length

    - max_lengthï¼šæŒ‡å®šæœ€å¤§åºåˆ—çš„é•¿åº¦

      é»˜è®¤æ˜¯æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œæ¯”å¦‚BERTä¸º512

    - truncationï¼šTrueï¼Œé»˜è®¤ä¸ºFalse

      å°†åºåˆ—æˆªæ–­è‡³max_length

    - return_tensorsï¼šè¿”å›çš„å¼ é‡çš„ç±»å‹

      ptï¼šPyTorchå¼ é‡

      tfï¼šTensorFlowå¼ é‡

      npï¼šNumPyæ•°ç»„

```python
tokenized_inputs = tokenizer.tokenizer(sequence, padding="max_length", max_length=8, trucncation=True, return_tensors="pt")

model(tokenized_inputs)
```

- æ‰‹åŠ¨è¿›è¡Œåˆ†è¯å’Œç¼–ç 

æ‰‹åŠ¨è¿›è¡Œåˆ†è¯å’Œç¼–ç åéœ€è¦æ‰‹åŠ¨å†æ·»åŠ ä¸€ä¸ªç»´åº¦ï¼Œå› ä¸ºæ¨¡å‹é»˜è®¤æƒ…å†µä¸‹éƒ½éœ€è¦å¤šä¸ªå¥å­ï¼Œè€Œè¯¥ä¾‹å­ä¸­åªæœ‰ä¸€ä¸ªå¥å­

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor([ids]) # æ³¨æ„è¿™é‡Œè¦æ‰‹åŠ¨æ·»åŠ ä¸€ä¸ªç»´åº¦
# This line will fail.
model(input_ids)
```

## 1.2 å¾®è°ƒæ¨¡å‹

### 1.2.1 æ•°æ®é¢„å¤„ç†

1. åŠ è½½æ•°æ®é›†

   å¯ä»¥ä»æ¨¡å‹ä¸­å¿ƒç›´æ¥åŠ è½½å·²æœ‰æ•°æ®é›†

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets

outputs:
    DatasetDict({
        train: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 3668
        })
        validation: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 408
        })
        test: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 1725
        })
    })
```

â€‹		ä¼šå¾—åˆ°ä¸€ä¸ªå­—å…¸ï¼Œå¯ä»¥é€šè¿‡é”®å¾—åˆ°æ¯éƒ¨åˆ†

```python
raw_datasets["train"] # å¾—åˆ°è®­ç»ƒé›†
raw_datasets["train"][0] # å¾—åˆ°è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªæ•°æ®
raw_datasets["train"]["sentence1"] # å¾—åˆ°è®­ç»ƒé›†çš„æ‰€æœ‰æ•°æ®ä¸­çš„ç¬¬ä¸€ä¸ªå¥å­
```

2. å¯¹æ•°æ®é›†è¿›è¡Œtokenize

    æ–¹æ³•ä¸€ï¼šç›´æ¥è°ƒç”¨tokenizer

    ç¼ºç‚¹æ˜¯éœ€è¦åˆ†åˆ«ç»™è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®é›†ç¼–ç ï¼Œå¹¶ä¸”å¾—åˆ°çš„ç»“æœæ˜¯å­—å…¸ï¼Œä¸å®œç”¨

    ```python
    checkpoint = "bert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    
    tokenized_dataset = tokenizer(
        raw_datasets["train"]["sentence1"], # è¿™å°†ä¼šè·å–è®­ç»ƒé›†çš„æ‰€æœ‰ç¬¬ä¸€ä¸ªå¥å­
        raw_datasets["train"]["sentence2"], # è¿™å°†ä¼šè·å–è®­ç»ƒé›†çš„æ‰€æœ‰ç¬¬äºŒä¸ªå¥å­
        padding=True,
        truncation=True,
    )
    ä¼šå¾—åˆ°ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºï¼šè¾“å…¥è¯ï¼ˆinput_idsï¼Œå³ç¼–ç åçš„å¥å­ï¼‰ã€attention_maskå’Œtoken_type_idsï¼ˆç”¨äºæ ‡è®°è¿™æ˜¯ç¬¬ä¸€ä¸ªå¥å­è¿˜æ˜¯ç¬¬äºŒä¸ªå¥å­ï¼‰
    ```

    æ–¹æ³•äºŒï¼šä½¿ç”¨mapå‡½æ•°ç»™æ•°æ®é›†ä¸­æ¯ä¸ªæ•°æ®éƒ½åº”ç”¨ä¸€ä¸ªtokenizeå‡½æ•°

    mapå‡½æ•°ä¼šåœ¨åŸæ•°æ®é›†çš„åŸºç¡€ä¸Šï¼Œç»™æ¯æ¡æ•°æ®ä¸­æ·»åŠ tokenizeåçš„ç¼–ç ç»“æœï¼ˆå³ç»™æ¯æ¡æ•°æ®å¤šæ·»åŠ äº†input_ids, attention_mask, token_type_idsä¸‰ä¸ªå­—æ®µï¼‰

    åœ¨mapå‡½æ•°ä¸­ï¼Œå¦‚æœbatched=Trueï¼Œåˆ™ä¼ å…¥tokenize_functionçš„å‚æ•°exampleå°±æ˜¯å¤šä¸ªæ ·æœ¬ç»„æˆçš„åˆ—è¡¨ï¼Œå¦‚æœbatche=Falseï¼Œåˆ™å‚æ•°ä¸ºå•ä¸ªæ ·æœ¬

    ```python
    def tokenize_function(example): # ç»™æ¯ä¸ªæ•°æ®åº”ç”¨çš„å‡½æ•°
        return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
    
    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
    tokenized_datasets
    
    outputsï¼š
    DatasetDict({
        train: Dataset({
            features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
            num_rows: 3668
        })
        validation: Dataset({
            features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
            num_rows: 408
        })
        test: Dataset({
            features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
            num_rows: 1725
        })
    })
    ```

3. åŠ¨æ€å¡«å……

    åœ¨ä¸Šé¢tokenizeæ—¶å¹¶æ²¡æœ‰æ·»åŠ paddingå‚æ•°ï¼Œè¿™æ˜¯å› ä¸ºåœ¨tokenizeæ—¶å°†æ‰€æœ‰æ ·æœ¬å¡«å……åˆ°æœ€å¤§é•¿åº¦æ•ˆç‡ä¸é«˜ã€‚è½¬è€Œæˆ‘ä»¬åœ¨æ„å»ºæ‰¹å¤„ç†ï¼ˆbatchï¼‰æ—¶å¡«å……æ ·æœ¬ï¼Œè¿™æ ·çš„è¯åªéœ€è¦å¡«å……åˆ°è¯¥batchä¸­çš„æœ€å¤§é•¿åº¦
    
    ğŸ¤—transformeråº“ä¸­æä¾›äº†DataCollatorWithPaddingå‡½æ•°
    
    ```python
    from transformers import DataCollatorWithPadding
    
    # åˆ›å»ºå¯¹è±¡éœ€è¦ä¸€ä¸ªtokenizerå¯¹è±¡ï¼Œä»¥ä¾¿çŸ¥é“å¡«å……è§„åˆ™ï¼ˆç”¨å“ªä¸ªè¯å¡«å……ï¼Œå¡«å……åœ¨å‰é¢è¿˜æ˜¯åé¢ï¼‰
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    samples = tokenized_datasets["train"][:8] # å‡è®¾å–å‡ºå‰8ä¸ªæ ·æœ¬ä½œä¸ºä¸€ç»„
    batch = data_collator(samples) # å¯¹è¯¥ç»„æ ·æœ¬è¿›è¡Œå¡«å……
    ```

### 1.2.2 ä½¿ç”¨Trainerç±»å¾®è°ƒæ¨¡å‹

ğŸ¤—Transformersæä¾›äº†ä¸€ä¸ªTrainerç±»ï¼Œå¯ä»¥å¾®è°ƒä»»ä½•é¢„è®­ç»ƒæ¨¡å‹

1. é¦–å…ˆè¦å®šä¹‰å¥½æ¨¡å‹å’Œé¢„å¤„ç†å¥½æ•°æ®é›†

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

2. å®šä¹‰ä¸€ä¸ªTrainingArgumentsç±»

    è¯¥ç±»åŒ…å«Trainerç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„æ‰€æœ‰è¶…å‚æ•°

    ```python
    from transformers import TrainingArguments

    training_args = TrainingArguments(output_dir="./", # è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ä¿å­˜åœ°å€
                                      evaluation_strategy="epoch", # è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯„ä¼°ç­–ç•¥ï¼Œéš”å¤šä¹…è¯„ä¼°ä¸€æ¬¡
                                      logging_dir="./", # æ—¥å¿—æ–‡ä»¶å¤¹
                                      logging_steps=100 # è®­ç»ƒå¤šå°‘å¸ƒä¿å­˜y
                                     )
    ```

3. å®šä¹‰æ¨¡å‹

   ç”±äºåŸæ¨¡å‹BERTå¹¶æ²¡æœ‰å®šä¹‰å¥å­åˆ†ç±»çš„ä»»åŠ¡ï¼Œæ‰€ä»¥AutoModelForSequenceClassificationå®é™…ä¸Šæ˜¯ç»™BERTåé¢æ–°åŠ ä¸€ä¸ªå…¨é“¾æ¥å±‚ç”¨äºåˆ†ç±»ä»»åŠ¡

    ```python
   from transformers import AutoModelForSequenceClassification

   model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
    ```

4. å®šä¹‰Trainerå¯¹è±¡

    ```python
    from transformers import Trainer

    trainer = Trainer(
        model,
        training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    ```

5. å¼€å§‹è®­ç»ƒ

    ```python
    trainer.train()
    ```

### 1.2.3 è¯„ä¼°æ¨¡å‹

**predictæ–¹æ³•**

trainerç±»ä¸­æä¾›äº†ä¸€ä¸ªè¯„ä¼°æ¨¡å‹çš„æ–¹æ³•predictï¼Œéœ€è¦ä¼ å…¥ä¸€ä¸ªæµ‹è¯•é›†ï¼Œè¿”å›ä¸€ä¸ªæœ‰ä¸‰ä¸ªå­—æ®µçš„å‘½åå…ƒç¥–

- predictionsï¼šæµ‹è¯•ç»“æœï¼Œç»™å‡ºçš„æ˜¯æ¯ä¸ªç­”æ¡ˆçš„æ¦‚ç‡æ•°ç»„ï¼Œä½¿ç”¨æ—¶éœ€è¦åœ¨ç¬¬äºŒä¸ªè½´ä¸Šå–æœ€å¤§å€¼ç´¢å¼•

- label_idsï¼šæµ‹è¯•é›†çš„æ ‡ç­¾ï¼Œå³æ­£ç¡®ç»“æœ
- metricsï¼šåŒ…å«æ•°æ®é›†çš„lossã€è¿è¡Œæ—¶é—´ç­‰ä¿¡æ¯

```python
predictions = trainer.predict(tokenized_datasets["validation"])
predictions.predictions
predictions.label_ids
predictions.metrics
```

**evaluateåº“**

evaluateåº“å¯ä»¥ç›´æ¥åŠ è½½æ•°æ®é›†å¯¹åº”çš„è¯„ä»·æŒ‡æ ‡ï¼Œå¹¶æä¾›äº†æ–¹æ³•å¯¹è¿™äº›æŒ‡æ ‡è¿›è¡Œè®¡ç®—

```python
import evaluate

metric = evaluate.load("accuracy", "f1") # åŠ è½½è¯„ä»·æŒ‡æ ‡
metric.compute(predictions=preds, references=predictions.label_ids) # ä¼ å…¥é¢„æµ‹ç»“æœå’ŒçœŸå®ç»“æœï¼Œè®¡ç®—æŒ‡æ ‡
```

**compute_metric()å‡½æ•°**

compute_metric()å‡½æ•°å¯ä»¥ä½œä¸ºå‚æ•°åˆ›å»ºtrainerå¯¹è±¡ï¼Œè¿™æ ·åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°±ä¼šæ¯éš”ä¸€å®šepochå°±è¯„ä¼°ä¸€ä¸‹æ¨¡å‹è®­ç»ƒç»“æœ

```python
import evaluate
import numpy as np

def compute_metrics(eval_preds):
    metric = evaluate.load("accuracy", "f1") # åŠ è½½è¯„ä»·æŒ‡æ ‡
    logits, labels = eval_preds # eval_predsæ˜¯ä½¿ç”¨trainer.predictçš„ç»“æœï¼Œlogitsæ˜¯é¢„æµ‹ç»“æœï¼Œlabelsæ˜¯æ­£ç¡®ç»“æœ
    predictions = np.argmax(logits, axis=-1) # å¤„ç†é¢„æµ‹ç»“æœ
    return metric.compute(predictions=predictions, references=labels) # è®¡ç®—æŒ‡æ ‡
```

æœ€åç”¨compute_metric()å‡½æ•°ä½œä¸ºå‚æ•°åˆ›å»ºtrainerå¯¹è±¡ï¼Œå³å¯å¾—åˆ°æ¯ä¸ªè®­ç»ƒé˜¶æ®µæ¨¡å‹çš„æŒ‡æ ‡

```python
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model, # æ¨¡å‹
    training_args, # è®­ç»ƒè¶…å‚æ•°
    train_dataset=tokenized_datasets["train"], # æŒ‡å®šè®­ç»ƒé›†
    eval_dataset=tokenized_datasets["validation"], # æŒ‡å®šæµ‹è¯•é›†
    data_collator=data_collator, # åŠ¨æ€å¡«å……
    tokenizer=tokenizer, # ç¼–ç å™¨
    compute_metrics=compute_metrics, # è¯„ä¼°å‡½æ•°
)
```

### 1.2.4 æ‰‹åŠ¨å¾®è°ƒæ¨¡å‹

æ‰‹åŠ¨å¾®è°ƒæ¨¡å‹æ˜¯æŒ‡ä¸ç”¨Trainerç±»çš„æƒ…å†µä¸‹è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè½¬è€Œæˆ‘ä»¬è¦ä½¿ç”¨pytorchæ¥å®ç°è¿™ä¸ªè®­ç»ƒè¿‡ç¨‹

1. é¢„å¤„ç†æ•°æ®é›†

   ```python
   from datasets import load_dataset
   from transformers import AutoTokenizer, DataCollatorWithPadding
   
   raw_datasets = load_dataset("glue", "mrpc")
   checkpoint = "bert-base-uncased"
   tokenizer = AutoTokenizer.from_pretrained(checkpoint)
   
   
   def tokenize_function(example):
       return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
   
   
   tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
   data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

2. å¯¹tokenized_datasetsåšä¸€äº›å¤„ç†

   - åˆ é™¤ä¸€äº›æ¨¡å‹ä¸éœ€è¦çš„åˆ—ï¼ˆå¦‚sentence1å’Œsentence2åˆ—ï¼‰

     ```python
     tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
     ```

   - å°†åˆ—ålabelé‡å‘½åä¸ºlabelsï¼ˆæ¨¡å‹éœ€è¦ï¼‰

     ```python
     tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
     ```

   - å°†æ•°æ®é›†æ ¼å¼æ”¹ä¸ºPyTorchå¼ é‡

     ```python
     tokenized_datasets.set_format("torch")
     ```

   ä¿®æ”¹å®Œä¹‹åæ£€æŸ¥ä¸€ä¸‹æ•°æ®é›†ä¸­çš„åˆ—

   ```python'
   tokenized_datasets["train"].column_names
   outputs:
   ["attention_mask", "input_ids", "labels", "token_type_ids"]
   ```

3. å®šä¹‰æ•°æ®åŠ è½½å™¨DataLoader

   ```python
   from torch.utils.data import DataLoader
   
   train_dataloader = DataLoader(
       tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
   )
   eval_dataloader = DataLoader(
       tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
   )
   ```

   å¯ä»¥æ£€éªŒå…¶ä¸­ä¸€ä¸ªbatchçœ‹æ•°æ®å¤„ç†ä¸­æœ‰æ²¡æœ‰é”™è¯¯

   ```python
   for batch in train_dataloader:
       break
   {k: v.shape for k, v in batch.items()}
   ```

4. åŠ è½½æ¨¡å‹

   ```python
   from transformers import AutoModelForSequenceClassification
   
   model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
   ```

   ä¼ é€’ä¸€ä¸ªbatchçš„æ•°æ®æµ‹è¯•ä¸€ä¸‹

   ```python
   outputs = model(**batch)
   print(outputs.loss, outputs.logits.shape)
   ```

5. åŠ è½½ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨

   ä¼˜åŒ–å™¨ä½¿ç”¨AdamW

   ```python
   from transformers import AdamW
   from transformers import get_scheduler
   
   optimizer = AdamW(model.parameters(), lr=5e-5)
   
   num_epochs = 3
   num_training_steps = num_epochs * len(train_dataloader)
   lr_scheduler = get_scheduler(
       "linear",
       optimizer=optimizer,
       num_warmup_steps=0,
       num_training_steps=num_training_steps,
   )
   ```

6. ä½¿ç”¨GPU

   ```python
   import torch
   
   device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
   model.to(device)
   ```

7. è®­ç»ƒå¾ªç¯

   tqdmåº“ç”¨äºæ‰“å°è®­ç»ƒè¿›åº¦æ¡

   ```python
   from tqdm.auto import tqdm
   
   progress_bar = tqdm(range(num_training_steps))
   
   model.train()
   for epoch in range(num_epochs):
       for batch in train_dataloader:
           batch = {k: v.to(device) for k, v in batch.items()}
           outputs = model(**batch)
           loss = outputs.loss
           loss.backward()
   
           optimizer.step()
           lr_scheduler.step()
           optimizer.zero_grad()
           progress_bar.update(1)
   ```

8. è¯„ä¼°æ¨¡å‹ç»“æœ

   ```python
   import evaluate
   
   metric = evaluate.load("glue", "mrpc")
   model.eval()
   for batch in eval_dataloader:
       batch = {k: v.to(device) for k, v in batch.items()}
       with torch.no_grad():
           outputs = model(**batch)
   
       logits = outputs.logits
       predictions = torch.argmax(logits, dim=-1)
       metric.add_batch(predictions=predictions, references=batch["labels"])
   
   metric.compute()
   ```

## 1.3 DATASETSåº“

### 1.3.1 åŠ è½½æœ¬åœ°æ•°æ®é›†

ğŸ¤—DATASETSæ”¯æŒçš„æ•°æ®é›†æ ¼å¼ï¼š

| Data format        | Loading script | Example                                                 |
| ------------------ | -------------- | ------------------------------------------------------- |
| CSV & TSV          | `csv`          | `load_dataset("csv", data_files="my_file.csv")`         |
| Text files         | `text`         | `load_dataset("text", data_files="my_file.txt")`        |
| JSON & JSON Lines  | `json`         | `load_dataset("json", data_files="my_file.jsonl")`      |
| Pickled DataFrames | `pandas`       | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

**åŠ è½½æ•°æ®é›†**

åŒæ—¶åŠ è½½è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†

```python
from datasets import load_dataset

data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files)
```

### 1.3.2 DatasetBuilderç±»

åŠ è½½æ•°æ®é›†çš„load_datasetæ–¹æ³•æ˜¯é€šè¿‡DatasetBuilderç±»å®ç°çš„ï¼Œå¦‚æœè¦å°†è‡ªå·±çš„æ•°æ®é›†ä¸Šä¼ åˆ°HuggingFaceåº“æˆ–è€…ä¿®æ”¹åº“ä¸­å·²æœ‰çš„æ•°æ®é›†æ—¶ï¼Œéƒ½è¦ç»§æ‰¿è¿™ä¸ªç±»

ç±»ä¸­åªè¦åŒ…æ‹¬ä¸‰ä¸ªæ–¹æ³•ï¼š

1. infoï¼šæ•°æ®é›†çš„ä¿¡æ¯
2. split_generatorï¼šè´Ÿè´£ä»ç¡¬ç›˜æˆ–è€…ç½‘ç›˜ä¸­è¯»å…¥/ä¸‹è½½æ•°æ®ï¼Œå¹¶å°†æ•°æ®æŒ‰ç…§æ¯”ä¾‹åˆ†ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
3. generate_examplesï¼šè¯»å–æ•°æ®é›†ï¼Œå¹¶åœ¨è°ƒç”¨æ˜¯ç»™å‡ºæ•°æ®é¡¹

## 1.4 TOKENIZERSåº“

## 1.5 EVALUATEåº“

```python
!pip install evaluate
import evaluate
```

### 1.5.1 è¯„ä»·æŒ‡æ ‡

è¾“å…¥ä¸­ï¼Œpredictionså¯ç”±outputså–argmaxå–å¾—ï¼Œå³

```python
predictions = outputs.argmax(1)
```

- [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)ï¼šå‡†ç¡®ç‡ï¼Œé¢„æµ‹å‡†ç¡®çš„æ ·æœ¬æ•°å å…¨éƒ¨æ ·æœ¬æ•°çš„æ¯”å€¼

  ```python
  accuracy_metric = evaluate.load("accuracy")
  results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])
  ```

- [f1](https://huggingface.co/spaces/evaluate-metric/f1/blob/main/README.md)ï¼šf1æŒ‡æ ‡ï¼Œprecisionå’Œrecallçš„è°ƒå’Œå¹³å‡æ•°ï¼Œ`F1 = 2 * (precision * recall) / (precision + recall)`

  ```python
  f1_metric = evaluate.load("f1")
  results = f1_metric.compute(predictions=[0, 1], references=[0, 1])
  ```

- [precision](https://huggingface.co/spaces/evaluate-metric/precision)ï¼šæ‰€æœ‰è¢«æ¨¡å‹é¢„æµ‹ä¸ºé˜³æ€§çš„æ ·æœ¬æ•°ä¸­ï¼ŒçœŸé˜³æ€§çš„æ¯”ä¾‹ï¼Œ`Precision = TP / (TP + FP)`

  ```python
  precision_metric = evaluate.load("precision")
  results = precision_metric.compute(references=[0, 1], predictions=[0, 1])
  ```

- [recall](https://huggingface.co/spaces/evaluate-metric/recall)ï¼šæ‰€æœ‰æ ‡è®°ä¸ºé˜³æ€§æ ·æœ¬ä¸­ï¼Œè¢«æ¨¡å‹æ­£ç¡®é¢„æµ‹ä¸ºé˜³æ€§çš„æ ·æœ¬æ¯”ä¾‹ï¼Œ`Recall = TP / (TP + FN) `

  ```python
  recall_metric = evaluate.load('recall')
  results = recall_metric.compute(references=[0, 1], predictions=[0, 1])
  ```

# 3 Pandas

```python
import pandas as pd
```

## 3.1 è¯»å–æ•°æ®

- è¯»å–çº¯æ–‡æœ¬csvã€tsvã€txtæ–‡ä»¶

  csvç”¨é€—å·åˆ†å‰²ï¼Œtsvç”¨åˆ¶è¡¨ç¬¦\tåˆ†å‰²ï¼Œtxtæ–‡ä»¶è‡ªå®šä¹‰åˆ†å‰²ç¬¦

  ```python
  csv_file = pd.read_csv(path)
  # txtæ–‡ä»¶è¯»å–æ—¶éœ€è¦è‡ªå·±æŒ‡å®šåˆ†éš”ç¬¦ã€è¡¨å¤´ï¼ˆåˆ—åï¼‰
  txt_file = pd.read_cvs(
  	path,
      sep="\n",
      header=None, # åŸæ–‡ä»¶ä¸­æ²¡æœ‰è¡¨å¤´
      names=['name', 'hooby'] # è‡ªå®šä¹‰è¡¨å¤´
  )
  ```

- è¯»å–excelæ–‡ä»¶

  ```python
  excel = pd.read_excel(path)
  ```

- è¯»å–mysql

  ```python
  import pymysql
  conn = pymysql.connect(
  	host="127.0.0.1",
      user="root",
      password="",
      database="test_database",
      charset="utf8"
  )
  
  mysql_page = pd.read_sql("select * from test_table", con=conn)
  ```

- è¯»å–jsonæ–‡ä»¶

  ```python
  pd.read_json(path)
  ```

  è¯»å–jsonlæ–‡ä»¶ï¼ˆå³JSON Linesï¼Œä»¥è¡Œæ ¼å¼å­˜å‚¨ï¼‰

  ```python
  pd.read_json(path, lines=True)
  ```

**ä¿å­˜æ–‡ä»¶**

```python
# ä¿å­˜ä¸ºCSVæ–‡ä»¶
df.to_csv('output.csv', index=False)

# ä¿å­˜ä¸ºExcelæ–‡ä»¶
df.to_excel('output.xlsx', index=False)

# ä¿å­˜ä¸ºJSONæ–‡ä»¶
df.to_json('output.json', orient='records')

# ä¿å­˜ä¸ºHTMLæ–‡ä»¶
df.to_html('output.html', index=False)

# ä¿å­˜ä¸ºPickleæ–‡ä»¶
df.to_pickle('output.pkl')
```

## 3.2 DataFrame

ä½¿ç”¨pandasè¯»å…¥çš„æ–‡ä»¶éƒ½å°†å˜æˆä¸€ä¸ªDataFrameå¯¹è±¡ï¼ŒDataFrameå°±æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼Œä½†æ˜¯ä½¿ç”¨èµ·æ¥ç±»ä¼¼äºå­—å…¸

å¦‚æœæŸ¥è¯¢ä¸€åˆ—æˆ–ä¸€è¡Œï¼Œä¼šå¾—åˆ°ä¸€ä¸ªpd.Serieså¯¹è±¡ï¼Œç”¨èµ·æ¥ç±»ä¼¼äºå­—å…¸ï¼Œæ˜¯ç´¢å¼•+å€¼çš„å½¢å¼

DataFrameä¸­çš„å±æ€§å’Œæ–¹æ³•ï¼š

**æŸ¥çœ‹å±æ€§**

- æŸ¥çœ‹æ¯åˆ—çš„ç±»åˆ«ï¼š`df.dtypes`
- æŸ¥çœ‹æ‰€æœ‰åˆ—åï¼š`df.columns`
- æŸ¥çœ‹æ‰€æœ‰è¡Œç´¢å¼•ï¼š`df.index`
- å¯¹æŸåˆ—æ‰€æœ‰å€¼æŒ‰ç±»å‹åˆ†ç±»å¹¶è®¡ç®—æ¯ä¸€ç±»çš„æ•°é‡ï¼š`df["column"].value_counts()`

**æŸ¥è¯¢è¡¨æ•°æ®**

- æŸ¥è¯¢ä¸€åˆ—ï¼š`df["column_name"]`ï¼Œå¾—åˆ°ä¸€ä¸ªSerieså¯¹è±¡
- æŸ¥è¯¢ä¸€è¡Œï¼š`df.loc[1]`ï¼Œå¾—åˆ°ä¸€ä¸ªSerieså¯¹è±¡
- æŸ¥è¯¢å¤šåˆ—ï¼š`df[["column_name1","column_name2"]]`
- æŸ¥è¯¢å¤šè¡Œï¼š`df.loc[3:5]`ï¼Œæ³¨æ„è¿™é‡Œéƒ½æ˜¯é—­åŒºé—´ï¼Œä¹Ÿå°±æ˜¯è¯´ä¼šæŸ¥è¯¢åˆ°3ï¼Œ4ï¼Œ5ä¸‰è¡Œæ•°æ®

## 3.3 å¯¹åˆ—è¿ç®—

### 3.3.1 Seriesè¿ç®—

- å¯ä»¥ç›´æ¥ä½¿ç”¨èµ‹å€¼è¯­å¥åˆ›å»ºæ–°åˆ—

  ```python
  df["new_column"] = 0
  ```

  åˆ›å»ºä¸€ä¸ªåä¸ºnew_columnçš„åˆ—ï¼Œåˆ—ä¸­æ¯è¡Œå€¼éƒ½æ˜¯0

- ä¹Ÿå¯ä»¥ç›´æ¥ç”¨å·²æœ‰çš„ä¸¤ä¸ªSeriesè¿›è¡Œç®€å•è¿ç®—å¾—åˆ°æ–°åˆ—

  æ¯”å¦‚ï¼Œå°†ä¸¤åˆ—æ±‚å’Œå¾—åˆ°sumåˆ—

    ```python
    df.loc[:, "sum"] = df["column1"] + df["column2"]
    ```

- ä¹Ÿå¯ä»¥ç›´æ¥æ›¿æ¢æŸä¸€åˆ—çš„å€¼

  å°†columnè¿™åˆ—çš„å€¼å…¨éƒ¨+1

  ```python
  df.loc[:, 'column'] = df['column'] + 1
  ```

### 3.3.2 applyæ–¹æ³•

é€šè¿‡æŒ‡å®šæ–¹å‘ï¼Œå¯¹æ¯è¡Œæˆ–è€…æ¯åˆ—ä¾æ¬¡åº”ç”¨ä¸€ä¸ªå‡½æ•°

å¯¹æ¯è¡Œåº”ç”¨ï¼šaxis=1

```python
def get_big_small(row): # è¿™é‡Œçš„xæ˜¯DataFrameä¼ æ¥çš„ä¸€è¡Œæ•°æ®
    if row["column"] > 20:
        return 'big'
    else:
        return 'small'

# å¯¹æ‰€æœ‰è¡Œéƒ½åˆ›å»ºä¸€ä¸ªæ–°åˆ—ï¼Œåˆ—åä¸ºisBigï¼Œå€¼ç”±get_big_smallå¾—åˆ°
df.loc[:, "isBig"] = df.apply(get_big_small, axis=1)
```

### 3.3.3 åŸºæœ¬æ“ä½œ

```css
A,B,C,D
1,2,3,4
5,6,7,8
9,10,11,12
```

**åˆ é™¤åˆ—**

```python
# åˆ é™¤åˆ—Bå’ŒD
df = df.drop(['B', 'D'], axis=1)
```

**é‡å‘½ååˆ—**

```python
df = df.rename(columns={'A': 'Alpha', 'B': 'Bravo', 'C': 'Charlie', 'D': 'Delta'})
```

**æ£€æŸ¥åˆ—æ˜¯å¦ä¸ºç©º**

pandasåœ¨ä¿å­˜æ–‡ä»¶æ—¶ï¼Œä¼šè‡ªåŠ¨å°†ç©ºå€¼ï¼ˆæ¯”å¦‚ç©ºå­—ç¬¦ä¸²ï¼‰å’ŒNoneå€¼æ›¿æ¢ä¸ºç¼ºçœå€¼NaN

ä½¿ç”¨`isnull()`å’Œ`notnull()`æ¥æ£€æŸ¥æ˜¯å¦æœ‰ç©ºå€¼

**æŒ‰æ¡ä»¶ç»Ÿè®¡æŸåˆ—**

æ¯”å¦‚ç»Ÿè®¡å€¼ä¸ºNaNçš„åˆ—çš„ä¸ªæ•°

```python
none_count = (dataset['comment'].isnull()).sum()
```

# 2. è®ºæ–‡

## 2.1 æ•°æ®é›†

2.1.1 Draper. 

- è®ºæ–‡é¢˜ç›®ï¼šAutomated Vulnerability Detection in Source Code Using Deep Representation Learning

- åœ°å€ï¼š`/home/sazer/Documents/DefectDetection/datasets/finetune/draper`

- æ•°æ®é›†æè¿°ï¼š

  æ•°æ®é›†å°±æ˜¯æºä»£ç +æ ‡ç­¾ï¼Œæœ‰æ³¨é‡Š

  ç”±SATE IV Juliet Test Suiteã€Debian Linux distributionã€public Git repositoriesç»„æˆï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå°±æ˜¯CWEäººé€ æ•°æ®é›†é›†åˆï¼Œå‰©ä½™ä¸¤ä¸ªæ˜¯çœŸå®ä¸–ç•Œæ•°æ®é›†

### 2.1.2 Vuldeepecker

- è®ºæ–‡é¢˜ç›®ï¼šVulDeePecker: A Deep Learning-Based System for Vulnerability Detection

- åœ°å€ï¼š`/home/sazer/Documents/DefectDetection/datasets/VulDeePecker`

- æ•°æ®é›†æè¿°ï¼š

  äººé€ æ•°æ®é›†ï¼Œç”±cwe-119å’Œcwe-399ç»„æˆï¼Œcweç³»åˆ—éƒ½æ˜¯äººé€ æ•°æ®é›†ï¼Œå¹¶ä¸”æœ‰é‡å¤æƒ…å†µï¼ˆæµ‹è¯•é›†å’Œè®­ç»ƒé›†æœ‰é‡å¤ï¼Œä¼šé€ æˆæ•°æ®æ³„éœ²ï¼‰

### 2.1.3 TransferRepresentationLearning

- è®ºæ–‡é¢˜ç›®ï¼šCross-Project Transfer Representation Learning for Vulnerable Function Discovery

- åœ°å€ï¼š

  `/home/sazer/Documents/DefectDetection/datasets/TransferRepresentationLearning/Data/VulnerabilityData`

- æ•°æ®é›†æè¿°ï¼šè¿™ä¸ªæ•°æ®é›†ä¸ºå‡½æ•°çº§ï¼Œä¸€ä¸ªå‡½æ•°ä¸€ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­æœ‰ç¼ºé™·çš„å‡½æ•°æ–‡ä»¶åä»¥cwe-å¼€å¤´ã€‚æœ‰äº›å‡½æ•°ä¸­åŒ…æ‹¬æ³¨é‡Š

  çœŸå®æ•°æ®é›†ï¼Œä½¿ç”¨å¼€æºé¡¹ç›®è¿›è¡Œæ‰‹åŠ¨æ•°æ®æ ‡æ³¨

### 2.1.4 REVEAL

- è®ºæ–‡é¢˜ç›®ï¼šDeep Learning based Vulnerability Detection: Are We There Yet? ï¼ˆè®ºæ–‡ä¸­ç»™å‡ºçš„githubåº“ä¸­æä¾›çš„æ•°æ®å’Œæ¨¡å‹ä¸‹è½½è¿æ¥å‡å·²å¤±æ•ˆï¼‰

- æœ¬åœ°ä¿å­˜åœ°å€ï¼š`/home/sazer/Documents/DefectDetection/datasets/finetune/reveal`

- æ•°æ®é›†æè¿°ï¼š

  æ•°æ®é›†ä¸ºä»£ç ï¼ˆå‡½æ•°çº§ï¼‰å’Œå¯¹åº”æ ‡ç­¾ï¼Œæ²¡æœ‰æ³¨é‡Š

  çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œæ¥è‡ªChromiumå’ŒDebiané¡¹ç›®

### 2.1.5 d2a

**åŸºæœ¬ä¿¡æ¯**

- è®ºæ–‡é¢˜ç›®ï¼šD2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis

- æœ¬åœ°ä¿å­˜åœ°å€ï¼š`/home/sazer/Documents/DefectDetection/datasets/finetune/d2a`

- æ•°æ®é›†æè¿°ï¼š

- æ•°æ®é›†æ ¼å¼ï¼š.csv

  æ–‡ä»¶ä»¥id,label,codeçš„å½¢å¼ç»™å‡ºï¼Œç¬¬ä¸€ä¸ªä¾‹å­å¦‚ä¸‹

  ```c++
  id,label,code
  1,0,"static void srt_to_ass(AVCodecContext *avctx, AVBPrint *dst,
                         const char *in, int x1, int y1, int x2, int y2)
  {
      if (x1 >= 0 && y1 >= 0) {
          /* XXX: here we rescale coordinate assuming they are in DVD resolution
           * (720x480) since we don't have anything better */
  
          if (x2 >= 0 && y2 >= 0 && (x2 != x1 || y2 != y1) && x2 >= x1 && y2 >= y1) {
              /* text rectangle defined, write the text at the center of the rectangle */
              const int cx = x1 + (x2 - x1)/2;
              const int cy = y1 + (y2 - y1)/2;
              const int scaled_x = cx * (int64_t)ASS_DEFAULT_PLAYRESX / 720;
              const int scaled_y = cy * (int64_t)ASS_DEFAULT_PLAYRESY / 480;
              av_bprintf(dst, ""{\\an5}{\\pos(%d,%d)}"", scaled_x, scaled_y);
          } else {
              /* only the top left corner, assume the text starts in that corner */
              const int scaled_x = x1 * (int64_t)ASS_DEFAULT_PLAYRESX / 720;
              const int scaled_y = y1 * (int64_t)ASS_DEFAULT_PLAYRESY / 480;
              av_bprintf(dst, ""{\\an1}{\\pos(%d,%d)}"", scaled_x, scaled_y);
          }
      }
  
      ff_htmlmarkup_to_ass(avctx, dst, in);
  }"
  ```

### 2.1.6 sard

sardæ•°æ®é›†æ˜¯ç¾å›½å›½å®¶æ ‡å‡†æŠ€æœ¯ç ”ç©¶é™¢NISTå‘å¸ƒçš„ä¸€ä¸ªç¼ºé™·æ£€æµ‹æ•°æ®é›†

**åŸºæœ¬ä¿¡æ¯**

- è®ºæ–‡é¢˜ç›®ï¼šA Software Assurance Reference Dataset: Thousands of Programs With Known Bugs

- æ•°æ®é›†ä»‹ç»ï¼šSARD: Thousands of Reference Programs for Software Assurance

  è¿™ä¸ªæ˜¯ä½œè€…åœ¨research gateä¸Šå‘å¸ƒçš„ä¸€ç¯‡æ–‡ç« ï¼Œæ–‡ç« ä¸­å¯¹sardæ•°æ®é›†åšäº†ä»‹ç»

- æ•°æ®é›†åœ°å€ï¼š[sard](https://samate.nist.gov/SARD/)

  åœ¨è¿™ä¸ªåœ°å€ä¸­è‡ªå·±æ£€ç´¢æ‰€éœ€æ•°æ®é›†å¹¶ä¸‹è½½ä½¿ç”¨ï¼Œå®ƒä¹Ÿæä¾›äº†ä¸€äº›testcase suitæ•°æ®é›†é›†åˆï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨

### 2.1.7 devign

**åŸºæœ¬ä¿¡æ¯**

- è®ºæ–‡é¢˜ç›®ï¼šDevign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks

- æ•°æ®é›†åœ°å€ï¼š[devign](https://sites.google.com/view/devign)

  æœ¬åœ°åœ°å€ï¼š`/home/sazer/Documents/DefectDetection/datasets/finetune/devign/Devign.json`

  è¿™ä¸ªæ•°æ®é›†ä½œè€…åªæ”¾å‡ºäº†ä¸¤éƒ¨åˆ†ï¼ŒFFmpegå’ŒQemu

æ•°æ®é›†åˆ—ï¼šproject, commit_id, targetï¼ˆ0è¡¨ç¤ºæœ‰ç¼ºé™·ï¼Œ1è¡¨ç¤ºæ— ç¼ºé™·ï¼‰, func



### 2.1.8 bigvul

**åŸºæœ¬ä¿¡æ¯**

- è®ºæ–‡é¢˜ç›®ï¼šA C/C++ Code Vulnerability Dataset with Code Changes and CVE Summaries

- githubåœ°å€ï¼š[BigVul:MSR_20_Code_vulnerability_CSV_Dataset](https://github.com/ZeoVan/MSR_20_Code_vulnerability_CSV_Dataset)

  æœ‰æ¸…ç†åå¯ä»¥ç›´æ¥ä½¿ç”¨çš„æ•°æ®é›† [cleaned_dataset](https://drive.google.com/file/d/1-0VhnHBp9IGh90s2wCNjeCMuy70HPl8X/view?usp=sharing)

- æœ¬åœ°åœ°å€ï¼š`/home/sazer/Documents/DefectDetection/datasets/MSR_data_cleaned.csv`

æ•°æ®é›†ä¸­åŒ…å«çš„åˆ—åï¼š

```python
Index(['Unnamed: 0', 'Access Gained', 'Attack Origin',
       'Authentication Required', 'Availability', 'CVE ID', 'CVE Page',
       'CWE ID', 'Complexity', 'Confidentiality', 'Integrity',
       'Known Exploits', 'Publish Date', 'Score', 'Summary', 'Update Date',
       'Vulnerability Classification', 'add_lines', 'codeLink', 'commit_id',
       'commit_message', 'del_lines', 'file_name', 'files_changed',
       'func_after', 'func_before', 'lang', 'lines_after', 'lines_before',
       'parentID', 'patch', 'project', 'project_after', 'project_before',
       'vul', 'vul_func_with_fix'],
      dtype='object')
```

func_beforeæ˜¯æœ‰ç¼ºé™·çš„ä»£ç æºç ï¼Œfunc_afteræ˜¯ä¿®å¤åçš„æ²¡ç¼ºé™·çš„ä»£ç ï¼Œvulæ˜¯å½“å‰å‡½æ•°æ˜¯å¦å­˜åœ¨ç¼ºé™·ï¼Œ0è¡¨ç¤ºæ— ç¼ºé™·ï¼Œ1è¡¨ç¤ºæœ‰ç¼ºé™·

































1. å¤ç°VulBerta

è¿™ä¸ªæ¨¡å‹ç”¨äºæºä»£ç ç¼ºé™·æ£€æµ‹ï¼Œåœ¨RoBERTaçš„åŸºç¡€ä¸Šé’ˆå¯¹C/C++ä»£ç è¡¨ç¤ºå·¥ä½œè¿›è¡Œpre-trainå¾—åˆ°æ¨¡å‹VulBERTaï¼Œä¹‹ååœ¨VulBERTaçš„åŸºç¡€ä¸Šé’ˆå¯¹ç¼ºé™·æ£€æµ‹ä»»åŠ¡è¿›è¡Œfine-tuneå¾®è°ƒ

1.1 æ•°æ®é›†å’Œbaseline

**pretrainæ•°æ®é›†**

- GitHubï¼šä½œè€…è‡ªå·±åœ¨GitHubä¸Šçˆ¬çš„C/C++é¡¹ç›®ä»£ç 

- Draper VDISC Datasetï¼šä¸€ä¸ªå¼€æºçš„software vulnerability detection dataset

  æ•°æ®é›†å¯¹åº”è®ºæ–‡ä¸ºarXiv:1807.04320  Automated Vulnerability Detection in Source Code Using Deep Representation Learning

**fine-tuneæ•°æ®é›†**

- Vuldeepecker
- Draper
- REVEAL
- muVuldeepeckerï¼ˆMVDï¼‰
- Devign
- D2A

**baseline**

- BiLSTM

- TextCNN

  

2. source code representation

å¯¹æºä»£ç è¿›è¡Œè¿›ä¸€æ­¥æ‰‹åŠ¨ç‰¹å¾æå–ï¼Œå†æ”¾å…¥æ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒ

ä½¿ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œæºä»£ç ç¼ºé™·æ£€æµ‹çš„æ­¥éª¤ï¼š

1. å°†æºä»£ç è½¬åŒ–ä¸ºå‘é‡ â€”â€” source code representation

   ç°æœ‰æŠ€æœ¯æœ‰ï¼šå°†æºä»£ç è§†ä¸ºæ–‡æœ¬å¹¶ç›´æ¥è½¬åŒ–ä¸ºå‘é‡ï¼ˆword2vecï¼‰ã€å°†æºä»£ç è¡¨ç¤ºä¸ºASTå†è½¬åŒ–ä¸ºå‘é‡ï¼ˆï¼‰ã€å°†æºä»£ç è¿›è¡Œä¸€éƒ¨åˆ†ç¼–è¯‘å·¥ä½œå†å°†ç¼–è¯‘å¾—åˆ°çš„tokenè½¬åŒ–ä¸ºå‘é‡ï¼ˆæ¯”å¦‚ç”¨clangè¿›è¡Œparseç„¶åå†è¿›è¡Œencoderï¼‰

2. å°†å‘é‡ä½œä¸ºæ¨¡å‹è¾“å…¥ï¼Œè¿›è¡Œè®­ç»ƒ

## 2.2 è®­ç»ƒè¿‡ç¨‹

### 2.2.1 CodeBertFinetune

ç›´æ¥ä½¿ç”¨CodeBertæ¨¡å‹åœ¨CodeXGLUEæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒ

ä½¿ç”¨é»˜è®¤çš„Adamä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒæ—¶ï¼Œä¼šå‡ºç°è®­ç»ƒè¿‡ç¨‹ä¸­evalæ•°æ®é›†ä¸Šå‡†ç¡®ç‡å®Œå…¨ä¸å˜çš„æƒ…å†µï¼Œæ¢æˆSGDä¼˜åŒ–å™¨åå‡†ç¡®ç‡æ­£å¸¸ä¸Šå‡

åœ¨å¾®è°ƒCodeBERTæ—¶ï¼Œè®­ç»ƒ20epochåï¼Œtrain_losså’Œvalid_lossä¾ç„¶åœ¨åŸºäºä¸‹é™ï¼Œå¯èƒ½ä¾ç„¶æœªæ”¶æ•›

- ç›´æ¥å°†CodeXGLUEæ•°æ®é›†æ”¾å…¥CodeBertä¸­å¾®è°ƒ

  è®­ç»ƒè¿‡ç¨‹çš„è¾“å‡ºåœ¨ï¼š

  å‰åä¸ªepochï¼š`~/Documents/DefectDetection/codebertFinetune/CodebertFinetune_Craft_SGD_10_2024.06.27.output`

  ååä¸ªepochï¼š`~/Documents/DefectDetection/codebertFinetune/CodebertFinetune_Craft_SGD_20_2024.06.27.output`

  æœ€åå¾—åˆ°çš„accuracy=0.608

  æ¨¡å‹åœ¨ï¼š`~/Documents/DefectDetection/codebertFinetune/codebert_SGD_epoch17.pth`

- å°†CodeXGLUEæ•°æ®é›†ä¸­çš„æ³¨é‡Šå»æ‰ï¼Œå†è¿›è¡Œå¾®è°ƒ

  è®­ç»ƒè¿‡ç¨‹çš„è¾“å‡ºåœ¨ï¼š`~/Documents/DefectDetection/codebertFinetune/CodebertFinetune_Craft_SGD_20_2024.06.27.output/CBFT_Craft_SGD_nocomment_20_2024.06.28.output`

  æœ€åå¾—åˆ°çš„accuracy=0.581

