#  0. è®­ç»ƒæ³¨æ„äº‹é¡¹

## 0.1 æœåŠ¡å™¨ç½‘ç»œç¯å¢ƒ

ğŸ¤—åº“ä½¿ç”¨æ—¶éœ€è¦è¿æ¥huggingfaceæœåŠ¡å™¨åœ¨çº¿è¿è¡Œï¼ˆä¹Ÿå¯ä»¥æŠŠè¦ç”¨åˆ°çš„æ¨¡å‹æå‰åŠ è½½å¥½ä¿å­˜åœ¨æœ¬åœ°ï¼‰

huggingfaceæœåŠ¡å™¨åœ¨å¤–ç½‘ï¼Œè€Œè®­ç»ƒç”¨çš„æœåŠ¡å™¨å¯èƒ½æ— æ³•è¿æ¥è‡³å¤–ç½‘ï¼Œæ­¤æ—¶å¯ä»¥è®©è¿œç¨‹æœåŠ¡å™¨ä½¿ç”¨æœ¬æœºä»£ç†ï¼ˆä»…é™æ ¡å›­ç½‘ï¼‰

1. é¦–å…ˆåœ¨æœ¬æœºClash for Windowsä¸­æ‰“å¼€ Allow LAN

2. ä½¿ç”¨ifconfigå‘½ä»¤ï¼ˆwindowsä¸ºipconfigï¼‰æŸ¥çœ‹æœ¬æœºipåœ°å€

3. ä¹‹åå°±å¯ä»¥åœ¨ä½¿ç”¨å‘½ä»¤æ—¶æ·»åŠ ä»£ç†

   ```shell
   pip install xxx --proxy=ip:7890
   ```

4. ä¹Ÿå¯ä»¥è®¾ç½®å…¨å±€ä»£ç†

   åœ¨å½“å‰terminalä¸­è®¾ç½®å…¨å±€ä»£ç†

   ```shell
   export all_proxy="ip:7890"
   ```

## 0.2 åå°è¿è¡Œä»»åŠ¡



# 1. PyTorchä½¿ç”¨



# 1. Hugging faceåº“Transformerä½¿ç”¨

è¦ä½¿ç”¨ğŸ¤—Transformeråº“ï¼Œç¬¬ä¸€æ­¥éœ€è¦å®‰è£…

ç›´æ¥å®‰è£…ï¼š`pip install transormers`

å®‰è£…å¼€å‘ç‰ˆæœ¬ï¼ˆå¸¦æœ‰å‡ ä¹æ‰€æœ‰æ‰€éœ€çš„ä¾èµ–é¡¹ï¼‰:`pip install transformers[sentencepiece]`

## 1.1 åŸºæœ¬ä½¿ç”¨

Huggingfaceä¸­æœ‰å¾ˆå¤šåŸºäºTransformeræ¶æ„çš„ç°æˆæ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥æ‹¿æ¥å®Œæˆä¸€äº›NLPä»»åŠ¡

### 1.1.1 ç›´æ¥ä½¿ç”¨pipeline

Transformersåº“ä¸­æœ€åŸºæœ¬çš„å¯¹è±¡æ˜¯pipeline()å‡½æ•°ï¼Œé€šè¿‡pipelineå¯ä»¥ç›´æ¥åŠ è½½å¹¶ä½¿ç”¨ä¸€ä¸ªTrasformerå·²æœ‰çš„è®­ç»ƒå¥½çš„æ¨¡å‹

åŠ è½½ä»£ç ä¼šåœ¨æ¯ä¸ªåº“çš„Use this modelä¸­æ ‡æ˜

**pipelineä½¿ç”¨**

æ¯”å¦‚ä½¿ç”¨BERTæ¨¡å‹

```python
# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("fill-mask", model="google-bert/bert-base-cased")
pipe("Hello I'm a [MASK] model.")
outputs:
    [{'score': 0.17099590599536896,
  'token': 2959,
  'token_str': 'sorry',
  'sequence': "I'm sorry, who are you."},
 {'score': 0.12543228268623352,
  'token': 1303,
  'token_str': 'here',
  'sequence': "I'm here, who are you."},
 {'score': 0.03565218672156334,
  'token': 4853,
  'token_str': 'confused',
  'sequence': "I'm confused, who are you."},
 {'score': 0.018463322892785072,
  'token': 4107,
  'token_str': 'asking',
  'sequence': "I'm asking, who are you."},
 {'score': 0.01268229354172945,
  'token': 1128,
  'token_str': 'you',
  'sequence': "I'm you, who are you."}]
```

pipelineç¬¬ä¸€ä¸ªå‚æ•°ä¸­æŒ‡å®šå®Œæˆçš„ä»»åŠ¡ï¼ˆå®Œå½¢å¡«ç©ºï¼‰ï¼Œmodelå‚æ•°æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹ï¼ˆBERTï¼‰ï¼Œä¼šè¿”å›ä¸€ä¸ªæ¨¡å‹å¯¹è±¡

ä¹‹åç›´æ¥å°†è¦é¢„æµ‹å•è¯çš„å¥å­æ”¾å…¥pipeä¸­ï¼Œå³å¯å¾—åˆ°é¢„æµ‹ç»“æœã€‚

**pipelineå†…éƒ¨**

pipelineå®é™…ä¸Šæ˜¯å°†ä¸‰ä¸ªæ­¥éª¤ç»„åˆåœ¨äº†ä¸€èµ·ï¼š

1. å°†ä¼ å…¥çš„å­—ç¬¦ä¸²è¿›è¡ŒTokenizerï¼Œå³åˆ†è§£ä¸ºtokenï¼Œå¹¶è½¬åŒ–ä¸ºtensor
2. tokenizedä¹‹åçš„ç»“æœä¼ å…¥Modelä¸­è¿›è¡Œé¢„æµ‹
3. å°†Modelçš„è¾“å‡ºè¿›è¡ŒPost Porcessingåå¤„ç†ï¼Œä»¥äººç±»èƒ½ç†è§£çš„æ–¹å¼è¾“å‡º

è¿™ç§æ–¹å¼æ˜“äºä½¿ç”¨ï¼Œä½†æ˜¯åªèƒ½å®Œæˆç‰¹å®šä»»åŠ¡ï¼Œå¦‚æœæƒ³æ›´çµæ´»çš„ä½¿ç”¨æ¨¡å‹ï¼ŒTransformerä¹Ÿæä¾›äº†å¯¹Tokenizerå’Œmodelçš„è·å–

### 1.1.2 tokenizer

**åŸºæœ¬ç”¨æ³•tokenizers**

- åŠ è½½

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

- ä½¿ç”¨

```python
tokenizer("Using a Transformer network is simple")
outputs:
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

- ä¿å­˜

```python
tokenizer.save_pretrained("directory_on_my_computer")
```

**å°†å¥å­æ‹†åˆ†ä¸ºtoken**

ä¼šæ ¹æ®åŠ è½½çš„æ¨¡å‹çš„tokenizerè§„åˆ™è¿›è¡Œæ‹†åˆ†

```python
sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
outputs:
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

**ç¼–ç ï¼šå°†tokenè½¬åŒ–ä¸ºtensor**

è¿™é‡Œçš„ç¼–ç æ˜¯æ ¹æ®æ¨¡å‹ä¸­tokenizeré¢„å¤„ç†ç”Ÿæˆçš„è¯æ±‡è¡¨è½¬åŒ–çš„

```python
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
outputs:
    [7993, 170, 11303, 1200, 2443, 1110, 3014]
```

ä¹Ÿå¯ä»¥è¿›è¡Œè§£ç ï¼Œå³å°†è¯æ±‡è¡¨ç¼–å·è½¬åŒ–ä¸ºå­—ç¬¦ä¸²

```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
outputs:
    'Using a Transformer network is simple'
```



### 1.1.3 Model

**åŠ è½½æ¨¡å‹**

ä»Huggingfaceä¸­åŠ è½½åˆ«äººä¸Šä¼ çš„ä½¿ç”¨Transformeræ¶æ„çš„æ¨¡å‹

- ä½¿ç”¨é»˜è®¤é…ç½®åŠ è½½

  è¿™ç§æ–¹å¼åŠ è½½çš„æ¨¡å‹å‚æ•°æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œéœ€è¦é‡æ–°è®­ç»ƒ

  ```python
  from transformers import BertConfig, BertModel
  
  # Building the config
  config = BertConfig()
  
  # Building the model from the config
  model = BertModel(config)
  ```

- åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

  ```python
  from transformers import BertModel
  
  model = BertModel.from_pretrained("bert-base-cased")
  ```

  ä¹Ÿå¯ä»¥ä½¿ç”¨AutoModelæ›¿æ¢BertModelï¼ŒAutoModelä¼šæ ¹æ®åé¢çš„æ£€æŸ¥ç‚¹ï¼ˆå³bert-base-casedï¼‰è‡ªåŠ¨æ¨æ–­æ¨¡å‹ç±»å‹

  ```python
  from transformers import A
  
  model = AutoModel.from_pretrained("bert-base-cased")
  ```

**ä¿å­˜æ¨¡å‹**

å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜åœ¨æœ¬åœ°ç£ç›˜

```python
model.save_pretrained("directory_on_my_computer")
```

è¿™ä¼šä¿å­˜ä¸¤ä¸ªæ–‡ä»¶ï¼š

- config.jsonï¼šæ¨¡å‹é…ç½®ï¼Œä¿å­˜äº†æ„å»ºæ¨¡å‹ä½“ç³»ç»“æ„éœ€è¦çš„æ‰€æœ‰å±æ€§
- pytorch_model.binï¼šæ¨¡å‹çš„æ‰€æœ‰æƒé‡

**ä½¿ç”¨æ¨¡å‹**

å°†tokenizerçš„è¾“å‡ºä½œä¸ºè¾“å‡ºæ”¾å…¥æ¨¡å‹ä¸­ï¼Œå³å¯å¾—åˆ°ç»“æœ

- ç›´æ¥ä½¿ç”¨tokenizer

  å‚æ•°ï¼š

  - sequenceï¼šè¦å¤„ç†çš„å­—ç¬¦ä¸²ï¼ˆåºåˆ—ï¼‰ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ªå­—ç¬¦ä¸²æ•°ç»„

  - paddingï¼šå¼€å¯å¡«å……åŠŸèƒ½

    å¯é€‰å€¼ï¼š"longest"ï¼Œå¡«å……è‡³å­—ç¬¦åºåˆ—ä¸­æœ€é•¿å¥å­çš„é•¿åº¦

    "max_length"æˆ–è€…Trueï¼Œå¡«å……è‡³max_length

  - max_lengthï¼šæŒ‡å®šæœ€å¤§åºåˆ—çš„é•¿åº¦

    é»˜è®¤æ˜¯æ¨¡å‹æœ€å¤§é•¿åº¦ï¼Œæ¯”å¦‚BERTä¸º512

  - truncationï¼šTrueï¼Œé»˜è®¤ä¸ºFalse

    å°†åºåˆ—æˆªæ–­è‡³max_length

  - return_tensorsï¼šè¿”å›çš„å¼ é‡çš„ç±»å‹

    ptï¼šPyTorchå¼ é‡

    tfï¼šTensorFlowå¼ é‡

    npï¼šNumPyæ•°ç»„

```python
tokenized_inputs = tokenizer.tokenizer(sequence, padding="max_length", max_length=8, trucncation=True, return_tensors="pt")

model(tokenized_inputs)
```

- æ‰‹åŠ¨è¿›è¡Œåˆ†è¯å’Œç¼–ç 

  æ‰‹åŠ¨è¿›è¡Œåˆ†è¯å’Œç¼–ç åéœ€è¦æ‰‹åŠ¨å†æ·»åŠ ä¸€ä¸ªç»´åº¦ï¼Œå› ä¸ºæ¨¡å‹é»˜è®¤æƒ…å†µä¸‹éƒ½éœ€è¦å¤šä¸ªå¥å­ï¼Œè€Œè¯¥ä¾‹å­ä¸­åªæœ‰ä¸€ä¸ªå¥å­

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor([ids]) # æ³¨æ„è¿™é‡Œè¦æ‰‹åŠ¨æ·»åŠ ä¸€ä¸ªç»´åº¦
# This line will fail.
model(input_ids)
```

## 1.2 å¾®è°ƒæ¨¡å‹

### 1.2.1 æ•°æ®é¢„å¤„ç†

1. åŠ è½½æ•°æ®é›†

   å¯ä»¥ä»æ¨¡å‹ä¸­å¿ƒç›´æ¥åŠ è½½å·²æœ‰æ•°æ®é›†

   ```python
   from datasets import load_dataset
   
   raw_datasets = load_dataset("glue", "mrpc")
   raw_datasets
   
   outputs:
   DatasetDict({
       train: Dataset({
           features: ['sentence1', 'sentence2', 'label', 'idx'],
           num_rows: 3668
       })
       validation: Dataset({
           features: ['sentence1', 'sentence2', 'label', 'idx'],
           num_rows: 408
       })
       test: Dataset({
           features: ['sentence1', 'sentence2', 'label', 'idx'],
           num_rows: 1725
       })
   })
   ```

   ä¼šå¾—åˆ°ä¸€ä¸ªå­—å…¸ï¼Œå¯ä»¥é€šè¿‡é”®å¾—åˆ°æ¯éƒ¨åˆ†

   ```python
   raw_datasets["train"] # å¾—åˆ°è®­ç»ƒé›†
   raw_datasets["train"][0] # å¾—åˆ°è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªæ•°æ®
   raw_datasets["train"]["sentence1"] # å¾—åˆ°è®­ç»ƒé›†çš„æ‰€æœ‰æ•°æ®ä¸­çš„ç¬¬ä¸€ä¸ªå¥å­
   ```

2. å¯¹æ•°æ®é›†è¿›è¡Œtokenize

   æ–¹æ³•ä¸€ï¼šç›´æ¥è°ƒç”¨tokenizer

   ç¼ºç‚¹æ˜¯éœ€è¦åˆ†åˆ«ç»™è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®é›†ç¼–ç ï¼Œå¹¶ä¸”å¾—åˆ°çš„ç»“æœæ˜¯å­—å…¸ï¼Œä¸å®œç”¨

   ```python
   checkpoint = "bert-base-uncased"
   tokenizer = AutoTokenizer.from_pretrained(checkpoint)
   
   tokenized_dataset = tokenizer(
       raw_datasets["train"]["sentence1"], # è¿™å°†ä¼šè·å–è®­ç»ƒé›†çš„æ‰€æœ‰ç¬¬ä¸€ä¸ªå¥å­
       raw_datasets["train"]["sentence2"], # è¿™å°†ä¼šè·å–è®­ç»ƒé›†çš„æ‰€æœ‰ç¬¬äºŒä¸ªå¥å­
       padding=True,
       truncation=True,
   )
   ä¼šå¾—åˆ°ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºï¼šè¾“å…¥è¯ï¼ˆinput_idsï¼Œå³ç¼–ç åçš„å¥å­ï¼‰ã€attention_maskå’Œtoken_type_idsï¼ˆç”¨äºæ ‡è®°è¿™æ˜¯ç¬¬ä¸€ä¸ªå¥å­è¿˜æ˜¯ç¬¬äºŒä¸ªå¥å­ï¼‰
   ```

   æ–¹æ³•äºŒï¼šä½¿ç”¨mapå‡½æ•°ç»™æ•°æ®é›†ä¸­æ¯ä¸ªæ•°æ®éƒ½åº”ç”¨ä¸€ä¸ªtokenizeå‡½æ•°

   mapå‡½æ•°ä¼šåœ¨åŸæ•°æ®é›†çš„åŸºç¡€ä¸Šï¼Œç»™æ¯æ¡æ•°æ®ä¸­æ·»åŠ tokenizeåçš„ç¼–ç ç»“æœï¼ˆå³ç»™æ¯æ¡æ•°æ®å¤šæ·»åŠ äº†input_ids, attention_mask, token_type_idsä¸‰ä¸ªå­—æ®µï¼‰

   ```python
   def tokenize_function(example): # ç»™æ¯ä¸ªæ•°æ®åº”ç”¨çš„å‡½æ•°
       return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
   
   tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
   tokenized_datasets
   
   outputsï¼š
   DatasetDict({
       train: Dataset({
           features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
           num_rows: 3668
       })
       validation: Dataset({
           features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
           num_rows: 408
       })
       test: Dataset({
           features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
           num_rows: 1725
       })
   })
   ```

3. åŠ¨æ€å¡«å……

   åœ¨ä¸Šé¢tokenizeæ—¶å¹¶æ²¡æœ‰æ·»åŠ paddingå‚æ•°ï¼Œè¿™æ˜¯å› ä¸ºåœ¨tokenizeæ—¶å°†æ‰€æœ‰æ ·æœ¬å¡«å……åˆ°æœ€å¤§é•¿åº¦æ•ˆç‡ä¸é«˜ã€‚è½¬è€Œæˆ‘ä»¬åœ¨æ„å»ºæ‰¹å¤„ç†ï¼ˆbatchï¼‰æ—¶å¡«å……æ ·æœ¬ï¼Œè¿™æ ·çš„è¯åªéœ€è¦å¡«å……åˆ°è¯¥batchä¸­çš„æœ€å¤§é•¿åº¦

   ğŸ¤—transformeråº“ä¸­æä¾›LDataCollatorWithPaddingå‡½æ•°

   ```python
   from transformers import DataCollatorWithPadding
   
   # åˆ›å»ºå¯¹è±¡éœ€è¦ä¸€ä¸ªtokenizerå¯¹è±¡ï¼Œä»¥ä¾¿çŸ¥é“å¡«å……è§„åˆ™ï¼ˆç”¨å“ªä¸ªè¯å¡«å……ï¼Œå¡«å……åœ¨å‰é¢è¿˜æ˜¯åé¢ï¼‰
   data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   
   samples = tokenized_datasets["train"][:8] # å‡è®¾å–å‡ºå‰8ä¸ªæ ·æœ¬ä½œä¸ºä¸€ç»„
   batch = data_collator(samples) # å¯¹è¯¥ç»„æ ·æœ¬è¿›è¡Œå¡«å……
   ```

### 1.2.2 ä½¿ç”¨Trainerç±»å¾®è°ƒæ¨¡å‹

ğŸ¤—Transformersæä¾›äº†ä¸€ä¸ªTrainerç±»ï¼Œå¯ä»¥å¾®è°ƒä»»ä½•é¢„è®­ç»ƒæ¨¡å‹

1. é¦–å…ˆè¦å®šä¹‰å¥½æ¨¡å‹å’Œé¢„å¤„ç†å¥½æ•°æ®é›†

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

2. å®šä¹‰ä¸€ä¸ªTrainingArgumentsç±»

   è¯¥ç±»åŒ…å«Trainerç”¨äºè®­ç»ƒå’Œè¯„ä¼°çš„æ‰€æœ‰è¶…å‚æ•°

   ```python
   from transformers import TrainingArguments
   
   training_args = TrainingArguments(output_dir="./", # è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ä¿å­˜åœ°å€
                                    evaluation_strategy="epoch", # è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯„ä¼°ç­–ç•¥ï¼Œéš”å¤šä¹…è¯„ä¼°ä¸€æ¬¡
                                     logging_dir="./", # æ—¥å¿—æ–‡ä»¶å¤¹
                                     logging_steps=100 # è®­ç»ƒå¤šå°‘å¸ƒä¿å­˜y
                                    )
   ```

3. å®šä¹‰æ¨¡å‹

   ç”±äºåŸæ¨¡å‹BERTå¹¶æ²¡æœ‰å®šä¹‰å¥å­åˆ†ç±»çš„ä»»åŠ¡ï¼Œæ‰€ä»¥AutoModelForSequenceClassificationå®é™…ä¸Šæ˜¯ç»™BERTåé¢æ–°åŠ ä¸€ä¸ªå…¨é“¾æ¥å±‚ç”¨äºåˆ†ç±»ä»»åŠ¡

   ```python
   from transformers import AutoModelForSequenceClassification
   
   model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
   ```

4. å®šä¹‰Trainerå¯¹è±¡

   ```python
   from transformers import Trainer
   
   trainer = Trainer(
       model,
       training_args,
       train_dataset=tokenized_datasets["train"],
       eval_dataset=tokenized_datasets["validation"],
       data_collator=data_collator,
       tokenizer=tokenizer,
   )
   ```

5. å¼€å§‹è®­ç»ƒ

   ```python
   trainer.train()
   ```

### 1.2.3 è¯„ä¼°æ¨¡å‹

**predictæ–¹æ³•**

trainerç±»ä¸­æä¾›äº†ä¸€ä¸ªè¯„ä¼°æ¨¡å‹çš„æ–¹æ³•predictï¼Œéœ€è¦ä¼ å…¥ä¸€ä¸ªæµ‹è¯•é›†ï¼Œè¿”å›ä¸€ä¸ªæœ‰ä¸‰ä¸ªå­—æ®µçš„å‘½åå…ƒç¥–

- predictionsï¼šæµ‹è¯•ç»“æœï¼Œç»™å‡ºçš„æ˜¯æ¯ä¸ªç­”æ¡ˆçš„æ¦‚ç‡æ•°ç»„ï¼Œä½¿ç”¨æ—¶éœ€è¦åœ¨ç¬¬äºŒä¸ªè½´ä¸Šå–æœ€å¤§å€¼ç´¢å¼•
- label_idsï¼šæµ‹è¯•é›†çš„æ ‡ç­¾ï¼Œå³æ­£ç¡®ç»“æœ
- metricsï¼šåŒ…å«æ•°æ®é›†çš„lossã€è¿è¡Œæ—¶é—´ç­‰ä¿¡æ¯

```python
predictions = trainer.predict(tokenized_datasets["validation"])
predictions.predictions
predictions.label_ids
predictions.metrics
```

**evaluateåº“**

evaluateåº“å¯ä»¥ç›´æ¥åŠ è½½æ•°æ®é›†å¯¹åº”çš„è¯„ä»·æŒ‡æ ‡ï¼Œå¹¶æä¾›äº†æ–¹æ³•å¯¹è¿™äº›æŒ‡æ ‡è¿›è¡Œè®¡ç®—

```python
import evaluate

metric = evaluate.load("glue", "mrpc") # åŠ è½½è¯„ä»·æŒ‡æ ‡
metric.compute(predictions=preds, references=predictions.label_ids) # ä¼ å…¥é¢„æµ‹ç»“æœå’ŒçœŸå®ç»“æœï¼Œè®¡ç®—æŒ‡æ ‡
```

**compute_metric()å‡½æ•°**

compute_metric()å‡½æ•°å¯ä»¥ä½œä¸ºå‚æ•°åˆ›å»ºtrainerå¯¹è±¡ï¼Œè¿™æ ·åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°±ä¼šæ¯éš”ä¸€å®šepochå°±è¯„ä¼°ä¸€ä¸‹æ¨¡å‹è®­ç»ƒç»“æœ

```python
import evaluate

def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc") # åŠ è½½è¯„ä»·æŒ‡æ ‡
    logits, labels = eval_preds # eval_predsæ˜¯ä½¿ç”¨trainer.predictçš„ç»“æœï¼Œlogitsæ˜¯é¢„æµ‹ç»“æœï¼Œlabelsæ˜¯æ­£ç¡®ç»“æœ
    predictions = np.argmax(logits, axis=-1) # å¤„ç†é¢„æµ‹ç»“æœ
    return metric.compute(predictions=predictions, references=labels) # è®¡ç®—æŒ‡æ ‡
```

æœ€åç”¨compute_metric()å‡½æ•°ä½œä¸ºå‚æ•°åˆ›å»ºtrainerå¯¹è±¡ï¼Œå³å¯å¾—åˆ°æ¯ä¸ªè®­ç»ƒé˜¶æ®µæ¨¡å‹çš„æŒ‡æ ‡

```python
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model, # æ¨¡å‹
    training_args, # è®­ç»ƒè¶…å‚æ•°
    train_dataset=tokenized_datasets["train"], # æŒ‡å®šè®­ç»ƒé›†
    eval_dataset=tokenized_datasets["validation"], # æŒ‡å®šæµ‹è¯•é›†
    data_collator=data_collator, # åŠ¨æ€å¡«å……
    tokenizer=tokenizer, # ç¼–ç å™¨
    compute_metrics=compute_metrics, # è¯„ä¼°å‡½æ•°
)
```

### 1.2.4 æ‰‹åŠ¨å¾®è°ƒæ¨¡å‹

æ‰‹åŠ¨å¾®è°ƒæ¨¡å‹æ˜¯æŒ‡ä¸ç”¨Trainerç±»çš„æƒ…å†µä¸‹è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè½¬è€Œæˆ‘ä»¬è¦ä½¿ç”¨pytorchæ¥å®ç°è¿™ä¸ªè®­ç»ƒè¿‡ç¨‹

1. é¢„å¤„ç†æ•°æ®é›†

   ```python
   from datasets import load_dataset
   from transformers import AutoTokenizer, DataCollatorWithPadding
   
   raw_datasets = load_dataset("glue", "mrpc")
   checkpoint = "bert-base-uncased"
   tokenizer = AutoTokenizer.from_pretrained(checkpoint)
   
   
   def tokenize_function(example):
       return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
   
   
   tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
   data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

2. å¯¹tokenized_datasetsåšä¸€äº›å¤„ç†

   - åˆ é™¤ä¸€äº›æ¨¡å‹ä¸éœ€è¦çš„åˆ—ï¼ˆå¦‚sentence1å’Œsentence2åˆ—ï¼‰

     ```python
     tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
     ```

   - å°†åˆ—ålabelé‡å‘½åä¸ºlabelsï¼ˆæ¨¡å‹éœ€è¦ï¼‰

     ```python
     tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
     ```

   - å°†æ•°æ®é›†æ ¼å¼æ”¹ä¸ºPyTorchå¼ é‡

     ```python
     tokenized_datasets.set_format("torch")
     ```

   ä¿®æ”¹å®Œä¹‹åæ£€æŸ¥ä¸€ä¸‹æ•°æ®é›†ä¸­çš„åˆ—

   ```python'
   tokenized_datasets["train"].column_names
   outputs:
   ["attention_mask", "input_ids", "labels", "token_type_ids"]
   ```

3. å®šä¹‰æ•°æ®åŠ è½½å™¨DataLoader

   ```python
   from torch.utils.data import DataLoader
   
   train_dataloader = DataLoader(
       tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
   )
   eval_dataloader = DataLoader(
       tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
   )
   ```

   å¯ä»¥æ£€éªŒå…¶ä¸­ä¸€ä¸ªbatchçœ‹æ•°æ®å¤„ç†ä¸­æœ‰æ²¡æœ‰é”™è¯¯

   ```python
   for batch in train_dataloader:
       break
   {k: v.shape for k, v in batch.items()}
   ```

4. åŠ è½½æ¨¡å‹

   ```python
   from transformers import AutoModelForSequenceClassification
   
   model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
   ```

   ä¼ é€’ä¸€ä¸ªbatchçš„æ•°æ®æµ‹è¯•ä¸€ä¸‹

   ```python
   outputs = model(**batch)
   print(outputs.loss, outputs.logits.shape)
   ```

5. åŠ è½½ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨

   ä¼˜åŒ–å™¨ä½¿ç”¨AdamW

   ```python
   from transformers import AdamW
   from transformers import get_scheduler
   
   optimizer = AdamW(model.parameters(), lr=5e-5)
   
   num_epochs = 3
   num_training_steps = num_epochs * len(train_dataloader)
   lr_scheduler = get_scheduler(
       "linear",
       optimizer=optimizer,
       num_warmup_steps=0,
       num_training_steps=num_training_steps,
   )
   ```

6. ä½¿ç”¨GPU

   ```python
   import torch
   
   device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
   model.to(device)
   ```

7. è®­ç»ƒå¾ªç¯

   tqdmåº“ç”¨äºæ‰“å°è®­ç»ƒè¿›åº¦æ¡

   ```python
   from tqdm.auto import tqdm
   
   progress_bar = tqdm(range(num_training_steps))
   
   model.train()
   for epoch in range(num_epochs):
       for batch in train_dataloader:
           batch = {k: v.to(device) for k, v in batch.items()}
           outputs = model(**batch)
           loss = outputs.loss
           loss.backward()
   
           optimizer.step()
           lr_scheduler.step()
           optimizer.zero_grad()
           progress_bar.update(1)
   ```

8. è¯„ä¼°æ¨¡å‹ç»“æœ

   ```python
   import evaluate
   
   metric = evaluate.load("glue", "mrpc")
   model.eval()
   for batch in eval_dataloader:
       batch = {k: v.to(device) for k, v in batch.items()}
       with torch.no_grad():
           outputs = model(**batch)
   
       logits = outputs.logits
       predictions = torch.argmax(logits, dim=-1)
       metric.add_batch(predictions=predictions, references=batch["labels"])
   
   metric.compute()
   ```

## 1.3 DATASETSåº“

### 1.3.1 åŠ è½½æœ¬åœ°æ•°æ®é›†

ğŸ¤—DATASETSæ”¯æŒçš„æ•°æ®é›†æ ¼å¼ï¼š

| Data format        | Loading script | Example                                                 |
| ------------------ | -------------- | ------------------------------------------------------- |
| CSV & TSV          | `csv`          | `load_dataset("csv", data_files="my_file.csv")`         |
| Text files         | `text`         | `load_dataset("text", data_files="my_file.txt")`        |
| JSON & JSON Lines  | `json`         | `load_dataset("json", data_files="my_file.jsonl")`      |
| Pickled DataFrames | `pandas`       | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

**åŠ è½½æ•°æ®é›†**

åŒæ—¶åŠ è½½è®­ç»ƒæ•°æ®é›†å’Œæµ‹è¯•æ•°æ®é›†

```python
from datasets import load_dataset

data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

### 1.3.2 DatasetBuilderç±»

åŠ è½½æ•°æ®é›†çš„load_datasetæ–¹æ³•æ˜¯é€šè¿‡DatasetBuilderç±»å®ç°çš„ï¼Œå¦‚æœè¦å°†è‡ªå·±çš„æ•°æ®é›†ä¸Šä¼ åˆ°HuggingFaceåº“æˆ–è€…ä¿®æ”¹åº“ä¸­å·²æœ‰çš„æ•°æ®é›†æ—¶ï¼Œéƒ½è¦ç»§æ‰¿è¿™ä¸ªç±»

ç±»ä¸­åªè¦åŒ…æ‹¬ä¸‰ä¸ªæ–¹æ³•ï¼š

1. infoï¼šæ•°æ®é›†çš„ä¿¡æ¯
2. split_generatorï¼šè´Ÿè´£ä»ç¡¬ç›˜æˆ–è€…ç½‘ç›˜ä¸­è¯»å…¥/ä¸‹è½½æ•°æ®ï¼Œå¹¶å°†æ•°æ®æŒ‰ç…§æ¯”ä¾‹åˆ†ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
3. generate_examplesï¼šè¯»å–æ•°æ®é›†ï¼Œå¹¶åœ¨è°ƒç”¨æ˜¯ç»™å‡ºæ•°æ®é¡¹

## 1.4 TOKENIZERSåº“



# 2. è®ºæ–‡

## 2.1 æ•°æ®é›†

### 2.1.1 Draper

- è®ºæ–‡é¢˜ç›®ï¼šAutomated Vulnerability Detection in Source Code Using Deep Representation Learning

- åœ°å€ï¼š`/home/sazer/Documents/DefectDetection/datasets/finetune/draper`

- æ•°æ®é›†æè¿°ï¼š

  æ•°æ®é›†å°±æ˜¯æºä»£ç +æ ‡ç­¾ï¼Œæœ‰æ³¨é‡Š

  ç”±SATE IV Juliet Test Suiteã€Debian Linux distributionã€public Git repositoriesç»„æˆï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå°±æ˜¯CWEäººé€ æ•°æ®é›†é›†åˆï¼Œå‰©ä½™ä¸¤ä¸ªæ˜¯çœŸå®ä¸–ç•Œæ•°æ®é›†

### 2.1.2 Vuldeepecker

- è®ºæ–‡é¢˜ç›®ï¼šVulDeePecker: A Deep Learning-Based System for Vulnerability Detection

- åœ°å€ï¼š`/home/sazer/Documents/DefectDetection/datasets/VulDeePecker`

- æ•°æ®é›†æè¿°ï¼š

  äººé€ æ•°æ®é›†ï¼Œç”±cwe-119å’Œcwe-399ç»„æˆï¼Œcweç³»åˆ—éƒ½æ˜¯äººé€ æ•°æ®é›†ï¼Œå¹¶ä¸”æœ‰é‡å¤æƒ…å†µï¼ˆæµ‹è¯•é›†å’Œè®­ç»ƒé›†æœ‰é‡å¤ï¼Œä¼šé€ æˆæ•°æ®æ³„éœ²ï¼‰

### 2.1.3 TransferRepresentationLearning

- è®ºæ–‡é¢˜ç›®ï¼šCross-Project Transfer Representation Learning for Vulnerable Function Discovery

- åœ°å€ï¼š

  `/home/sazer/Documents/DefectDetection/datasets/TransferRepresentationLearning/Data/VulnerabilityData`

- æ•°æ®é›†æè¿°ï¼šè¿™ä¸ªæ•°æ®é›†ä¸ºå‡½æ•°çº§ï¼Œä¸€ä¸ªå‡½æ•°ä¸€ä¸ªæ–‡ä»¶ï¼Œå…¶ä¸­æœ‰ç¼ºé™·çš„å‡½æ•°æ–‡ä»¶åä»¥cwe-å¼€å¤´ã€‚æœ‰äº›å‡½æ•°ä¸­åŒ…æ‹¬æ³¨é‡Š

  çœŸå®æ•°æ®é›†ï¼Œä½¿ç”¨å¼€æºé¡¹ç›®è¿›è¡Œæ‰‹åŠ¨æ•°æ®æ ‡æ³¨

### 2.1.4 REVEAL

- è®ºæ–‡é¢˜ç›®ï¼šDeep Learning based Vulnerability Detection: Are We There Yet? ï¼ˆè®ºæ–‡ä¸­ç»™å‡ºçš„githubåº“ä¸­æä¾›çš„æ•°æ®å’Œæ¨¡å‹ä¸‹è½½è¿æ¥å‡å·²å¤±æ•ˆï¼‰

- æœ¬åœ°ä¿å­˜åœ°å€ï¼š`/home/sazer/Documents/DefectDetection/datasets/finetune/reveal`

- æ•°æ®é›†æè¿°ï¼š

  æ•°æ®é›†ä¸ºä»£ç ï¼ˆå‡½æ•°çº§ï¼‰å’Œå¯¹åº”æ ‡ç­¾ï¼Œæ²¡æœ‰æ³¨é‡Š

  çœŸå®ä¸–ç•Œæ•°æ®é›†ï¼Œæ¥è‡ªChromiumå’ŒDebiané¡¹ç›®

### 2.1.5 d2a

**åŸºæœ¬ä¿¡æ¯**

- è®ºæ–‡é¢˜ç›®ï¼šD2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis

- æœ¬åœ°ä¿å­˜åœ°å€ï¼š`/home/sazer/Documents/DefectDetection/datasets/finetune/d2a`

- æ•°æ®é›†æè¿°ï¼š

- æ•°æ®é›†æ ¼å¼ï¼š.csv

  æ–‡ä»¶ä»¥id,label,codeçš„å½¢å¼ç»™å‡ºï¼Œç¬¬ä¸€ä¸ªä¾‹å­å¦‚ä¸‹

  ```c++
  id,label,code
  1,0,"static void srt_to_ass(AVCodecContext *avctx, AVBPrint *dst,
                         const char *in, int x1, int y1, int x2, int y2)
  {
      if (x1 >= 0 && y1 >= 0) {
          /* XXX: here we rescale coordinate assuming they are in DVD resolution
           * (720x480) since we don't have anything better */
  
          if (x2 >= 0 && y2 >= 0 && (x2 != x1 || y2 != y1) && x2 >= x1 && y2 >= y1) {
              /* text rectangle defined, write the text at the center of the rectangle */
              const int cx = x1 + (x2 - x1)/2;
              const int cy = y1 + (y2 - y1)/2;
              const int scaled_x = cx * (int64_t)ASS_DEFAULT_PLAYRESX / 720;
              const int scaled_y = cy * (int64_t)ASS_DEFAULT_PLAYRESY / 480;
              av_bprintf(dst, ""{\\an5}{\\pos(%d,%d)}"", scaled_x, scaled_y);
          } else {
              /* only the top left corner, assume the text starts in that corner */
              const int scaled_x = x1 * (int64_t)ASS_DEFAULT_PLAYRESX / 720;
              const int scaled_y = y1 * (int64_t)ASS_DEFAULT_PLAYRESY / 480;
              av_bprintf(dst, ""{\\an1}{\\pos(%d,%d)}"", scaled_x, scaled_y);
          }
      }
  
      ff_htmlmarkup_to_ass(avctx, dst, in);
  }"
  ```

  

# éœ€è¦åšçš„äº‹

æŠŠğŸ¤—Transformeråº“çš„æ¨¡å‹å’Œtokenizerä¿å­˜åœ¨æœ¬åœ°ï¼Œåœ¨è¶…ç®—å¹³å°ä¸­ä½¿ç”¨

æ‰¾åˆ°åˆé€‚çš„æ•°æ®é›†è¿›è¡Œä¸€æ¬¡å¾®è°ƒ































1. å¤ç°VulBerta

è¿™ä¸ªæ¨¡å‹ç”¨äºæºä»£ç ç¼ºé™·æ£€æµ‹ï¼Œåœ¨RoBERTaçš„åŸºç¡€ä¸Šé’ˆå¯¹C/C++ä»£ç è¡¨ç¤ºå·¥ä½œè¿›è¡Œpre-trainå¾—åˆ°æ¨¡å‹VulBERTaï¼Œä¹‹ååœ¨VulBERTaçš„åŸºç¡€ä¸Šé’ˆå¯¹ç¼ºé™·æ£€æµ‹ä»»åŠ¡è¿›è¡Œfine-tuneå¾®è°ƒ

1.1 æ•°æ®é›†å’Œbaseline

**pretrainæ•°æ®é›†**

- GitHubï¼šä½œè€…è‡ªå·±åœ¨GitHubä¸Šçˆ¬çš„C/C++é¡¹ç›®ä»£ç 

- Draper VDISC Datasetï¼šä¸€ä¸ªå¼€æºçš„software vulnerability detection dataset

  æ•°æ®é›†å¯¹åº”è®ºæ–‡ä¸ºarXiv:1807.04320  Automated Vulnerability Detection in Source Code Using Deep Representation Learning

**fine-tuneæ•°æ®é›†**

- Vuldeepecker
- Draper
- REVEAL
- muVuldeepeckerï¼ˆMVDï¼‰
- Devign
- D2A

**baseline**

- BiLSTM

- TextCNN

  

2. source code representation

å¯¹æºä»£ç è¿›è¡Œè¿›ä¸€æ­¥æ‰‹åŠ¨ç‰¹å¾æå–ï¼Œå†æ”¾å…¥æ¨¡å‹ä¸­è¿›è¡Œè®­ç»ƒ

ä½¿ç”¨æ·±åº¦å­¦ä¹ è¿›è¡Œæºä»£ç ç¼ºé™·æ£€æµ‹çš„æ­¥éª¤ï¼š

1. å°†æºä»£ç è½¬åŒ–ä¸ºå‘é‡ â€”â€” source code representation

   ç°æœ‰æŠ€æœ¯æœ‰ï¼šå°†æºä»£ç è§†ä¸ºæ–‡æœ¬å¹¶ç›´æ¥è½¬åŒ–ä¸ºå‘é‡ï¼ˆword2vecï¼‰ã€å°†æºä»£ç è¡¨ç¤ºä¸ºASTå†è½¬åŒ–ä¸ºå‘é‡ï¼ˆï¼‰ã€å°†æºä»£ç è¿›è¡Œä¸€éƒ¨åˆ†ç¼–è¯‘å·¥ä½œå†å°†ç¼–è¯‘å¾—åˆ°çš„tokenè½¬åŒ–ä¸ºå‘é‡ï¼ˆæ¯”å¦‚ç”¨clangè¿›è¡Œparseç„¶åå†è¿›è¡Œencoderï¼‰

2. å°†å‘é‡ä½œä¸ºæ¨¡å‹è¾“å…¥ï¼Œè¿›è¡Œè®­ç»ƒ



