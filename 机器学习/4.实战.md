#  0. 训练注意事项

## 0.1 服务器网络环境

🤗库使用时需要连接huggingface服务器在线运行（也可以把要用到的模型提前加载好保存在本地）

huggingface服务器在外网，而训练用的服务器可能无法连接至外网，此时可以让远程服务器使用本机代理（仅限校园网）

1. 首先在本机Clash for Windows中打开 Allow LAN

2. 使用ifconfig命令（windows为ipconfig）查看本机ip地址

3. 之后就可以在使用命令时添加代理

   ```shell
   pip install xxx --proxy=ip:7890
   ```

4. 也可以设置全局代理

   在当前terminal中设置全局代理

   ```shell
   export all_proxy="ip:7890"
   ```

## 0.2 后台运行任务



# 1. PyTorch使用

可以通过`dir()`来查看python的某个库的组成，或者说列举某个库的所有子库

通过`help()`来查看某个库的使用帮助

```python
dir(torch)
dir(torch.cuda)
help(torch.cuda)
```

## 1.1 操作数据

PyTorch中提供的数据操作类有两个：

**Dataset**：用于读取数据文件，从原始数据文件中获取数据及其label

**Dataloader**：将Dataset中的数据进行打包分组等操作，以方便地提供给后面的神经网络使用

### 1.1.1 Dataset

自定义的加载数据集类都需要继承Dataset类

需要重写的函数

- \__getitem__：获取数据集中指定下标index的一条数据及其对应label

  ```python
  def __getitem__(self, index):
      ###
      return data, label
  ```

- \__len__：获取数据集大小

  ```python
  def __len__(self):
  ```

- \__add__：添加数据

  ```python
  def __add__(self, )
  ```

### 1.1.2 Dataloader

DataLoader类是数据读取器，每次读取指定batch_size大小的数据作为模型训练一次的输入 

**创建DataLoader**

```python
data_loader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=0, drop_last=True)
```

**使用DataLoader**

```python
for _ in range(epoch):
    for data in data_loader:
        datas, labels = data
```

使用for循环从data_loader中读取数据，每次取出batch_size条数据，相当于调用batch_size次\__getitem__方法

取出的四条数据和标签，会将4条数据本身打包成数组，作为datas返回，将4个标签打包成一个数组，作为labels返回。

每次epoch都会将所有数据读取一遍后结束

**参数**

- dataset：从哪个dataset对象中读取数据

- batch_size：每次读取几条数据
- shuffle：每个epoch中，每个batch_size得到的数据是否相同

- drop_last：当数据集中最后剩下的数据不足batch_size时，是否丢掉

## 1.2 构建模型

模型相关内容在torch.nn包中

### 1.2.1 nn.Module

所有神经网络的base class，也就是说所有神经网络都要继承nn.Module类

在自定义Model类中，应该重写\__init__方法和forward方法

- \__init__：用于初始化模型对象，应该调用`super().__init__`

- forward：给定模型输入x，得到模型的输出outputs

```python
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
    
# 使用：
model = Model()
input = torch.ones((10, 3, 32, 32)) # 创建好模型之后一般使用一个虚拟数据来检查模型是否正确，比如m10张 3通道 32x32 大小的图片
output = model(input)
```



### 1.2.2 修改已有模型

修改模型结构，比如添加或者删除一些层。可以使用add_module修改已有模型

先直接使用print打印模型信息，然后在指定位置进行添加或者修改

- 添加一个全链接层

```python
vgg_16 = torchvision.models.vgg16(pretrained=True)

# 在vgg16模型最后添加一个名为add_linear的线性层
vgg16.add_module('add_linear', nn.Linear(1000, 10))

# 也可以在模型的某个模块最后加一个线性层
vgg_16.classifier.add_module('add_linear', nn.Linear(1000, 10)) # 在classifier模块最后加一个线性层
```

- 修改某一层

```python
vgg16.classifier[6] = nn.Linear(4096. 10) # 修改模型classifier模块第7层的大小
```



### 1.2.3 模型保存

有两种方式保存模型

1. 保存完整模型+参数

    model是已经创建好的模型，后面是要保存的文件名称

    ```python
    torch.save(model, "model.pth")
    ```

    这种方式保存可以直接通过文件获取模型

    陷阱：其实加载时还需要有模型的定义类，只是不需要创建模型对象了

    ```python
    model = torch.load("model.pth")
    ```

2. 只保存模型参数（官方推荐）

    参数以字典的形式保存，所以叫state_dict

    ```python
    torch.save(model.state_dict(), "model.pth")
    ```

    这种方式加载模型需要提前准备好模型结构（即创建好模型对象），然后再使用模型对象从文件中加载参数

    ```python
    model = my_model()
    model.load_state_dict(torch.load("model.pth"))
    ```



## 1.3 训练过程

训练过程

1. 从数据集中取出一条数据放入模型中得到输出

2. 根据 模型得到的输出 和 标签（真实值），使用损失函数计算损失

3. 根据损失计算反向传播
4. 使用优化器根据反向传播结果更新模型参数

### 1.3.1 损失函数及反向传播

损失函数类定义在torch.nn库中

损失函数中一般传入output和target两个参数，来计算 计算值和真实值之间的差距，为反向传播的计算提供依据

```python
# 创建损失函数
loss = nn.CrossEntropyLoss()
for data,label in dataloader: # 取出一条数据及其对应标签（目标）
    outputs = model(data) # 将数据放入模型计算输出
    result_loss = loss(inputs, targets) # 计算损失
    result_loss.backward()
```

### 1.3.2 优化器

优化器在torch.optim库中

在计算完损失函数，并计算完反向传播得到优化方向之后，就需要调用优化器对模型参数进行优化

优化器需要的参数：

- params：模型参数，告诉优化器更新什么
- lr：学习率

- 不同优化器特定参数

```python
optim = torch.optim.SGD(model.parameters(), lr=0.01) # 指定优化器

for input, target in dataset:
    optimizer.zero_grad() # 防止上一轮的梯度对本轮计算造成影响
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optim.step()
```

### 1.3.3 模型评估

一般在每个epoch训练结束之后，使用训练数据集对模型进行一个评估，来得到一个损失值，帮助判断模型是否在正常训练

**损失值**

```python
total_test_loss = 0 # 将整个测试数据集上的所有loss累加起来
with torch.no_grad():
    for data,label in test_dataloader:
        outputs = model(data)
        loss = loss(outputs, label)
        total_test_loss = total_test_loss + loss.item()
print("loss：{}".format(total_test_loss))
```

**准确率**

对于分类问题特有的评价指标

准确率 = 预测正确的个数 / 训练集总数据条数

```python
total_accuracy = 0
with torch.no_grad():
    for data, label in test_dataloader:
        outputs = model(data)
        outputs = outputs.argmax(1) # 取出预测概率最大的那个类别作为本次预测的结果
        # 准确率等于正确的结果和总数据条数的比值
        accuracy = (outputs == target).sum()
        total_accuracy = total_accuracy + accuracy
    print("整体测试集上的正确率：{}".format(total_accuracy / len(test_dataloader)))
```

### 1.3.4 使用GPU训练

1. 获取GPU

   ```python
   device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
   ```

2. 将模型和数据等转移到GPU上

   ```python
   model.to(device) # 转移模型
   loss.to(device) # 转移损失函数
   
   # 注意转移数据时需要重新赋值
   data = data.to(device) # 转移训练数据
   label = label.to(device) # 转移标签
   ```

### 1.3.5 完整训练过程

1. 加载和处理数据集
   1. 加载数据集
   2. tokenize数据集
   3. 将数据集放入DataLoader中
   4. 测试：从DataLoader中取出一组数据查看数据维度

2. 搭建神经网络
   1. 创建Model类（继承nn.Module）
   2. 创建model对象
   3. 测试：将DataLoader中的一组数据放入模型中，看能否得到想要格式的输出

3. 创建损失函数
4. 创建优化器
5. 设置训练参数 
6. 开始训练
   1. 训练
   2. 评估

7. 模型评估（使用测试数据集）

```python
from transformers import AutoTokenizer, AutoModel,DataCollatorWithPadding
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd

# 1. 加载和处理数据集
## 1.1 加载数据集
from datasets import load_dataset

path = "/root/autodl-tmp/CodeXGLUE/tackled_{}.csv"
data_files = {"train":path.format("train"), "valid":path.format("valid"), "test":path.format("test")}
raw_dataset = load_dataset("csv", data_files=data_files)
## 1.2 tokenize数据集
tokenizer = AutoTokenizer.from_pretrained("./codebert_tokenizer")

def tokenize_function(example):
    return tokenizer(example["funcSource"], truncation=True)

tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)
tokenized_datasets = tokenized_datasets.remove_columns(["funcSource"])
tokenized_datasets.set_format("torch")
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
## 1.3 将数据集放入DataLoader中
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"],
    shuffle=True,
    batch_size=8,
    collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["valid"],
    batch_size=8,
    collate_fn=data_collator
)
test_dataloader = DataLoader(
    tokenized_datasets["test"],
    batch_size=8,
    collate_fn=data_collator
)
## 1.4 测试：从DataLoader中取出一组数据查看数据维度
for batch in train_dataloader:
    break
{k:v.shape for k, v in batch.items()}

# 2.搭建神经网络
## 2.1 创建Model类（继承nn.Module）
codebert_model = AutoModel.from_pretrained("./codebert_model")
class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.codebert = codebert_model
        self.linear = nn.Linear(768, 2)

    def forward(self, input_ids, attention_mask):
        output = self.codebert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state
        return self.linear(output[:,0,:])
## 2.2 创建model对象
model = Model()
## 2.3 测试：将DataLoader中的一组数据放入模型中，看能否得到想要格式的输出
for batch in train_dataloader:
    break
with torch.no_grad():
    output = model(batch['input_ids'], batch['attention_mask'])
output
# 3. 创建损失函数
loss_fn = nn.CrossEntropyLoss()
# 4. 创建优化器
from torch.optim import Adam,AdamW,SGD
optimizer = SGD(model.parameters(), lr=1e-4)
# 5. 设置训练参数
num_epochs = 10
num_training_steps = num_epochs * len(train_dataloader)
from tqdm.auto import tqdm # 训练进度条
progress_bar = tqdm(range(num_training_steps))

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

# 6. 开始训练
for epoch in range(num_epochs):
## 6.1 训练
    model.train()

    total_train_loss = 0
    for batch in train_dataloader:
        batch = {k:v.to(device) for k, v in batch.items()}
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']

        optimizer.zero_grad()
        output = model(input_ids, attention_mask)
        loss = loss_fn(output, labels)
        loss.backward()

        optimizer.step()

        total_train_loss += loss.item()
        progress_bar.update(1)
## 6.2 评估（eval数据集）
    model.eval()
    with torch.no_grad():
        for batch in eval_dataloader:
            batch = {k:v.to(device) for k, v in batch.items()}
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            outputs = model(input_ids, attention_mask)
            loss = loss_fn(outputs, labels)
            total_eval_loss += loss.item()
            
            predictions = outputs.argmax(dim=1)
            accuracy = (predictions == labels).sum().item()
            total_eval_accuracy += accuracy
            total_samples += labels.size(0)
            
    avg_eval_loss = total_eval_loss / len(eval_dataloader)
    avg_training_loss = total_train_loss / len(train_dataloader)
    
    avg_accuracy = total_eval_accuracy / total_samples
    print("----------------------")
    print(f"training_loss:{avg_training_loss},  validation_loss:{avg_eval_loss}")
    print(f"epoch_accuracy:{avg_accuracy}")
    
    torch.save(model.state_dict(), f"codebert_SGD_epoch{epoch + 10}")
    
# 7. 训练完成后进行模型评估（test数据集）
total_test_loss = 0
total_test_accuracy = 0
total_samples = 0
with torch.no_grad():
    model.eval()
    for batch in test_dataloader:
        batch = {k:v.to(device) for k, v in batch.items()}
        input_ids = batch['input_ids']
        attention_mask = batch['attention_mask']
        labels = batch['labels']
        
        outputs = model(input_ids, attention_mask)
        loss = loss_fn(outputs, labels)
        total_test_loss += loss.item()
        
        predictions = outputs.argmax(1)
        accuracy = (predictions == labels).sum().item()
        total_test_accuracy += accuracy
        total_samples += labels.size(0)
        
    avg_accuracy = total_test_accuracy / total_samples
    print(f"accuracy:{avg_accuracy}")
        
```

# 1. Hugging face库Transformer使用

要使用🤗Transformer库，第一步需要安装

直接安装：`pip install transormers`

安装开发版本（带有几乎所有所需的依赖项）:`pip install transformers[sentencepiece]`

## 1.1 基本使用

Huggingface中有很多基于Transformer架构的现成模型，可以直接拿来完成一些NLP任务

### 1.1.1 直接使用pipeline

Transformers库中最基本的对象是pipeline()函数，通过pipeline可以直接加载并使用一个Trasformer已有的训练好的模型

加载代码会在每个库的Use this model中标明

**pipeline使用**

比如使用BERT模型

```python
# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("fill-mask", model="google-bert/bert-base-cased")
pipe("Hello I'm a [MASK] model.")
outputs:
    [{'score': 0.17099590599536896,
  'token': 2959,
  'token_str': 'sorry',
  'sequence': "I'm sorry, who are you."},
 {'score': 0.12543228268623352,
  'token': 1303,
  'token_str': 'here',
  'sequence': "I'm here, who are you."},
 {'score': 0.03565218672156334,
  'token': 4853,
  'token_str': 'confused',
  'sequence': "I'm confused, who are you."},
 {'score': 0.018463322892785072,
  'token': 4107,
  'token_str': 'asking',
  'sequence': "I'm asking, who are you."},
 {'score': 0.01268229354172945,
  'token': 1128,
  'token_str': 'you',
  'sequence': "I'm you, who are you."}]
```

pipeline第一个参数中指定完成的任务（完形填空），model参数指定要使用的模型（BERT），会返回一个模型对象

之后直接将要预测单词的句子放入pipe中，即可得到预测结果。

**pipeline内部**

pipeline实际上是将三个步骤组合在了一起：

1. 将传入的字符串进行Tokenizer，即分解为token，并转化为tensor

2. tokenized之后的结果传入Model中进行预测

3. 将Model的输出进行Post Porcessing后处理，以人类能理解的方式输出

这种方式易于使用，但是只能完成特定任务，如果想更灵活的使用模型，Transformer也提供了对Tokenizer和model的获取

### 1.1.2 tokenizer

**基本用法tokenizers**

- 加载

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

- 使用

```python
tokenizer("Using a Transformer network is simple")
outputs:
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

- 保存

```python
tokenizer.save_pretrained("directory_on_my_computer")
```

**将句子拆分为token**

会根据加载的模型的tokenizer规则进行拆分

```python
sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
outputs:
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

**编码：将token转化为tensor**

这里的编码是根据模型中tokenizer预处理生成的词汇表转化的

```python
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
outputs:
    [7993, 170, 11303, 1200, 2443, 1110, 3014]
```

也可以进行解码，即将词汇表编号转化为字符串

```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
outputs:
    'Using a Transformer network is simple'
```

### 1.1.3 Model

**加载模型**

从Huggingface中加载别人上传的使用Transformer架构的模型

- 使用默认配置加载

  这种方式加载的模型参数是随机初始化的，需要重新训练

  ```python
  from transformers import BertConfig, BertModel
  
  # Building the config
  config = BertConfig()
  
  # Building the model from the config
  model = BertModel(config)
  ```

- 加载预训练模型

  ``` python
  from transformers import BertModel
  
  model = BertModel.from_pretrained("bert-base-cased")
  ```

  也可以使用AutoModel替换BertModel，AutoModel会根据后面的检查点（即bert-base-cased）自动推断模型类型

  ```python
  from transformers import AutoModel
  
  model = AutoModel.from_pretrained("bert-base-cased")
  ```

**保存模型**

将训练好的模型保存在本地磁盘

```python
model.save_pretrained("directory_on_my_computer")
```

这会保存两个文件：

- config.json：模型配置，保存了构建模型体系结构需要的所有属性

- pytorch_model.bin：模型的所有权重

**使用模型**

将tokenizer的输出作为输出放入模型中，即可得到结果

- 直接使用tokenizer

    参数：

    - sequence：要处理的字符串（序列），可以是一个，也可以是一个字符串数组

    - padding：开启填充功能

      可选值："longest"，填充至字符序列中最长句子的长度

      "max_length"或者True，填充至max_length

    - max_length：指定最大序列的长度

      默认是模型最大长度，比如BERT为512

    - truncation：True，默认为False

      将序列截断至max_length

    - return_tensors：返回的张量的类型

      pt：PyTorch张量

      tf：TensorFlow张量

      np：NumPy数组

```python
tokenized_inputs = tokenizer.tokenizer(sequence, padding="max_length", max_length=8, trucncation=True, return_tensors="pt")

model(tokenized_inputs)
```

- 手动进行分词和编码

手动进行分词和编码后需要手动再添加一个维度，因为模型默认情况下都需要多个句子，而该例子中只有一个句子

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor([ids]) # 注意这里要手动添加一个维度
# This line will fail.
model(input_ids)
```

## 1.2 微调模型

### 1.2.1 数据预处理

1. 加载数据集

   可以从模型中心直接加载已有数据集

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets

outputs:
    DatasetDict({
        train: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 3668
        })
        validation: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 408
        })
        test: Dataset({
            features: ['sentence1', 'sentence2', 'label', 'idx'],
            num_rows: 1725
        })
    })
```

​		会得到一个字典，可以通过键得到每部分

```python
raw_datasets["train"] # 得到训练集
raw_datasets["train"][0] # 得到训练集的第一个数据
raw_datasets["train"]["sentence1"] # 得到训练集的所有数据中的第一个句子
```

2. 对数据集进行tokenize

    方法一：直接调用tokenizer

    缺点是需要分别给训练、验证、测试数据集编码，并且得到的结果是字典，不宜用

    ```python
    checkpoint = "bert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    
    tokenized_dataset = tokenizer(
        raw_datasets["train"]["sentence1"], # 这将会获取训练集的所有第一个句子
        raw_datasets["train"]["sentence2"], # 这将会获取训练集的所有第二个句子
        padding=True,
        truncation=True,
    )
    会得到一个字典，键为：输入词（input_ids，即编码后的句子）、attention_mask和token_type_ids（用于标记这是第一个句子还是第二个句子）
    ```

    方法二：使用map函数给数据集中每个数据都应用一个tokenize函数

    map函数会在原数据集的基础上，给每条数据中添加tokenize后的编码结果（即给每条数据多添加了input_ids, attention_mask, token_type_ids三个字段）

    在map函数中，如果batched=True，则传入tokenize_function的参数example就是多个样本组成的列表，如果batche=False，则参数为单个样本

    ```python
    def tokenize_function(example): # 给每个数据应用的函数
        return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
    
    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
    tokenized_datasets
    
    outputs：
    DatasetDict({
        train: Dataset({
            features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
            num_rows: 3668
        })
        validation: Dataset({
            features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
            num_rows: 408
        })
        test: Dataset({
            features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
            num_rows: 1725
        })
    })
    ```

3. 动态填充

    在上面tokenize时并没有添加padding参数，这是因为在tokenize时将所有样本填充到最大长度效率不高。转而我们在构建批处理（batch）时填充样本，这样的话只需要填充到该batch中的最大长度
    
    🤗transformer库中提供了DataCollatorWithPadding函数
    
    ```python
    from transformers import DataCollatorWithPadding
    
    # 创建对象需要一个tokenizer对象，以便知道填充规则（用哪个词填充，填充在前面还是后面）
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    samples = tokenized_datasets["train"][:8] # 假设取出前8个样本作为一组
    batch = data_collator(samples) # 对该组样本进行填充
    ```

### 1.2.2 使用Trainer类微调模型

🤗Transformers提供了一个Trainer类，可以微调任何预训练模型

1. 首先要定义好模型和预处理好数据集

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

2. 定义一个TrainingArguments类

    该类包含Trainer用于训练和评估的所有超参数

    ```python
    from transformers import TrainingArguments

    training_args = TrainingArguments(output_dir="./", # 训练好的模型参数保存地址
                                      evaluation_strategy="epoch", # 训练过程中的评估策略，隔多久评估一次
                                      logging_dir="./", # 日志文件夹
                                      logging_steps=100 # 训练多少布保存y
                                     )
    ```

3. 定义模型

   由于原模型BERT并没有定义句子分类的任务，所以AutoModelForSequenceClassification实际上是给BERT后面新加一个全链接层用于分类任务

    ```python
   from transformers import AutoModelForSequenceClassification

   model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
    ```

4. 定义Trainer对象

    ```python
    from transformers import Trainer

    trainer = Trainer(
        model,
        training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    ```

5. 开始训练

    ```python
    trainer.train()
    ```

### 1.2.3 评估模型

**predict方法**

trainer类中提供了一个评估模型的方法predict，需要传入一个测试集，返回一个有三个字段的命名元祖

- predictions：测试结果，给出的是每个答案的概率数组，使用时需要在第二个轴上取最大值索引

- label_ids：测试集的标签，即正确结果
- metrics：包含数据集的loss、运行时间等信息

```python
predictions = trainer.predict(tokenized_datasets["validation"])
predictions.predictions
predictions.label_ids
predictions.metrics
```

**evaluate库**

evaluate库可以直接加载数据集对应的评价指标，并提供了方法对这些指标进行计算

```python
import evaluate

metric = evaluate.load("accuracy", "f1") # 加载评价指标
metric.compute(predictions=preds, references=predictions.label_ids) # 传入预测结果和真实结果，计算指标
```

**compute_metric()函数**

compute_metric()函数可以作为参数创建trainer对象，这样在训练过程中就会每隔一定epoch就评估一下模型训练结果

```python
import evaluate
import numpy as np

def compute_metrics(eval_preds):
    metric = evaluate.load("accuracy", "f1") # 加载评价指标
    logits, labels = eval_preds # eval_preds是使用trainer.predict的结果，logits是预测结果，labels是正确结果
    predictions = np.argmax(logits, axis=-1) # 处理预测结果
    return metric.compute(predictions=predictions, references=labels) # 计算指标
```

最后用compute_metric()函数作为参数创建trainer对象，即可得到每个训练阶段模型的指标

```python
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model, # 模型
    training_args, # 训练超参数
    train_dataset=tokenized_datasets["train"], # 指定训练集
    eval_dataset=tokenized_datasets["validation"], # 指定测试集
    data_collator=data_collator, # 动态填充
    tokenizer=tokenizer, # 编码器
    compute_metrics=compute_metrics, # 评估函数
)
```

### 1.2.4 手动微调模型

手动微调模型是指不用Trainer类的情况下进行模型微调，转而我们要使用pytorch来实现这个训练过程

1. 预处理数据集

   ```python
   from datasets import load_dataset
   from transformers import AutoTokenizer, DataCollatorWithPadding
   
   raw_datasets = load_dataset("glue", "mrpc")
   checkpoint = "bert-base-uncased"
   tokenizer = AutoTokenizer.from_pretrained(checkpoint)
   
   
   def tokenize_function(example):
       return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
   
   
   tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
   data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

2. 对tokenized_datasets做一些处理

   - 删除一些模型不需要的列（如sentence1和sentence2列）

     ```python
     tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
     ```

   - 将列名label重命名为labels（模型需要）

     ```python
     tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
     ```

   - 将数据集格式改为PyTorch张量

     ```python
     tokenized_datasets.set_format("torch")
     ```

   修改完之后检查一下数据集中的列

   ```python'
   tokenized_datasets["train"].column_names
   outputs:
   ["attention_mask", "input_ids", "labels", "token_type_ids"]
   ```

3. 定义数据加载器DataLoader

   ```python
   from torch.utils.data import DataLoader
   
   train_dataloader = DataLoader(
       tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
   )
   eval_dataloader = DataLoader(
       tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
   )
   ```

   可以检验其中一个batch看数据处理中有没有错误

   ```python
   for batch in train_dataloader:
       break
   {k: v.shape for k, v in batch.items()}
   ```

4. 加载模型

   ```python
   from transformers import AutoModelForSequenceClassification
   
   model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
   ```

   传递一个batch的数据测试一下

   ```python
   outputs = model(**batch)
   print(outputs.loss, outputs.logits.shape)
   ```

5. 加载优化器和学习率调度器

   优化器使用AdamW

   ```python
   from transformers import AdamW
   from transformers import get_scheduler
   
   optimizer = AdamW(model.parameters(), lr=5e-5)
   
   num_epochs = 3
   num_training_steps = num_epochs * len(train_dataloader)
   lr_scheduler = get_scheduler(
       "linear",
       optimizer=optimizer,
       num_warmup_steps=0,
       num_training_steps=num_training_steps,
   )
   ```

6. 使用GPU

   ```python
   import torch
   
   device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
   model.to(device)
   ```

7. 训练循环

   tqdm库用于打印训练进度条

   ```python
   from tqdm.auto import tqdm
   
   progress_bar = tqdm(range(num_training_steps))
   
   model.train()
   for epoch in range(num_epochs):
       for batch in train_dataloader:
           batch = {k: v.to(device) for k, v in batch.items()}
           outputs = model(**batch)
           loss = outputs.loss
           loss.backward()
   
           optimizer.step()
           lr_scheduler.step()
           optimizer.zero_grad()
           progress_bar.update(1)
   ```

8. 评估模型结果

   ```python
   import evaluate
   
   metric = evaluate.load("glue", "mrpc")
   model.eval()
   for batch in eval_dataloader:
       batch = {k: v.to(device) for k, v in batch.items()}
       with torch.no_grad():
           outputs = model(**batch)
   
       logits = outputs.logits
       predictions = torch.argmax(logits, dim=-1)
       metric.add_batch(predictions=predictions, references=batch["labels"])
   
   metric.compute()
   ```

## 1.3 DATASETS库

### 1.3.1 加载本地数据集

🤗DATASETS支持的数据集格式：

| Data format        | Loading script | Example                                                 |
| ------------------ | -------------- | ------------------------------------------------------- |
| CSV & TSV          | `csv`          | `load_dataset("csv", data_files="my_file.csv")`         |
| Text files         | `text`         | `load_dataset("text", data_files="my_file.txt")`        |
| JSON & JSON Lines  | `json`         | `load_dataset("json", data_files="my_file.jsonl")`      |
| Pickled DataFrames | `pandas`       | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

**加载数据集**

同时加载训练数据集和测试数据集

```python
from datasets import load_dataset

data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files)
```

### 1.3.2 DatasetBuilder类

加载数据集的load_dataset方法是通过DatasetBuilder类实现的，如果要将自己的数据集上传到HuggingFace库或者修改库中已有的数据集时，都要继承这个类

类中只要包括三个方法：

1. info：数据集的信息
2. split_generator：负责从硬盘或者网盘中读入/下载数据，并将数据按照比例分为训练/验证/测试集
3. generate_examples：读取数据集，并在调用是给出数据项

## 1.4 TOKENIZERS库

## 1.5 EVALUATE库

```python
!pip install evaluate
import evaluate
```

### 1.5.1 评价指标

输入中，predictions可由outputs取argmax取得，即

```python
predictions = outputs.argmax(1)
```

- [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)：准确率，预测准确的样本数占全部样本数的比值

  ```python
  accuracy_metric = evaluate.load("accuracy")
  results = accuracy_metric.compute(references=[0, 1], predictions=[0, 1])
  ```

- [f1](https://huggingface.co/spaces/evaluate-metric/f1/blob/main/README.md)：f1指标，precision和recall的调和平均数，`F1 = 2 * (precision * recall) / (precision + recall)`

  ```python
  f1_metric = evaluate.load("f1")
  results = f1_metric.compute(predictions=[0, 1], references=[0, 1])
  ```

- [precision](https://huggingface.co/spaces/evaluate-metric/precision)：所有被模型预测为阳性的样本数中，真阳性的比例，`Precision = TP / (TP + FP)`

  ```python
  precision_metric = evaluate.load("precision")
  results = precision_metric.compute(references=[0, 1], predictions=[0, 1])
  ```

- [recall](https://huggingface.co/spaces/evaluate-metric/recall)：所有标记为阳性样本中，被模型正确预测为阳性的样本比例，`Recall = TP / (TP + FN) `

  ```python
  recall_metric = evaluate.load('recall')
  results = recall_metric.compute(references=[0, 1], predictions=[0, 1])
  ```

# 3 Pandas

```python
import pandas as pd
```

## 3.1 读取数据

- 读取纯文本csv、tsv、txt文件

  csv用逗号分割，tsv用制表符\t分割，txt文件自定义分割符

  ```python
  csv_file = pd.read_csv(path)
  # txt文件读取时需要自己指定分隔符、表头（列名）
  txt_file = pd.read_cvs(
  	path,
      sep="\n",
      header=None, # 原文件中没有表头
      names=['name', 'hooby'] # 自定义表头
  )
  ```

- 读取excel文件

  ```python
  excel = pd.read_excel(path)
  ```

- 读取mysql

  ```python
  import pymysql
  conn = pymysql.connect(
  	host="127.0.0.1",
      user="root",
      password="",
      database="test_database",
      charset="utf8"
  )
  
  mysql_page = pd.read_sql("select * from test_table", con=conn)
  ```

- 读取json文件

  ```python
  pd.read_json(path)
  ```

  读取jsonl文件（即JSON Lines，以行格式存储）

  ```python
  pd.read_json(path, lines=True)
  ```

**保存文件**

```python
# 保存为CSV文件
df.to_csv('output.csv', index=False)

# 保存为Excel文件
df.to_excel('output.xlsx', index=False)

# 保存为JSON文件
df.to_json('output.json', orient='records')

# 保存为HTML文件
df.to_html('output.html', index=False)

# 保存为Pickle文件
df.to_pickle('output.pkl')
```

## 3.2 DataFrame

使用pandas读入的文件都将变成一个DataFrame对象，DataFrame就是一个二维数组，但是使用起来类似于字典

如果查询一列或一行，会得到一个pd.Series对象，用起来类似于字典，是索引+值的形式

DataFrame中的属性和方法：

**查看属性**

- 查看每列的类别：`df.dtypes`
- 查看所有列名：`df.columns`
- 查看所有行索引：`df.index`
- 对某列所有值按类型分类并计算每一类的数量：`df["column"].value_counts()`

**查询表数据**

- 查询一列：`df["column_name"]`，得到一个Series对象
- 查询一行：`df.loc[1]`，得到一个Series对象
- 查询多列：`df[["column_name1","column_name2"]]`
- 查询多行：`df.loc[3:5]`，注意这里都是闭区间，也就是说会查询到3，4，5三行数据

## 3.3 对列运算

### 3.3.1 Series运算

- 可以直接使用赋值语句创建新列

  ```python
  df["new_column"] = 0
  ```

  创建一个名为new_column的列，列中每行值都是0

- 也可以直接用已有的两个Series进行简单运算得到新列

  比如，将两列求和得到sum列

    ```python
    df.loc[:, "sum"] = df["column1"] + df["column2"]
    ```

- 也可以直接替换某一列的值

  将column这列的值全部+1

  ```python
  df.loc[:, 'column'] = df['column'] + 1
  ```

### 3.3.2 apply方法

通过指定方向，对每行或者每列依次应用一个函数

对每行应用：axis=1

```python
def get_big_small(row): # 这里的x是DataFrame传来的一行数据
    if row["column"] > 20:
        return 'big'
    else:
        return 'small'

# 对所有行都创建一个新列，列名为isBig，值由get_big_small得到
df.loc[:, "isBig"] = df.apply(get_big_small, axis=1)
```

### 3.3.3 基本操作

```css
A,B,C,D
1,2,3,4
5,6,7,8
9,10,11,12
```

**删除列**

```python
# 删除列B和D
df = df.drop(['B', 'D'], axis=1)
```

**重命名列**

```python
df = df.rename(columns={'A': 'Alpha', 'B': 'Bravo', 'C': 'Charlie', 'D': 'Delta'})
```

**检查列是否为空**

pandas在保存文件时，会自动将空值（比如空字符串）和None值替换为缺省值NaN

使用`isnull()`和`notnull()`来检查是否有空值

**按条件统计某列**

比如统计值为NaN的列的个数

```python
none_count = (dataset['comment'].isnull()).sum()
```

# 2. 论文

## 2.1 数据集

2.1.1 Draper. 

- 论文题目：Automated Vulnerability Detection in Source Code Using Deep Representation Learning

- 地址：`/home/sazer/Documents/DefectDetection/datasets/finetune/draper`

- 数据集描述：

  数据集就是源代码+标签，有注释

  由SATE IV Juliet Test Suite、Debian Linux distribution、public Git repositories组成，其中第一个就是CWE人造数据集集合，剩余两个是真实世界数据集

### 2.1.2 Vuldeepecker

- 论文题目：VulDeePecker: A Deep Learning-Based System for Vulnerability Detection

- 地址：`/home/sazer/Documents/DefectDetection/datasets/VulDeePecker`

- 数据集描述：

  人造数据集，由cwe-119和cwe-399组成，cwe系列都是人造数据集，并且有重复情况（测试集和训练集有重复，会造成数据泄露）

### 2.1.3 TransferRepresentationLearning

- 论文题目：Cross-Project Transfer Representation Learning for Vulnerable Function Discovery

- 地址：

  `/home/sazer/Documents/DefectDetection/datasets/TransferRepresentationLearning/Data/VulnerabilityData`

- 数据集描述：这个数据集为函数级，一个函数一个文件，其中有缺陷的函数文件名以cwe-开头。有些函数中包括注释

  真实数据集，使用开源项目进行手动数据标注

### 2.1.4 REVEAL

- 论文题目：Deep Learning based Vulnerability Detection: Are We There Yet? （论文中给出的github库中提供的数据和模型下载连接均已失效）

- 本地保存地址：`/home/sazer/Documents/DefectDetection/datasets/finetune/reveal`

- 数据集描述：

  数据集为代码（函数级）和对应标签，没有注释

  真实世界数据集，来自Chromium和Debian项目

### 2.1.5 d2a

**基本信息**

- 论文题目：D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis

- 本地保存地址：`/home/sazer/Documents/DefectDetection/datasets/finetune/d2a`

- 数据集描述：

- 数据集格式：.csv

  文件以id,label,code的形式给出，第一个例子如下

  ```c++
  id,label,code
  1,0,"static void srt_to_ass(AVCodecContext *avctx, AVBPrint *dst,
                         const char *in, int x1, int y1, int x2, int y2)
  {
      if (x1 >= 0 && y1 >= 0) {
          /* XXX: here we rescale coordinate assuming they are in DVD resolution
           * (720x480) since we don't have anything better */
  
          if (x2 >= 0 && y2 >= 0 && (x2 != x1 || y2 != y1) && x2 >= x1 && y2 >= y1) {
              /* text rectangle defined, write the text at the center of the rectangle */
              const int cx = x1 + (x2 - x1)/2;
              const int cy = y1 + (y2 - y1)/2;
              const int scaled_x = cx * (int64_t)ASS_DEFAULT_PLAYRESX / 720;
              const int scaled_y = cy * (int64_t)ASS_DEFAULT_PLAYRESY / 480;
              av_bprintf(dst, ""{\\an5}{\\pos(%d,%d)}"", scaled_x, scaled_y);
          } else {
              /* only the top left corner, assume the text starts in that corner */
              const int scaled_x = x1 * (int64_t)ASS_DEFAULT_PLAYRESX / 720;
              const int scaled_y = y1 * (int64_t)ASS_DEFAULT_PLAYRESY / 480;
              av_bprintf(dst, ""{\\an1}{\\pos(%d,%d)}"", scaled_x, scaled_y);
          }
      }
  
      ff_htmlmarkup_to_ass(avctx, dst, in);
  }"
  ```

### 2.1.6 sard

sard数据集是美国国家标准技术研究院NIST发布的一个缺陷检测数据集

**基本信息**

- 论文题目：A Software Assurance Reference Dataset: Thousands of Programs With Known Bugs

- 数据集介绍：SARD: Thousands of Reference Programs for Software Assurance

  这个是作者在research gate上发布的一篇文章，文章中对sard数据集做了介绍

- 数据集地址：[sard](https://samate.nist.gov/SARD/)

  在这个地址中自己检索所需数据集并下载使用，它也提供了一些testcase suit数据集集合，可以直接使用

### 2.1.7 devign

**基本信息**

- 论文题目：Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks

- 数据集地址：[devign](https://sites.google.com/view/devign)

  本地地址：`/home/sazer/Documents/DefectDetection/datasets/finetune/devign/Devign.json`

  这个数据集作者只放出了两部分，FFmpeg和Qemu

数据集列：project, commit_id, target（0表示有缺陷，1表示无缺陷）, func



### 2.1.8 bigvul

**基本信息**

- 论文题目：A C/C++ Code Vulnerability Dataset with Code Changes and CVE Summaries

- github地址：[BigVul:MSR_20_Code_vulnerability_CSV_Dataset](https://github.com/ZeoVan/MSR_20_Code_vulnerability_CSV_Dataset)

  有清理后可以直接使用的数据集 [cleaned_dataset](https://drive.google.com/file/d/1-0VhnHBp9IGh90s2wCNjeCMuy70HPl8X/view?usp=sharing)

- 本地地址：`/home/sazer/Documents/DefectDetection/datasets/MSR_data_cleaned.csv`

数据集中包含的列名：

```python
Index(['Unnamed: 0', 'Access Gained', 'Attack Origin',
       'Authentication Required', 'Availability', 'CVE ID', 'CVE Page',
       'CWE ID', 'Complexity', 'Confidentiality', 'Integrity',
       'Known Exploits', 'Publish Date', 'Score', 'Summary', 'Update Date',
       'Vulnerability Classification', 'add_lines', 'codeLink', 'commit_id',
       'commit_message', 'del_lines', 'file_name', 'files_changed',
       'func_after', 'func_before', 'lang', 'lines_after', 'lines_before',
       'parentID', 'patch', 'project', 'project_after', 'project_before',
       'vul', 'vul_func_with_fix'],
      dtype='object')
```

func_before是有缺陷的代码源码，func_after是修复后的没缺陷的代码，vul是当前函数是否存在缺陷，0表示无缺陷，1表示有缺陷

































1. 复现VulBerta

这个模型用于源代码缺陷检测，在RoBERTa的基础上针对C/C++代码表示工作进行pre-train得到模型VulBERTa，之后在VulBERTa的基础上针对缺陷检测任务进行fine-tune微调

1.1 数据集和baseline

**pretrain数据集**

- GitHub：作者自己在GitHub上爬的C/C++项目代码

- Draper VDISC Dataset：一个开源的software vulnerability detection dataset

  数据集对应论文为arXiv:1807.04320  Automated Vulnerability Detection in Source Code Using Deep Representation Learning

**fine-tune数据集**

- Vuldeepecker
- Draper
- REVEAL
- muVuldeepecker（MVD）
- Devign
- D2A

**baseline**

- BiLSTM

- TextCNN

  

2. source code representation

对源代码进行进一步手动特征提取，再放入模型中进行训练

使用深度学习进行源代码缺陷检测的步骤：

1. 将源代码转化为向量 —— source code representation

   现有技术有：将源代码视为文本并直接转化为向量（word2vec）、将源代码表示为AST再转化为向量（）、将源代码进行一部分编译工作再将编译得到的token转化为向量（比如用clang进行parse然后再进行encoder）

2. 将向量作为模型输入，进行训练

## 2.2 训练过程

### 2.2.1 CodeBertFinetune

直接使用CodeBert模型在CodeXGLUE数据集上进行微调

使用默认的Adam优化器进行训练时，会出现训练过程中eval数据集上准确率完全不变的情况，换成SGD优化器后准确率正常上升

在微调CodeBERT时，训练20epoch后，train_loss和valid_loss依然在基于下降，可能依然未收敛

- 直接将CodeXGLUE数据集放入CodeBert中微调

  训练过程的输出在：

  前十个epoch：`~/Documents/DefectDetection/codebertFinetune/CodebertFinetune_Craft_SGD_10_2024.06.27.output`

  后十个epoch：`~/Documents/DefectDetection/codebertFinetune/CodebertFinetune_Craft_SGD_20_2024.06.27.output`

  最后得到的accuracy=0.608

  模型在：`~/Documents/DefectDetection/codebertFinetune/codebert_SGD_epoch17.pth`

- 将CodeXGLUE数据集中的注释去掉，再进行微调

  训练过程的输出在：`~/Documents/DefectDetection/codebertFinetune/CodebertFinetune_Craft_SGD_20_2024.06.27.output/CBFT_Craft_SGD_nocomment_20_2024.06.28.output`

  最后得到的accuracy=0.581

