#  0. 训练注意事项

## 0.1 服务器网络环境

🤗库使用时需要连接huggingface服务器在线运行（也可以把要用到的模型提前加载好保存在本地）

huggingface服务器在外网，而训练用的服务器可能无法连接至外网，此时可以让远程服务器使用本机代理（仅限校园网）

1. 首先在本机Clash for Windows中打开 Allow LAN

2. 使用ifconfig命令（windows为ipconfig）查看本机ip地址

3. 之后就可以在使用命令时添加代理

   ```shell
   pip install xxx --proxy=ip:7890
   ```

4. 也可以设置全局代理

   在当前terminal中设置全局代理

   ```shell
   export all_proxy="ip:7890"
   ```

## 0.2 后台运行任务



# 1. PyTorch使用



# 1. Hugging face库Transformer使用

要使用🤗Transformer库，第一步需要安装

直接安装：`pip install transormers`

安装开发版本（带有几乎所有所需的依赖项）:`pip install transformers[sentencepiece]`

## 1.1 基本使用

Huggingface中有很多基于Transformer架构的现成模型，可以直接拿来完成一些NLP任务

### 1.1.1 直接使用pipeline

Transformers库中最基本的对象是pipeline()函数，通过pipeline可以直接加载并使用一个Trasformer已有的训练好的模型

加载代码会在每个库的Use this model中标明

**pipeline使用**

比如使用BERT模型

```python
# Use a pipeline as a high-level helper
from transformers import pipeline

pipe = pipeline("fill-mask", model="google-bert/bert-base-cased")
pipe("Hello I'm a [MASK] model.")
outputs:
    [{'score': 0.17099590599536896,
  'token': 2959,
  'token_str': 'sorry',
  'sequence': "I'm sorry, who are you."},
 {'score': 0.12543228268623352,
  'token': 1303,
  'token_str': 'here',
  'sequence': "I'm here, who are you."},
 {'score': 0.03565218672156334,
  'token': 4853,
  'token_str': 'confused',
  'sequence': "I'm confused, who are you."},
 {'score': 0.018463322892785072,
  'token': 4107,
  'token_str': 'asking',
  'sequence': "I'm asking, who are you."},
 {'score': 0.01268229354172945,
  'token': 1128,
  'token_str': 'you',
  'sequence': "I'm you, who are you."}]
```

pipeline第一个参数中指定完成的任务（完形填空），model参数指定要使用的模型（BERT），会返回一个模型对象

之后直接将要预测单词的句子放入pipe中，即可得到预测结果。

**pipeline内部**

pipeline实际上是将三个步骤组合在了一起：

1. 将传入的字符串进行Tokenizer，即分解为token，并转化为tensor
2. tokenized之后的结果传入Model中进行预测
3. 将Model的输出进行Post Porcessing后处理，以人类能理解的方式输出

这种方式易于使用，但是只能完成特定任务，如果想更灵活的使用模型，Transformer也提供了对Tokenizer和model的获取

### 1.1.2 tokenizer

**基本用法tokenizers**

- 加载

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

- 使用

```python
tokenizer("Using a Transformer network is simple")
outputs:
{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

- 保存

```python
tokenizer.save_pretrained("directory_on_my_computer")
```

**将句子拆分为token**

会根据加载的模型的tokenizer规则进行拆分

```python
sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)

print(tokens)
outputs:
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```

**编码：将token转化为tensor**

这里的编码是根据模型中tokenizer预处理生成的词汇表转化的

```python
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
outputs:
    [7993, 170, 11303, 1200, 2443, 1110, 3014]
```

也可以进行解码，即将词汇表编号转化为字符串

```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
outputs:
    'Using a Transformer network is simple'
```



### 1.1.3 Model

**加载模型**

从Huggingface中加载别人上传的使用Transformer架构的模型

- 使用默认配置加载

  这种方式加载的模型参数是随机初始化的，需要重新训练

  ```python
  from transformers import BertConfig, BertModel
  
  # Building the config
  config = BertConfig()
  
  # Building the model from the config
  model = BertModel(config)
  ```

- 加载预训练模型

  ```python
  from transformers import BertModel
  
  model = BertModel.from_pretrained("bert-base-cased")
  ```

  也可以使用AutoModel替换BertModel，AutoModel会根据后面的检查点（即bert-base-cased）自动推断模型类型

  ```python
  from transformers import A
  
  model = AutoModel.from_pretrained("bert-base-cased")
  ```

**保存模型**

将训练好的模型保存在本地磁盘

```python
model.save_pretrained("directory_on_my_computer")
```

这会保存两个文件：

- config.json：模型配置，保存了构建模型体系结构需要的所有属性
- pytorch_model.bin：模型的所有权重

**使用模型**

将tokenizer的输出作为输出放入模型中，即可得到结果

- 直接使用tokenizer

  参数：

  - sequence：要处理的字符串（序列），可以是一个，也可以是一个字符串数组

  - padding：开启填充功能

    可选值："longest"，填充至字符序列中最长句子的长度

    "max_length"或者True，填充至max_length

  - max_length：指定最大序列的长度

    默认是模型最大长度，比如BERT为512

  - truncation：True，默认为False

    将序列截断至max_length

  - return_tensors：返回的张量的类型

    pt：PyTorch张量

    tf：TensorFlow张量

    np：NumPy数组

```python
tokenized_inputs = tokenizer.tokenizer(sequence, padding="max_length", max_length=8, trucncation=True, return_tensors="pt")

model(tokenized_inputs)
```

- 手动进行分词和编码

  手动进行分词和编码后需要手动再添加一个维度，因为模型默认情况下都需要多个句子，而该例子中只有一个句子

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor([ids]) # 注意这里要手动添加一个维度
# This line will fail.
model(input_ids)
```

## 1.2 微调模型

### 1.2.1 数据预处理

1. 加载数据集

   可以从模型中心直接加载已有数据集

   ```python
   from datasets import load_dataset
   
   raw_datasets = load_dataset("glue", "mrpc")
   raw_datasets
   
   outputs:
   DatasetDict({
       train: Dataset({
           features: ['sentence1', 'sentence2', 'label', 'idx'],
           num_rows: 3668
       })
       validation: Dataset({
           features: ['sentence1', 'sentence2', 'label', 'idx'],
           num_rows: 408
       })
       test: Dataset({
           features: ['sentence1', 'sentence2', 'label', 'idx'],
           num_rows: 1725
       })
   })
   ```

   会得到一个字典，可以通过键得到每部分

   ```python
   raw_datasets["train"] # 得到训练集
   raw_datasets["train"][0] # 得到训练集的第一个数据
   raw_datasets["train"]["sentence1"] # 得到训练集的所有数据中的第一个句子
   ```

2. 对数据集进行tokenize

   方法一：直接调用tokenizer

   缺点是需要分别给训练、验证、测试数据集编码，并且得到的结果是字典，不宜用

   ```python
   checkpoint = "bert-base-uncased"
   tokenizer = AutoTokenizer.from_pretrained(checkpoint)
   
   tokenized_dataset = tokenizer(
       raw_datasets["train"]["sentence1"], # 这将会获取训练集的所有第一个句子
       raw_datasets["train"]["sentence2"], # 这将会获取训练集的所有第二个句子
       padding=True,
       truncation=True,
   )
   会得到一个字典，键为：输入词（input_ids，即编码后的句子）、attention_mask和token_type_ids（用于标记这是第一个句子还是第二个句子）
   ```

   方法二：使用map函数给数据集中每个数据都应用一个tokenize函数

   map函数会在原数据集的基础上，给每条数据中添加tokenize后的编码结果（即给每条数据多添加了input_ids, attention_mask, token_type_ids三个字段）

   ```python
   def tokenize_function(example): # 给每个数据应用的函数
       return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
   
   tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
   tokenized_datasets
   
   outputs：
   DatasetDict({
       train: Dataset({
           features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
           num_rows: 3668
       })
       validation: Dataset({
           features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
           num_rows: 408
       })
       test: Dataset({
           features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
           num_rows: 1725
       })
   })
   ```

3. 动态填充

   在上面tokenize时并没有添加padding参数，这是因为在tokenize时将所有样本填充到最大长度效率不高。转而我们在构建批处理（batch）时填充样本，这样的话只需要填充到该batch中的最大长度

   🤗transformer库中提供LDataCollatorWithPadding函数

   ```python
   from transformers import DataCollatorWithPadding
   
   # 创建对象需要一个tokenizer对象，以便知道填充规则（用哪个词填充，填充在前面还是后面）
   data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   
   samples = tokenized_datasets["train"][:8] # 假设取出前8个样本作为一组
   batch = data_collator(samples) # 对该组样本进行填充
   ```

### 1.2.2 使用Trainer类微调模型

🤗Transformers提供了一个Trainer类，可以微调任何预训练模型

1. 首先要定义好模型和预处理好数据集

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset("glue", "mrpc")
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)


def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)


tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

2. 定义一个TrainingArguments类

   该类包含Trainer用于训练和评估的所有超参数

   ```python
   from transformers import TrainingArguments
   
   training_args = TrainingArguments(output_dir="./", # 训练好的模型参数保存地址
                                    evaluation_strategy="epoch", # 训练过程中的评估策略，隔多久评估一次
                                     logging_dir="./", # 日志文件夹
                                     logging_steps=100 # 训练多少布保存y
                                    )
   ```

3. 定义模型

   由于原模型BERT并没有定义句子分类的任务，所以AutoModelForSequenceClassification实际上是给BERT后面新加一个全链接层用于分类任务

   ```python
   from transformers import AutoModelForSequenceClassification
   
   model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
   ```

4. 定义Trainer对象

   ```python
   from transformers import Trainer
   
   trainer = Trainer(
       model,
       training_args,
       train_dataset=tokenized_datasets["train"],
       eval_dataset=tokenized_datasets["validation"],
       data_collator=data_collator,
       tokenizer=tokenizer,
   )
   ```

5. 开始训练

   ```python
   trainer.train()
   ```

### 1.2.3 评估模型

**predict方法**

trainer类中提供了一个评估模型的方法predict，需要传入一个测试集，返回一个有三个字段的命名元祖

- predictions：测试结果，给出的是每个答案的概率数组，使用时需要在第二个轴上取最大值索引
- label_ids：测试集的标签，即正确结果
- metrics：包含数据集的loss、运行时间等信息

```python
predictions = trainer.predict(tokenized_datasets["validation"])
predictions.predictions
predictions.label_ids
predictions.metrics
```

**evaluate库**

evaluate库可以直接加载数据集对应的评价指标，并提供了方法对这些指标进行计算

```python
import evaluate

metric = evaluate.load("glue", "mrpc") # 加载评价指标
metric.compute(predictions=preds, references=predictions.label_ids) # 传入预测结果和真实结果，计算指标
```

**compute_metric()函数**

compute_metric()函数可以作为参数创建trainer对象，这样在训练过程中就会每隔一定epoch就评估一下模型训练结果

```python
import evaluate

def compute_metrics(eval_preds):
    metric = evaluate.load("glue", "mrpc") # 加载评价指标
    logits, labels = eval_preds # eval_preds是使用trainer.predict的结果，logits是预测结果，labels是正确结果
    predictions = np.argmax(logits, axis=-1) # 处理预测结果
    return metric.compute(predictions=predictions, references=labels) # 计算指标
```

最后用compute_metric()函数作为参数创建trainer对象，即可得到每个训练阶段模型的指标

```python
training_args = TrainingArguments("test-trainer", evaluation_strategy="epoch")
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model, # 模型
    training_args, # 训练超参数
    train_dataset=tokenized_datasets["train"], # 指定训练集
    eval_dataset=tokenized_datasets["validation"], # 指定测试集
    data_collator=data_collator, # 动态填充
    tokenizer=tokenizer, # 编码器
    compute_metrics=compute_metrics, # 评估函数
)
```

### 1.2.4 手动微调模型

手动微调模型是指不用Trainer类的情况下进行模型微调，转而我们要使用pytorch来实现这个训练过程

1. 预处理数据集

   ```python
   from datasets import load_dataset
   from transformers import AutoTokenizer, DataCollatorWithPadding
   
   raw_datasets = load_dataset("glue", "mrpc")
   checkpoint = "bert-base-uncased"
   tokenizer = AutoTokenizer.from_pretrained(checkpoint)
   
   
   def tokenize_function(example):
       return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
   
   
   tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
   data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
   ```

2. 对tokenized_datasets做一些处理

   - 删除一些模型不需要的列（如sentence1和sentence2列）

     ```python
     tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])
     ```

   - 将列名label重命名为labels（模型需要）

     ```python
     tokenized_datasets = tokenized_datasets.rename_column("label", "labels")
     ```

   - 将数据集格式改为PyTorch张量

     ```python
     tokenized_datasets.set_format("torch")
     ```

   修改完之后检查一下数据集中的列

   ```python'
   tokenized_datasets["train"].column_names
   outputs:
   ["attention_mask", "input_ids", "labels", "token_type_ids"]
   ```

3. 定义数据加载器DataLoader

   ```python
   from torch.utils.data import DataLoader
   
   train_dataloader = DataLoader(
       tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
   )
   eval_dataloader = DataLoader(
       tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
   )
   ```

   可以检验其中一个batch看数据处理中有没有错误

   ```python
   for batch in train_dataloader:
       break
   {k: v.shape for k, v in batch.items()}
   ```

4. 加载模型

   ```python
   from transformers import AutoModelForSequenceClassification
   
   model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
   ```

   传递一个batch的数据测试一下

   ```python
   outputs = model(**batch)
   print(outputs.loss, outputs.logits.shape)
   ```

5. 加载优化器和学习率调度器

   优化器使用AdamW

   ```python
   from transformers import AdamW
   from transformers import get_scheduler
   
   optimizer = AdamW(model.parameters(), lr=5e-5)
   
   num_epochs = 3
   num_training_steps = num_epochs * len(train_dataloader)
   lr_scheduler = get_scheduler(
       "linear",
       optimizer=optimizer,
       num_warmup_steps=0,
       num_training_steps=num_training_steps,
   )
   ```

6. 使用GPU

   ```python
   import torch
   
   device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
   model.to(device)
   ```

7. 训练循环

   tqdm库用于打印训练进度条

   ```python
   from tqdm.auto import tqdm
   
   progress_bar = tqdm(range(num_training_steps))
   
   model.train()
   for epoch in range(num_epochs):
       for batch in train_dataloader:
           batch = {k: v.to(device) for k, v in batch.items()}
           outputs = model(**batch)
           loss = outputs.loss
           loss.backward()
   
           optimizer.step()
           lr_scheduler.step()
           optimizer.zero_grad()
           progress_bar.update(1)
   ```

8. 评估模型结果

   ```python
   import evaluate
   
   metric = evaluate.load("glue", "mrpc")
   model.eval()
   for batch in eval_dataloader:
       batch = {k: v.to(device) for k, v in batch.items()}
       with torch.no_grad():
           outputs = model(**batch)
   
       logits = outputs.logits
       predictions = torch.argmax(logits, dim=-1)
       metric.add_batch(predictions=predictions, references=batch["labels"])
   
   metric.compute()
   ```

## 1.3 DATASETS库

### 1.3.1 加载本地数据集

🤗DATASETS支持的数据集格式：

| Data format        | Loading script | Example                                                 |
| ------------------ | -------------- | ------------------------------------------------------- |
| CSV & TSV          | `csv`          | `load_dataset("csv", data_files="my_file.csv")`         |
| Text files         | `text`         | `load_dataset("text", data_files="my_file.txt")`        |
| JSON & JSON Lines  | `json`         | `load_dataset("json", data_files="my_file.jsonl")`      |
| Pickled DataFrames | `pandas`       | `load_dataset("pandas", data_files="my_dataframe.pkl")` |

**加载数据集**

同时加载训练数据集和测试数据集

```python
from datasets import load_dataset

data_files = {"train": "SQuAD_it-train.json", "test": "SQuAD_it-test.json"}
squad_it_dataset = load_dataset("json", data_files=data_files, field="data")
```

### 1.3.2 DatasetBuilder类

加载数据集的load_dataset方法是通过DatasetBuilder类实现的，如果要将自己的数据集上传到HuggingFace库或者修改库中已有的数据集时，都要继承这个类

类中只要包括三个方法：

1. info：数据集的信息
2. split_generator：负责从硬盘或者网盘中读入/下载数据，并将数据按照比例分为训练/验证/测试集
3. generate_examples：读取数据集，并在调用是给出数据项

## 1.4 TOKENIZERS库



# 2. 论文

## 2.1 数据集

### 2.1.1 Draper

- 论文题目：Automated Vulnerability Detection in Source Code Using Deep Representation Learning

- 地址：`/home/sazer/Documents/DefectDetection/datasets/finetune/draper`

- 数据集描述：

  数据集就是源代码+标签，有注释

  由SATE IV Juliet Test Suite、Debian Linux distribution、public Git repositories组成，其中第一个就是CWE人造数据集集合，剩余两个是真实世界数据集

### 2.1.2 Vuldeepecker

- 论文题目：VulDeePecker: A Deep Learning-Based System for Vulnerability Detection

- 地址：`/home/sazer/Documents/DefectDetection/datasets/VulDeePecker`

- 数据集描述：

  人造数据集，由cwe-119和cwe-399组成，cwe系列都是人造数据集，并且有重复情况（测试集和训练集有重复，会造成数据泄露）

### 2.1.3 TransferRepresentationLearning

- 论文题目：Cross-Project Transfer Representation Learning for Vulnerable Function Discovery

- 地址：

  `/home/sazer/Documents/DefectDetection/datasets/TransferRepresentationLearning/Data/VulnerabilityData`

- 数据集描述：这个数据集为函数级，一个函数一个文件，其中有缺陷的函数文件名以cwe-开头。有些函数中包括注释

  真实数据集，使用开源项目进行手动数据标注

### 2.1.4 REVEAL

- 论文题目：Deep Learning based Vulnerability Detection: Are We There Yet? （论文中给出的github库中提供的数据和模型下载连接均已失效）

- 本地保存地址：`/home/sazer/Documents/DefectDetection/datasets/finetune/reveal`

- 数据集描述：

  数据集为代码（函数级）和对应标签，没有注释

  真实世界数据集，来自Chromium和Debian项目

### 2.1.5 d2a

**基本信息**

- 论文题目：D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis

- 本地保存地址：`/home/sazer/Documents/DefectDetection/datasets/finetune/d2a`

- 数据集描述：

- 数据集格式：.csv

  文件以id,label,code的形式给出，第一个例子如下

  ```c++
  id,label,code
  1,0,"static void srt_to_ass(AVCodecContext *avctx, AVBPrint *dst,
                         const char *in, int x1, int y1, int x2, int y2)
  {
      if (x1 >= 0 && y1 >= 0) {
          /* XXX: here we rescale coordinate assuming they are in DVD resolution
           * (720x480) since we don't have anything better */
  
          if (x2 >= 0 && y2 >= 0 && (x2 != x1 || y2 != y1) && x2 >= x1 && y2 >= y1) {
              /* text rectangle defined, write the text at the center of the rectangle */
              const int cx = x1 + (x2 - x1)/2;
              const int cy = y1 + (y2 - y1)/2;
              const int scaled_x = cx * (int64_t)ASS_DEFAULT_PLAYRESX / 720;
              const int scaled_y = cy * (int64_t)ASS_DEFAULT_PLAYRESY / 480;
              av_bprintf(dst, ""{\\an5}{\\pos(%d,%d)}"", scaled_x, scaled_y);
          } else {
              /* only the top left corner, assume the text starts in that corner */
              const int scaled_x = x1 * (int64_t)ASS_DEFAULT_PLAYRESX / 720;
              const int scaled_y = y1 * (int64_t)ASS_DEFAULT_PLAYRESY / 480;
              av_bprintf(dst, ""{\\an1}{\\pos(%d,%d)}"", scaled_x, scaled_y);
          }
      }
  
      ff_htmlmarkup_to_ass(avctx, dst, in);
  }"
  ```

  

# 需要做的事

把🤗Transformer库的模型和tokenizer保存在本地，在超算平台中使用

找到合适的数据集进行一次微调































1. 复现VulBerta

这个模型用于源代码缺陷检测，在RoBERTa的基础上针对C/C++代码表示工作进行pre-train得到模型VulBERTa，之后在VulBERTa的基础上针对缺陷检测任务进行fine-tune微调

1.1 数据集和baseline

**pretrain数据集**

- GitHub：作者自己在GitHub上爬的C/C++项目代码

- Draper VDISC Dataset：一个开源的software vulnerability detection dataset

  数据集对应论文为arXiv:1807.04320  Automated Vulnerability Detection in Source Code Using Deep Representation Learning

**fine-tune数据集**

- Vuldeepecker
- Draper
- REVEAL
- muVuldeepecker（MVD）
- Devign
- D2A

**baseline**

- BiLSTM

- TextCNN

  

2. source code representation

对源代码进行进一步手动特征提取，再放入模型中进行训练

使用深度学习进行源代码缺陷检测的步骤：

1. 将源代码转化为向量 —— source code representation

   现有技术有：将源代码视为文本并直接转化为向量（word2vec）、将源代码表示为AST再转化为向量（）、将源代码进行一部分编译工作再将编译得到的token转化为向量（比如用clang进行parse然后再进行encoder）

2. 将向量作为模型输入，进行训练



