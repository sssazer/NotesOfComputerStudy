# 1. PyTorch神经网络基础

pytorch中有用于神经网络的nn模组，nn模组中还有一个functional，包含了神经网络中的常见方法（如ReLU）

```python
import torch
from torch import nn
from torch.nn import functional as F
```

## 1.1 模型构造

神经网络模型一般由多层构成

比如对于多层感知机MLP来说，一层由一个Linear线性模型和一个激活函数构成

### 1.1.0 Module

对于PyTorch做神经网络，Module是一个很重要的概念，给定一个输入给出一个输出就可以算作一个module

Module可以非常灵活的嵌套使用

它可以是：

- 一个完整的神经网络模型
- 某一层
- 某几层

也可以说，任何一个层或者神经网络都应该是Module的一个子类

nn.Sequential就定义了一种特殊的Module

```python
net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
```

### 1.1.1 自定义一个模型

自定义一个Module的话，它必须是nn.Module的子类

自定义模型的好处在于，可以在forward函数中执行许多自定义的计算，而不必受torch库中提供的函数的限制

从nn.Module中可以继承到两个函数：

- `__init__ `初始化模型参数
- `forward `做前向计算

**比如：**

```python
class MyMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.out = nn.Linear(256, 10)
        
 	def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
```

这个模型由两个线性层组成，激活函数为ReLU

**使用方法：**

```python
net = MyMLP()
net(X)
```

给定X，就可以得到模型的输出结果

### 1.1.2 实现nn.Sequential

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for block in args:
            self._modules[block] = block # _modules是一个有序表
            
	def forward(self, X):
        for block in self._modules.values():
            X = block(X)
        return X
```

这样就可以在MySequential中传入一系列有序的Module，来得到一个模型

### 1.1.3 自定义层（实现nn.Linear）

自定义层和自定义模型其实没有本质区别，不同的是自定义层需要接收参数来指定这个层的输入和输出维度，并手动给出forward函数的计算过程

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, out_units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, out_units))
        self.bais = nn.Parameter(torch.zeros(out_units, ))
    
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear) # 最后多加一个relu激活函数

# 使用这个层
layer = MyLinear(5, 3)
```

## 1.2 参数管理

### 1.2.1 参数访问

可以在从Module中访问每一层的参数

以自定义的一个net为例

```pytrhon
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
```

- 访问某一层的参数

  可以将Sequential视为一个list，通过下标访问某一层，再通过state_dict拿到该层参数

  ```python
  net[2].state_dict() # 拿到该层的weight和bias参数，得到一个有序字典
  out:
  OrderedDict([('weight',
                tensor([[ 0.0004, -0.0010, -0.0027, -0.0096, -0.0166,  0.0008,  0.0059,  0.0078]])),
               ('bias', tensor([0.]))])
  ```

- 访问某层的权重 weight

  ```python
  net[2].weight # 输出包括权重值和梯度计算信息
  out:
  Parameter containing:
  tensor([[ 0.0004, -0.0010, -0.0027, -0.0096, -0.0166,  0.0008,  0.0059,  0.0078]],
         requires_grad=True)
  ```

  可以通过data直接拿到权重值

  ```python
  net[2].weight.data
  out:
  tensor([[ 0.0004, -0.0010, -0.0027, -0.0096, -0.0166,  0.0008,  0.0059,  0.0078]])
  ```

  可以通过grad拿到这一层保存的梯度计算信息

  ```python
  net[2].weight.grad
  ```

- 访问某层的偏移 bias

  ```python
  net[2].bias # 输出包括偏移值和梯度计算信息
  out:
  Parameter containing:
  tensor([0.], requires_grad=True)
  ```
  
  可以通过data直接拿到权重值
  
  ```python
  net[2].bias.data
  out:
  tensor([0.])
  ```
  
- 一次性访问所有参数

  使用net.named_parameters()函数

  或者使用 `net.state_dict()`

  ```python
  print(*[(name, param) for name, param in net.named_parameters()])
  out:
  ('0.weight', Parameter containing:
  tensor([[ 0.0011,  0.0143, -0.0037, -0.0063],
          [-0.0044,  0.0069, -0.0078,  0.0048],
          [-0.0170,  0.0278, -0.0074,  0.0056],
          [-0.0016,  0.0185,  0.0067, -0.0013],
          [-0.0163, -0.0057, -0.0051, -0.0077],
          [-0.0012,  0.0081, -0.0028,  0.0008],
          [ 0.0062,  0.0115, -0.0021,  0.0098],
          [ 0.0072,  0.0021, -0.0179,  0.0076]], requires_grad=True)) ('0.bias', Parameter containing:
  tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)) ('2.weight', Parameter containing:
  tensor([[ 0.0004, -0.0010, -0.0027, -0.0096, -0.0166,  0.0008,  0.0059,  0.0078]],
         requires_grad=True)) ('2.bias', Parameter containing:
  tensor([0.], requires_grad=True))
  ```

  可以看到第1层的权重被命名为 0.weight，第1层的偏移被命名为 0.bias。也可以通过这个名字来访问对应参数

  ```python
  net.state_dict()['2.bias'].data
  ```

  

  



### 1.2.2 参数初始化

1. 自定义一个初始化参数的函数

   示例函数表示：传入一个线性层，将线性层的weight用正态分布随机初始化，bias置为0

   这里调用了nn中提供了正态分布和全置为0的函数

```python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
```

2. 将这个函数应用于Sequential的所有层中

   这个将会遍历，嵌套遍历所有的层并应用init_normal函数

```python
net.apply(init_normal)
```

3. 也可以对特定层单独应用特定初始化函数

   比如，对第一层做xavier初始化

```python
def xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)

net[0].apply(xavier)
```

也可以直接把参数拿出来赋值

## 1.3 读写文件

使用torch.save()来保存数据，使用torch.load()来加载数据

- 加载和保存张量

  保存到x-file文件中

  ```python
  x = torch.arange(4)
  torch.save(x, 'x-file')
  ```

  加载

  ```python
  x2 = torch.load("x-file")
  ```

- 存取张量列表

    保存到x-file文件中

    ```python
    x = torch.arange(4)
    y = torch.zeros(4)
    torch.save([x,y], 'x-file')
    ```
    
    加载
    
    ```python
    x2, y2 = torch.load("x-file")
    ```
    
- 加载和保存模型

    并不能加载和保存整个模型，只能保存模型中每一层的参数

    保存

    ```python
    net = MLP()
    torch.save(net.state_dict(), 'mlp.params')
    ```

    读取

    读取时需要先获得原模型才能读取

    ```python
    clone = MLP()
    clone.load_state_dict(torch.load("mlp.params"))
    ```


## 1.4 使用和购买GPU

深度学习框架默认情况下使用CPU做运算

### 1.4.1 使用GPU

**获取GPU**

- 查看GPU

  ```python
  !nvidia-smi
  ```

  这个可以查看GPU使用情况，但是需要安装对应的库

- 获得对应的设备

  ```python
  torch.device('cpu') # 获得cput
  torch.cuda.device('cuda') # 获得GPU，默认获得第0块GPU
  torch.cuda.device('cuda:1') # 指定获得第1块GPU
  ```

- 查询可用gpu数量

  ```python
  torch.cuda.device_count()
  ```

**使用GPU进行张量运算**

**使用GPU进行神经网络**

# 2. 卷积神经网络CNN

CNN在图像处理领域应用广泛

它解决了两个问题

- 图片精度过高，导致图像处理要处理的数据量太大
- 图像在数字化的过程中很难保留原有的特征（比如如果直接将图片拉成一维张量）

大概由三层组成（每层都可以有多个）

1. 卷积层
2. 池化层
3. 全连接层（softmax回归，用于最后输出分类结果）

## 2.1 卷积层

### 2.1.1 卷积层概念

用于提取图像中的特征，有两个要求

- 平移不变性：不管检测对象出现在图像哪个位置，神经网络都应该对相同图像的区域有相似的反应

- 局部性：应该只关注输入图像的局部区域，而不过度在意图像中相隔较远区域的关系

  最终聚合这些局部特征对整个图像进行预测

**卷积核**

为了达成这一目的，我们可以使用卷积核。这是一个固定大小的张量

它从原图像左上角开始从左到右，从上到下滑动，提取原图像中每一块局部的信息，形成一个新的张量，这就是提取出来的特征信息。这个卷积核也被称为filter，可以应用多层卷积，最终提取出一个特征

![卷积核](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\卷积核.webp)

卷积核和对应块的运算实际上是做互相关运算，即对应位置元素相乘再相加，因此每一块运算的结果是一个值。

通过选择不同的卷积核可以获取不同的特征，比如平滑、锐化、模糊等

卷积层有四个超参数：填充、步幅、输入通道数、输出通道数

**填充**

可以看到当使用卷积核进行卷积操作之后，得到的特征对比原来的图像会变小

如果我们不想让图像变小，或者不想让图像变小太快，甚至想要图像变大，就可以在图像周围进行填充，使得最后得到的特征变大

![填充](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\填充.webp)

**步幅**

如果图像很大，而我们想要快速将它的特征变小一点

一种方式是增大卷积核的大小，但卷积核最好不能太大

另一种方式就是增大步幅，原来是一次向右移动一格，那现在可以一次向右移动两格甚至更多，使得得到的特征可以快速缩小

![步长](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\步长.webp)

如图为步幅为2的卷积，每次移动两格

**输入通道数**

当使用彩色图像时会有多个通道，比如RGB三个通道，因此输入的图像张量是 (c * h * w)，c为通道数，h为图片高度，w为图片宽度

计算时，每个通道都有一个对应的卷积核，最后结果是所有通道对应的卷积核做卷积运算，最后再求和（这是对于输出通道数为1来说，事实上输入通道数和输出通道数没有关系）

![多输入通道](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\多输入通道.png)



对应代码实现：

```python
def corr2d_multi_in(X, K):
    return sum(corr2d(x, k) for x, k in zip(X, K))
```

X是原图像，K是卷积核，两个都是三维向量，即（通道数，宽，高）

那么结果就是，每次都从X和K中取出一个通道 x和k（都是二维矩阵），将二者做卷积运算，最后将结果求和

**输出通道数**

如果指定多个输出通道，是想在对图像的一次处理中，获得多个不同的特征。

- 输入X：ci * nh * nw
- 核W：co * ci * kh * kw
- 输出Y：co * mh * mw

就是说对于一个多通道的图像X来说，我准备了co个不同的卷积核用于提取不同的特征，最终输出就是这co个不同的卷积核分别作用在原图像之后得到的co个不同的特征

每个输出可能会对应于一个非常局部的特征，再将这些局部特征（在后面的层中）按照不同权重进行组合，组合成一个更宏观的整体

代码实现：

```python
def corr2d_multi_in_out(X, K):
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)
```

此时输入的卷积核K是四维张量，即（co，ci，kh，kw）

就是将每个特征卷积核拿出来，分别和原图像做多输出通道的卷积运算，最后通过stack运算将输出结果连接起来

### 2.1.2 代码

torch.nn中有卷积层，包括1维卷积层、二维卷积层、三维卷积层

```python
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)
```

- in_channels：输入图像通道数
- out_channels：卷积产生的通道数
- kernel_size：卷积核的大小。可以是int类型，表示长宽相同，也可以是tuple类型，表示长宽
- stride：步长
- padding：填充。可以是int类型，表示上下左右填充相同行数，也可以是tuple类型(x,y)，表示上下填充x行，左右填充y列

## 2.2 池化层

### 2.2.1 池化层概念

池化层目的：

模仿人的视觉系统对数据进行降维

1. 降低信息冗余，可以快速降低图像大小
2. 提升模型的不变性，降低卷积层输出的特征对位置的敏感性
3. 防止过拟合

池化层实际上就是采样，在每一块中选出一个值来代表这个块。根据选择值的规则可以分为最大值池化、均值池化、随机池化、中位池化等。

![池化层](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\池化层.png)

池化层同样有窗口大小、填充、步幅做为超参数，但是没有输入输出维度

### 2.2.2 代码

```python
pool2d = nn.MaxPool2d(3) # 以最大池化为例，参数为窗口大小，窗口为3*3
```

框架中的步幅默认和池化窗口的大小相同，也就是和上面的图一样，窗口之间没有重叠

## 2.3 LeNet

最有名的深度神经网络，1980s

![LeNet](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\LeNet.png)

代码实现：

使用的数据集是MNIST数据集，每张图片通道数为1，大小为（32*32）

```python
net = torch.nn.Sequential(
    # 卷积层+激活，将输出通道增加到6，同时缩小特征大小
	nn.Conv2d(1, 6, kernel_size=5), nn.Sigmoid(), # torch.Size([1, 6, 28, 28])
    # 平均池化，将特征大小变为原来的一半
    nn.AvgPool2d(kernel_size=2, stride=2), # torch.Size([1, 6, 14, 14])
    # 卷积层+激活，将输出通道从6增加到16
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(), # torch.Size([1, 16, 10, 10])
    # 平均池化，将特征大小变为原来的一半
    nn.AvgPool2d(kernel_size=2, stride=2), # torch.Size([1, 16, 5, 5])
    # 将图像特征全部展开为1维，开始训练全连接层。展开后的大小应该是（1， 16 * 5 * 5）
    nn.Flatten(), # torch.Size([1, 400])
    # 线性隐藏层+激活函数
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(), # torch.Size([1, 120])
    # 线性隐藏层+激活函数
    nn.Linear(120, 84), nn.Sigmoid(), # torch.Size([1, 84])
    # softmax分类输出层，最后输出十个特征
    nn.Linear(84, 10) # torch.Size([1, 10])
)
```

查看每层的输出大小：

```python
X = torch.rand(size=(1, 1, 32, 32), dtype=torch.float32) # 给一个模拟数据，输入每层中查看大小
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__, 'output shape: \t', X.shape)
```

## 2.4 AlexNet

2012年赢得ImageNet竞赛，使得神经网络开始发展。它的出现让机器学习从人工提取数据集特征变为让机器通过卷积层和池化层自己学习特征

AlexNet就是更深更大的LeNet，主要改进为：

- 更深更大：增加了卷积层的个数，增加了输出通道

- 使用了丢弃法

- 使用ReLu激活函数代替Sigmoid

- 使用MaxPooling代替AvgPooling

- 使用了数据增强

  比如对于给定的一个样本，我会采取随机截取一部分、更改亮度、更改颜色等操作，将其变为很多不同的样本分别输入到模型中

**代码实现**

使用的数据集是ImageNet数据集，大小为（3，224，224），通道数为3（RGB），图片大小为（24 * 224）

```python
X = torch.randn(1, 1, 224, 224) # 测试数据大小

net = nn.Sequential(
    # 卷积层+激活
	nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), # torch.Size([1, 96, 54, 54])
    # 最大池化层
    nn.MaxPool2d(kernel_size=3, stride=2), # torch.Size([1, 96, 26, 26])
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(), # torch.Size([1, 256, 26, 26])
    nn.MaxPool2d(kernel_size=3, stride=2), # torch.Size([1, 256, 12, 12])
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(), # torch.Size([1, 384, 12, 12])
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(), # torch.Size([1, 384, 12, 12])
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), # torch.Size([1, 256, 12, 12])
    nn.MaxPool2d(kernel_size=3, stride=2), # torch.Size([1, 256, 5, 5])
    nn.Flatten(), # torch.Size([1, 6400])
    nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5), # torch.Size([1, 4096])
    nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5), # torch.Size([1, 4096])
    # 分类数为1000
    nn.Linear(4096, 1000) # torch.Size([1, 1000])
)
```

## 2.5 VGG

VGG使得深度学习观念开始转变，提出块的概念，即可以通过叠加块来有规律的提升模型的深度和宽度（而不是像AlexNet一样有些杂乱的提升）。

更大更深的AlexNet

AlexNet中有一部分是使用了多个相同通道数的卷积层，VGG就是将这一部分摘取出来作为一个块，然后拿多个这种块进行堆叠得到VGG网络

![VGG块](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\VGG块.png)

用多个输出通道相同的卷积层（每个卷积层后面会跟一个激活曾），加上一个最大池化层，组成一个VGG块

每一块要做的事情基本都是：通道数翻倍，高宽减半

**VGG架构**

VGG架构就是多个VGG块组合，后面接全连接层得到分类结果

根据VGG块的不同重复次数，可以得到不同架构，如：VGG-16、VGG-19

<img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\VGG架构.png" alt="VGG架构" style="zoom:50%;" />

**代码实现**

- 定义VGG块

  ```python
  # 参数：块中使用的卷积层数量， 输入通道数， 输出通道数
  def vgg_block(num_convs, in_channels, out_channels):
      layers = []
      for _ in range(num_convs): # 在循环中添加卷积层
          # 添加卷积层
          layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
          # 每个卷积层后面都要跟一个激活层
          layers.append(nn.ReLU())
          # 块中每个卷积层的输出通道数都相同
          in_channels = out_channels
      # 块的最后是一个池化层
      layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
      # 将这个VGG块变成torch中的一个块
      return nn.Sequential(*layers)
  ```

- 创建VGG架构网络

  创建VGG架构网络需要一个conv_arch参数，里面给定了每层VGG块的卷积层数和输出通道数

  ```python
  conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))
  
  def vgg(conv_arch):
      conv_blks = []
      in_channels = 1 # 假设输入的图像通道数是1
      for (num_convs, out_channels) in conv_arch:
          # 加入一个VGG块
          conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
          in_channels = out_channels
      
      # 将创建好的VGG块再加上最后的全连接层组成网络
      return nn.Sequential(
      	*conv_blks, nn.Flatten(),
          nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
          nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
          nn.Linear(4096, 10)
      )
  
  net = vgg(conv_arch)
  ```

- VGG架构中每层的输出大小

  ```python
  X = torch.randn(size=(1, 1, 224, 224))
  for blk in net:
      X = blk(X)
      print(blk.__class__.__name__, 'output shape:\t', X.shape)
  ```

  ```
  Sequential output shape:	 torch.Size([1, 64, 112, 112])
  Sequential output shape:	 torch.Size([1, 128, 56, 56])
  Sequential output shape:	 torch.Size([1, 256, 28, 28])
  Sequential output shape:	 torch.Size([1, 512, 14, 14])
  Sequential output shape:	 torch.Size([1, 512, 7, 7])
  ```

  可以看到每个VGG块都将原图像的大小变为原来的一半，同时通道数翻倍，这也是神经网络中卷积层的经典设计思路

## 2.5 NiN

Net in Net

前面的网络都需要在卷积层后面连接全连接层，这个问题在于卷积层后的第一个全连接层参数会非常大，导致占用很大的内存以及出现过拟合问题

NiN的思想就是，用卷积层替代全连接层，完全不要全连接层

**NiN块**

由一个卷积层后面跟两个1x1（核窗口大小为1x1）的卷积层组成，1x1的卷积层不会改变图片大小和通道数，因此可以相当于全连接层

![NiN块](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\NiN块.png)

**NiN架构**

无全连接层，交替使用NiN块和步幅为2的最大池化层（逐步减小高宽、增大通道数），最后使用全局平均池化层得到输出，输入通道数就是要预测的类别数

![NiN架构](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\NiN架构.png)

## 2.6 批量归一化层

### 2.6.1 概念

由于深度学习的网络层数比较深，而上层的参数又会因为底层参数改变而改变，导致上层网络需要不停适应下层的改变，使得模型训练变得困难。所以需要想一个办法，使得在学习底部层时避免变化顶部层。

批量归一化层（Batch Normalization）能够实现网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。

![BatchNormalization](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\BatchNormalization.png)

一般作用在：

- 全连接层和卷积层的输出之后，激活函数之前
- 全连接层和卷积层输入层之前

优点：

- 使用批量归一化层之后可以不使用dropout丢弃层

- 允许使用更大的学习率
- 可以加速收敛速度，但一般不改变模型精度

### 2.6.2 代码实现

一般放在卷积层或者线性层的输出之后，激活层之前

需要的参数是特征数

- 对于卷积层来说，特征数就是输出通道数
- 对于线性层来说，特征数就是输出特征数

```python
net = nn.Sequential(
	nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6),
    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(), 
    nn.Linear(256, 120), nn.BatchNorm1d(120), 
    nn.Sigmoid(), nn.Linear(120, 10)
)
```

## 2.7 ResNet

残差网络，可以使得很深的网络更加容易训练，对随后的深层神经网络设计产生了深远的影响，后面的Bert等网络也都使用了ResNet层（因为使用ResNet层才能让网络变深）

### 2.7.1 基本思想

ResNet的思想是，让神经网络的加深一定能带来更好的结果。

![ResNet](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\ResNet.png)

假设蓝色的星星 f* 是要找的最优解

- 左图：当神经网络加深时，反而可能会导致距离最优解变远
- 右图：如果当神经网络加深时，使得每次加深都在原来的基础上，也就是都完全包含上一次更浅的结果，那么最起码可以保证效果不会变差

并且这个也会使得，在训练更深的网络之前，可以先训练比较浅的网络，因此残差块的加入导致可以训练很深的神经网络

### 2.7.2 残差块

假设残差块为 f(x) ，残差块之前的结果为 x

则残差块会通过加入快速通道的方式来得到 x + f(x) 的结构，作为整个残差块的输出

![残差块](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\残差块.png)

残差块的具体结构和VGG块类似，就是卷积+激活

比如3x3Conv -- Batch Norm -- ReLU -- 3x3Conv -- Batch Norm，快速通道中X要做一次1x1卷积是因为要保证X和f(x)的形状相同

![残差块举例](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\残差块举例.png) 

事实上残差块可以加在任何地方，可以加在块中的任意一层

![不同的残差块](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\不同的残差块.png)

ResNet架构就是不同的ResNet块的堆叠

### 2.7.3 代码实现

# 3. 训练硬件使用

# 4. 数据增广

在一个已有数据集上，通过对数据集中的数据进行变形来增加数据，使得数据有更多的多样性，进而使得模型泛化性能更好

- 在语言中可以加入各种不同的背景噪音
- 图片可以改变图片的颜色和形状

# 5. 微调

微调通过使用在大数据上得到的预训练好的模型来初始化模型权重来完成提升精度

使用大模型的初始化参数可以使得训练速度变快，精度变高

**代码实现**

```python
finetune_net = torchvision.models.resnet18(pretrained=True) # 获得预训练模型resnet，并且将模型的参数也拿过来
finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2) # 取出预训练模型的最后一层（输出层），保留输入的维度，更改输出维度为2
nn.init.xavier_uniform_(finetune_net.fc.weight) # 只对最后的输出层做随机初始化，前面层d依然用预训练模型的参数
```

# 6. NLP

## 6.1 序列模型

如果想预测在 t 时间（或者说顺序为t）的变量xt，我们可以通过前面 1~t-1 的变量进行预测

即 p(x) ~ (x1, x2, ..., xt-1)，由于这 t-1 个变量是不相互独立的，所以需要用条件概率展开

![时序模型条件概率](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\时序模型条件概率.png)

序列模型就是对条件概率建模： p(xt|f(x1, x2, ..., xt-1)) 就是对前面t-1个数据进行建模，来预测xt。

由于标号（要预测的第t个数据）和 样本（前面t-1个数据）是同一个东西，因此这样的模型也称为自回归模型

对前t-1个数据建模的方案：

**马尔科夫假设**

假设当前数据之和过去 t (tao) 个数据点相关，那么问题就简化为了给定固定数量个数据预测一个数据。

就可以使用前面的模型（比如线性回归，MLP）解决

**潜变量（latent variable）模型**

引入潜变量ht来表示过去信息    ht = f(x1, ..., xt-1)，这样xt = p(xt|ht)

![潜变量模型](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\潜变量模型.png)

通过h和x加上模型1得到h‘，通过h’和x加上模型2得到x‘，这样每个模型都跟一个或者两个变量相关，使得计算较为简单

## 6.2 文本预处理

文本预处理就是将文本处理为一个时序序列

处理过程：

1. 读入原文本

   将原文本按行读入，拿到 lines=String[] 格式的数据

2. 将文本行拆分成token（以单词为基本单元为例）

   拆分完之后的格式为 `List<List<word>>`

   ```python
   def tokenize(lines):
       return [line.split() for line in lines] # 将每一行按空格分割成单词
   ```

3. 构造词汇表（vocabulary）

   将字符串类型的token映射到数字索引中

   ```python
   """
   tokens: 要处理的token列表
   min_freq: 设置最小频率，就是说如果有token出现次数小于min_freq，就直接丢掉，标记为unknown token
   reserved_tokens: 保留token
   """
   class Vocab:
       def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
           if tokens is None:
               tokens = []
           if reserved_tokens is None:
               reserved_tokens = []
           # 统计每个token的出现次数，counter是一个字典
           counter = collections.Counter(tokens)
           # 将token按出现次数排序
           self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
           # 创建id和token（list格式） token和id（字典格式）的映射
           self.idx_to_token = ['<unk>'] + reserved_tokens
           self.token_to_idx = {token:idx for idx, token in enumerate(self.idx_to_token)}
           # 按照token出现的频率给每个token编号
           for token, freq in self_token_freqs:
               if freq < min_freq:
                   break
               if token not in self.token_to_idx:
                   self.idx_to_token.append(token)
                   self.token_to_idx[token] = len(self.idx_to_token) - 1
   	
       def __len__(self):
           return len(self.idx_to_token)
       
       # 给定一个token列表，返回序列化后的结果
       def __getitem__(self, tokens):
           # 如果给的是单个token，就直接返回对应的index
           if not isinstance(tokens, (list, tuple)):
               return self.token_to_idx.get(tokens, self.unk) # 找不到给定的token就返回unknown token
           return [self.__getitem__(token) for token in tokens]
       
       # 给定一个 index 序列，返回对应的 token字符串
       def to_tokens(self, indices):
           if not isinstance(indices, (list, tuple)):
               return self.idx_to_token[indices]
           return [self.idx_to_token[index] for index in indices]
   ```

   
