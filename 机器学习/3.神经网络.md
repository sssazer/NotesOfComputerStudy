# 1. PyTorch神经网络基础

pytorch中有用于神经网络的nn模组，nn模组中还有一个functional，包含了神经网络中的常见方法（如ReLU）

```python
import torch
from torch import nn
from torch.nn import functional as F
```

## 1.1 模型构造

神经网络模型一般由多层构成

比如对于多层感知机MLP来说，一层由一个Linear线性模型和一个激活函数构成

### 1.1.0 Module

对于PyTorch做神经网络，Module是一个很重要的概念，给定一个输入给出一个输出就可以算作一个module

Module可以非常灵活的嵌套使用

它可以是：

- 一个完整的神经网络模型
- 某一层
- 某几层

也可以说，任何一个层或者神经网络都应该是Module的一个子类

nn.Sequential就定义了一种特殊的Module

```python
net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
```

### 1.1.1 自定义一个模型

自定义一个Module的话，它必须是nn.Module的子类

自定义模型的好处在于，可以在forward函数中执行许多自定义的计算，而不必受torch库中提供的函数的限制

从nn.Module中可以继承到两个函数：

- `__init__ `初始化模型参数
- `forward `做前向计算

**比如：**

```python
class MyMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.out = nn.Linear(256, 10)
        
 	def forward(self, X):
        return self.out(F.relu(self.hidden(X)))
```

这个模型由两个线性层组成，激活函数为ReLU

**使用方法：**

```python
net = MyMLP()
net(X)
```

给定X，就可以得到模型的输出结果

### 1.1.2 实现nn.Sequential

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for block in args:
            self._modules[block] = block # _modules是一个有序表
            
	def forward(self, X):
        for block in self._modules.values():
            X = block(X)
        return X
```

这样就可以在MySequential中传入一系列有序的Module，来得到一个模型

### 1.1.3 自定义层（实现nn.Linear）

自定义层和自定义模型其实没有本质区别，不同的是自定义层需要接收参数来指定这个层的输入和输出维度，并手动给出forward函数的计算过程

```python
class MyLinear(nn.Module):
    def __init__(self, in_units, out_units):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(in_units, out_units))
        self.bais = nn.Parameter(torch.zeros(out_units, ))
    
    def forward(self, X):
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return F.relu(linear) # 最后多加一个relu激活函数

# 使用这个层
layer = MyLinear(5, 3)
```

## 1.2 参数管理

### 1.2.1 参数访问

可以在从Module中访问每一层的参数

以自定义的一个net为例

```pytrhon
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
```

- 访问某一层的参数

  可以将Sequential视为一个list，通过下标访问某一层，再通过state_dict拿到该层参数

  ```python
  net[2].state_dict() # 拿到该层的weight和bias参数，得到一个有序字典
  out:
  OrderedDict([('weight',
                tensor([[ 0.0004, -0.0010, -0.0027, -0.0096, -0.0166,  0.0008,  0.0059,  0.0078]])),
               ('bias', tensor([0.]))])
  ```

- 访问某层的权重 weight

  ```python
  net[2].weight # 输出包括权重值和梯度计算信息
  out:
  Parameter containing:
  tensor([[ 0.0004, -0.0010, -0.0027, -0.0096, -0.0166,  0.0008,  0.0059,  0.0078]],
         requires_grad=True)
  ```

  可以通过data直接拿到权重值

  ```python
  net[2].weight.data
  out:
  tensor([[ 0.0004, -0.0010, -0.0027, -0.0096, -0.0166,  0.0008,  0.0059,  0.0078]])
  ```

  可以通过grad拿到这一层保存的梯度计算信息

  ```python
  net[2].weight.grad
  ```

- 访问某层的偏移 bias

  ```python
  net[2].bias # 输出包括偏移值和梯度计算信息
  out:
  Parameter containing:
  tensor([0.], requires_grad=True)
  ```
  
  可以通过data直接拿到权重值
  
  ```python
  net[2].bias.data
  out:
  tensor([0.])
  ```
  
- 一次性访问所有参数

  使用net.named_parameters()函数

  或者使用 `net.state_dict()`

  ```python
  print(*[(name, param) for name, param in net.named_parameters()])
  out:
  ('0.weight', Parameter containing:
  tensor([[ 0.0011,  0.0143, -0.0037, -0.0063],
          [-0.0044,  0.0069, -0.0078,  0.0048],
          [-0.0170,  0.0278, -0.0074,  0.0056],
          [-0.0016,  0.0185,  0.0067, -0.0013],
          [-0.0163, -0.0057, -0.0051, -0.0077],
          [-0.0012,  0.0081, -0.0028,  0.0008],
          [ 0.0062,  0.0115, -0.0021,  0.0098],
          [ 0.0072,  0.0021, -0.0179,  0.0076]], requires_grad=True)) ('0.bias', Parameter containing:
  tensor([0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)) ('2.weight', Parameter containing:
  tensor([[ 0.0004, -0.0010, -0.0027, -0.0096, -0.0166,  0.0008,  0.0059,  0.0078]],
         requires_grad=True)) ('2.bias', Parameter containing:
  tensor([0.], requires_grad=True))
  ```

  可以看到第1层的权重被命名为 0.weight，第1层的偏移被命名为 0.bias。也可以通过这个名字来访问对应参数

  ```python
  net.state_dict()['2.bias'].data
  ```


### 1.2.2 参数初始化

1. 自定义一个初始化参数的函数

   示例函数表示：传入一个线性层，将线性层的weight用正态分布随机初始化，bias置为0

   这里调用了nn中提供了正态分布和全置为0的函数

```python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
```

2. 将这个函数应用于Sequential的所有层中

   这个将会遍历，嵌套遍历所有的层并应用init_normal函数

```python
net.apply(init_normal)
```

3. 也可以对特定层单独应用特定初始化函数

   比如，对第一层做xavier初始化

```python
def xavier(m):
    if type(m) == nn.Linear:
        nn.init.xavier_uniform_(m.weight)

net[0].apply(xavier)
```

也可以直接把参数拿出来赋值

## 1.3 读写文件

使用torch.save()来保存数据，使用torch.load()来加载数据

- 加载和保存张量

  保存到x-file文件中

  ```python
  x = torch.arange(4)
  torch.save(x, 'x-file')
  ```

  加载

  ```python
  x2 = torch.load("x-file")
  ```

- 存取张量列表

    保存到x-file文件中

    ```python
    x = torch.arange(4)
    y = torch.zeros(4)
    torch.save([x,y], 'x-file')
    ```
    
    加载
    
    ```python
    x2, y2 = torch.load("x-file")
    ```
    
- 加载和保存模型

    并不能加载和保存整个模型，只能保存模型中每一层的参数

    保存

    ```python
    net = MLP()
    torch.save(net.state_dict(), 'mlp.params')
    ```

    读取

    读取时需要先获得原模型才能读取

    ```python
    clone = MLP()
    clone.load_state_dict(torch.load("mlp.params"))
    ```


## 1.4 使用GPU

深度学习框架默认情况下使用CPU做运算，pytorch如果没注意可能安装的也是CPU版本，如果想用GPU运算，需要CUDA（nvidia显卡）和GPU版pytorch，并且CUDA和pytorch版本要匹配

### 1.4.1 conda安装GPU pytorch

**创建conda环境并安装pytorch**

1. 在conda中创建环境并激活

   ```bash
   conda create --name pytorchGPU python=3.8
   conda activate pytorchGPU
   ```

   如果激活失败显示shell没有被正确配置，则需要先进入base环境进行init操作

   ```bash
   source ~/miniconda3/bin/activate # 此时终端命令前应该有(base)表示进入base环境
   conda init bash # 初始化自己的终端
   source ~/.bashrc # 之后重启终端或者执行这个命令，刷新状态
   conda activate pytorchGPU # 之后应该就可以成功激活环境
   ```

2. 运行nvcc --version查看cuda版本，之后在pytorch官网中找到对应版本的pytorch

   [pytorch官网版本对应](https://pytorch.org/get-started/previous-versions/)

3. 安装pytorch

   推荐用pip安装，conda安装由于网络原因会特别慢

   ```bash
   pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116
   ```

4. 验证是否安装成功

   ```python
   import torch
   print(torch.cuda.is_available())
   # 输出True即为成功安装GPU版pytorch
   ```

**在jupyter notebook中使用conda环境**

可以直接在每个conda环境中安装一个新的jupyternotebook，这样就可以保证各个jupyternotebook相互独立

也可以将conda环境注册为一个kernel，使用系统打开jupyternotebook然后切换kernel即可

1. 进入自己的conda环境

   ```bash
   conda activate pytorchGPU
   ```

2. 安装ipykernel工具

   ```bash
   conda install ipykernel
   ```

3. 配置jupyter使用当前conda环境内核

   ```python
   ipython kernel install --user --name=pytorchGPU
   ```

4. 启动jupyter，即可在切换内核时找到对应的conda环境

### 1.4.1 使用GPU

**获取GPU**

- 查看GPU状态

  ```python
  !nvidia-smi
  ```

  这个可以查看GPU使用情况，但是需要安装对应的库

- 获得对应的设备

  ```python
  torch.device('cpu') # 获得cput
  torch.cuda.device('cuda') # 获得GPU，默认获得第0块GPU
  torch.cuda.device('cuda:1') # 指定获得第1块GPU
  ```

- 查询可用gpu数量

  ```python
  torch.cuda.device_count()
  ```

**使用GPU进行张量运算**

**使用GPU进行神经网络**

## 1.5 深度学习常见环境配置

- **Transformer**

  安装transformer时最好使用hugging-face的源码方式安装

  ```bash
  pip install git+https://github.com/huggingface/transformers
  ```

  

# 2. 卷积神经网络CNN

CNN在图像处理领域应用广泛

它解决了两个问题

- 图片精度过高，导致图像处理要处理的数据量太大
- 图像在数字化的过程中很难保留原有的特征（比如如果直接将图片拉成一维张量）

大概由三层组成（每层都可以有多个）

1. 卷积层
2. 池化层
3. 全连接层（softmax回归，用于最后输出分类结果）

## 2.1 卷积层

### 2.1.1 卷积层概念

用于提取图像中的特征，有两个要求

- 平移不变性：不管检测对象出现在图像哪个位置，神经网络都应该对相同图像的区域有相似的反应

- 局部性：应该只关注输入图像的局部区域，而不过度在意图像中相隔较远区域的关系

  最终聚合这些局部特征对整个图像进行预测

**卷积核**

为了达成这一目的，我们可以使用卷积核。这是一个固定大小的张量

它从原图像左上角开始从左到右，从上到下滑动，提取原图像中每一块局部的信息，形成一个新的张量，这就是提取出来的特征信息。这个卷积核也被称为filter，可以应用多层卷积，最终提取出一个特征

![卷积核](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\卷积核.webp)

卷积核和对应块的运算实际上是做互相关运算，即对应位置元素相乘再相加，因此每一块运算的结果是一个值。

通过选择不同的卷积核可以获取不同的特征，比如平滑、锐化、模糊等

卷积层有四个超参数：填充、步幅、输入通道数、输出通道数

**填充**

可以看到当使用卷积核进行卷积操作之后，得到的特征对比原来的图像会变小

如果我们不想让图像变小，或者不想让图像变小太快，甚至想要图像变大，就可以在图像周围进行填充，使得最后得到的特征变大

![填充](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\填充.webp)

**步幅**

如果图像很大，而我们想要快速将它的特征变小一点

一种方式是增大卷积核的大小，但卷积核最好不能太大

另一种方式就是增大步幅，原来是一次向右移动一格，那现在可以一次向右移动两格甚至更多，使得得到的特征可以快速缩小

![步长](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\步长.webp)

如图为步幅为2的卷积，每次移动两格

**输入通道数**

当使用彩色图像时会有多个通道，比如RGB三个通道，因此输入的图像张量是 (c * h * w)，c为通道数，h为图片高度，w为图片宽度

计算时，每个通道都有一个对应的卷积核，最后结果是所有通道对应的卷积核做卷积运算，最后再求和（这是对于输出通道数为1来说，事实上输入通道数和输出通道数没有关系）

![多输入通道](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\多输入通道.png)



对应代码实现：

```python
def corr2d_multi_in(X, K):
    return sum(corr2d(x, k) for x, k in zip(X, K))
```

X是原图像，K是卷积核，两个都是三维向量，即（通道数，宽，高）

那么结果就是，每次都从X和K中取出一个通道 x和k（都是二维矩阵），将二者做卷积运算，最后将结果求和

**输出通道数**

如果指定多个输出通道，是想在对图像的一次处理中，获得多个不同的特征。

- 输入X：ci * nh * nw
- 核W：co * ci * kh * kw
- 输出Y：co * mh * mw

就是说对于一个多通道的图像X来说，我准备了co个不同的卷积核用于提取不同的特征，最终输出就是这co个不同的卷积核分别作用在原图像之后得到的co个不同的特征

每个输出可能会对应于一个非常局部的特征，再将这些局部特征（在后面的层中）按照不同权重进行组合，组合成一个更宏观的整体

代码实现：

```python
def corr2d_multi_in_out(X, K):
    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)
```

此时输入的卷积核K是四维张量，即（co，ci，kh，kw）

就是将每个特征卷积核拿出来，分别和原图像做多输出通道的卷积运算，最后通过stack运算将输出结果连接起来

### 2.1.2 代码

torch.nn中有卷积层，包括1维卷积层、二维卷积层、三维卷积层

```python
conv2d = nn.Conv2d(1, 1, kernel_size=3, padding=1)
```

- in_channels：输入图像通道数
- out_channels：卷积产生的通道数
- kernel_size：卷积核的大小。可以是int类型，表示长宽相同，也可以是tuple类型，表示长宽
- stride：步长
- padding：填充。可以是int类型，表示上下左右填充相同行数，也可以是tuple类型(x,y)，表示上下填充x行，左右填充y列

## 2.2 池化层

### 2.2.1 池化层概念

池化层目的：

模仿人的视觉系统对数据进行降维

1. 降低信息冗余，可以快速降低图像大小
2. 提升模型的不变性，降低卷积层输出的特征对位置的敏感性
3. 防止过拟合

池化层实际上就是采样，在每一块中选出一个值来代表这个块。根据选择值的规则可以分为最大值池化、均值池化、随机池化、中位池化等。

![池化层](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\池化层.png)

池化层同样有窗口大小、填充、步幅做为超参数，但是没有输入输出维度

### 2.2.2 代码

```python
pool2d = nn.MaxPool2d(3) # 以最大池化为例，参数为窗口大小，窗口为3*3
```

框架中的步幅默认和池化窗口的大小相同，也就是和上面的图一样，窗口之间没有重叠

## 2.3 LeNet

最有名的深度神经网络，1980s

![LeNet](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\LeNet.png)

代码实现：

使用的数据集是MNIST数据集，每张图片通道数为1，大小为（32*32）

```python
net = torch.nn.Sequential(
    # 卷积层+激活，将输出通道增加到6，同时缩小特征大小
	nn.Conv2d(1, 6, kernel_size=5), nn.Sigmoid(), # torch.Size([1, 6, 28, 28])
    # 平均池化，将特征大小变为原来的一半
    nn.AvgPool2d(kernel_size=2, stride=2), # torch.Size([1, 6, 14, 14])
    # 卷积层+激活，将输出通道从6增加到16
    nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(), # torch.Size([1, 16, 10, 10])
    # 平均池化，将特征大小变为原来的一半
    nn.AvgPool2d(kernel_size=2, stride=2), # torch.Size([1, 16, 5, 5])
    # 将图像特征全部展开为1维，开始训练全连接层。展开后的大小应该是（1， 16 * 5 * 5）
    nn.Flatten(), # torch.Size([1, 400])
    # 线性隐藏层+激活函数
    nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(), # torch.Size([1, 120])
    # 线性隐藏层+激活函数
    nn.Linear(120, 84), nn.Sigmoid(), # torch.Size([1, 84])
    # softmax分类输出层，最后输出十个特征
    nn.Linear(84, 10) # torch.Size([1, 10])
)
```

查看每层的输出大小：

```python
X = torch.rand(size=(1, 1, 32, 32), dtype=torch.float32) # 给一个模拟数据，输入每层中查看大小
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__, 'output shape: \t', X.shape)
```

## 2.4 AlexNet

2012年赢得ImageNet竞赛，使得神经网络开始发展。它的出现让机器学习从人工提取数据集特征变为让机器通过卷积层和池化层自己学习特征

AlexNet就是更深更大的LeNet，主要改进为：

- 更深更大：增加了卷积层的个数，增加了输出通道

- 使用了丢弃法

- 使用ReLu激活函数代替Sigmoid

- 使用MaxPooling代替AvgPooling

- 使用了数据增强

  比如对于给定的一个样本，我会采取随机截取一部分、更改亮度、更改颜色等操作，将其变为很多不同的样本分别输入到模型中

**代码实现**

使用的数据集是ImageNet数据集，大小为（3，224，224），通道数为3（RGB），图片大小为（24 * 224）

```python
X = torch.randn(1, 1, 224, 224) # 测试数据大小

net = nn.Sequential(
    # 卷积层+激活
	nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), # torch.Size([1, 96, 54, 54])
    # 最大池化层
    nn.MaxPool2d(kernel_size=3, stride=2), # torch.Size([1, 96, 26, 26])
    nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(), # torch.Size([1, 256, 26, 26])
    nn.MaxPool2d(kernel_size=3, stride=2), # torch.Size([1, 256, 12, 12])
    nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(), # torch.Size([1, 384, 12, 12])
    nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(), # torch.Size([1, 384, 12, 12])
    nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), # torch.Size([1, 256, 12, 12])
    nn.MaxPool2d(kernel_size=3, stride=2), # torch.Size([1, 256, 5, 5])
    nn.Flatten(), # torch.Size([1, 6400])
    nn.Linear(6400, 4096), nn.ReLU(), nn.Dropout(p=0.5), # torch.Size([1, 4096])
    nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(p=0.5), # torch.Size([1, 4096])
    # 分类数为1000
    nn.Linear(4096, 1000) # torch.Size([1, 1000])
)
```

## 2.5 VGG

VGG使得深度学习观念开始转变，提出块的概念，即可以通过叠加块来有规律的提升模型的深度和宽度（而不是像AlexNet一样有些杂乱的提升）。

更大更深的AlexNet

AlexNet中有一部分是使用了多个相同通道数的卷积层，VGG就是将这一部分摘取出来作为一个块，然后拿多个这种块进行堆叠得到VGG网络

![VGG块](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\VGG块.png)

用多个输出通道相同的卷积层（每个卷积层后面会跟一个激活曾），加上一个最大池化层，组成一个VGG块

每一块要做的事情基本都是：通道数翻倍，高宽减半

**VGG架构**

VGG架构就是多个VGG块组合，后面接全连接层得到分类结果

根据VGG块的不同重复次数，可以得到不同架构，如：VGG-16、VGG-19

<img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\VGG架构.png" alt="VGG架构" style="zoom:50%;" />

**代码实现**

- 定义VGG块

  ```python
  # 参数：块中使用的卷积层数量， 输入通道数， 输出通道数
  def vgg_block(num_convs, in_channels, out_channels):
      layers = []
      for _ in range(num_convs): # 在循环中添加卷积层
          # 添加卷积层
          layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
          # 每个卷积层后面都要跟一个激活层
          layers.append(nn.ReLU())
          # 块中每个卷积层的输出通道数都相同
          in_channels = out_channels
      # 块的最后是一个池化层
      layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
      # 将这个VGG块变成torch中的一个块
      return nn.Sequential(*layers)
  ```

- 创建VGG架构网络

  创建VGG架构网络需要一个conv_arch参数，里面给定了每层VGG块的卷积层数和输出通道数

  ```python
  conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))
  
  def vgg(conv_arch):
      conv_blks = []
      in_channels = 1 # 假设输入的图像通道数是1
      for (num_convs, out_channels) in conv_arch:
          # 加入一个VGG块
          conv_blks.append(vgg_block(num_convs, in_channels, out_channels))
          in_channels = out_channels
      
      # 将创建好的VGG块再加上最后的全连接层组成网络
      return nn.Sequential(
      	*conv_blks, nn.Flatten(),
          nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),
          nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
          nn.Linear(4096, 10)
      )
  
  net = vgg(conv_arch)
  ```

- VGG架构中每层的输出大小

  ```python
  X = torch.randn(size=(1, 1, 224, 224))
  for blk in net:
      X = blk(X)
      print(blk.__class__.__name__, 'output shape:\t', X.shape)
  ```

  ```
  Sequential output shape:	 torch.Size([1, 64, 112, 112])
  Sequential output shape:	 torch.Size([1, 128, 56, 56])
  Sequential output shape:	 torch.Size([1, 256, 28, 28])
  Sequential output shape:	 torch.Size([1, 512, 14, 14])
  Sequential output shape:	 torch.Size([1, 512, 7, 7])
  ```

  可以看到每个VGG块都将原图像的大小变为原来的一半，同时通道数翻倍，这也是神经网络中卷积层的经典设计思路

## 2.5 NiN

Net in Net

前面的网络都需要在卷积层后面连接全连接层，这个问题在于卷积层后的第一个全连接层参数会非常大，导致占用很大的内存以及出现过拟合问题

NiN的思想就是，用卷积层替代全连接层，完全不要全连接层

**NiN块**

由一个卷积层后面跟两个1x1（核窗口大小为1x1）的卷积层组成，1x1的卷积层不会改变图片大小和通道数，因此可以相当于全连接层

![NiN块](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\NiN块.png)

**NiN架构**

无全连接层，交替使用NiN块和步幅为2的最大池化层（逐步减小高宽、增大通道数），最后使用全局平均池化层得到输出，输入通道数就是要预测的类别数

![NiN架构](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\NiN架构.png)

## 2.6 批量归一化层

### 2.6.1 概念

由于深度学习的网络层数比较深，而上层的参数又会因为底层参数改变而改变，导致上层网络需要不停适应下层的改变，使得模型训练变得困难。所以需要想一个办法，使得在学习底部层时避免变化顶部层。

批量归一化层（Batch Normalization）能够实现网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。

![BatchNormalization](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\BatchNormalization.png)

一般作用在：

- 全连接层和卷积层的输出之后，激活函数之前
- 全连接层和卷积层输入层之前

优点：

- 使用批量归一化层之后可以不使用dropout丢弃层

- 允许使用更大的学习率
- 可以加速收敛速度，但一般不改变模型精度

### 2.6.2 代码实现

一般放在卷积层或者线性层的输出之后，激活层之前

需要的参数是特征数

- 对于卷积层来说，特征数就是输出通道数
- 对于线性层来说，特征数就是输出特征数

```python
net = nn.Sequential(
	nn.Conv2d(1, 6, kernel_size=5), nn.BatchNorm2d(6),
    nn.Sigmoid(), nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Flatten(), 
    nn.Linear(256, 120), nn.BatchNorm1d(120), 
    nn.Sigmoid(), nn.Linear(120, 10)
)
```

## 2.7 ResNet

残差网络，可以使得很深的网络更加容易训练，对随后的深层神经网络设计产生了深远的影响，后面的Bert等网络也都使用了ResNet层（因为使用ResNet层才能让网络变深）

### 2.7.1 基本思想

ResNet的思想是，让神经网络的加深一定能带来更好的结果。

![ResNet](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\ResNet.png)

假设蓝色的星星 f* 是要找的最优解

- 左图：当神经网络加深时，反而可能会导致距离最优解变远
- 右图：如果当神经网络加深时，使得每次加深都在原来的基础上，也就是都完全包含上一次更浅的结果，那么最起码可以保证效果不会变差

并且这个也会使得，在训练更深的网络之前，可以先训练比较浅的网络，因此残差块的加入导致可以训练很深的神经网络

### 2.7.2 残差块

假设残差块为 f(x) ，残差块之前的结果为 x

则残差块会通过加入快速通道的方式来得到 x + f(x) 的结构，作为整个残差块的输出

![残差块](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\残差块.png)

残差块的具体结构和VGG块类似，就是卷积+激活

比如3x3Conv -- Batch Norm -- ReLU -- 3x3Conv -- Batch Norm，快速通道中X要做一次1x1卷积是因为要保证X和f(x)的形状相同

![残差块举例](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\残差块举例.png) 

事实上残差块可以加在任何地方，可以加在块中的任意一层

![不同的残差块](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\不同的残差块.png)

ResNet架构就是不同的ResNet块的堆叠

### 2.7.3 代码实现

# 3. 训练硬件使用

# 4. 数据增广

在一个已有数据集上，通过对数据集中的数据进行变形来增加数据，使得数据有更多的多样性，进而使得模型泛化性能更好

- 在语言中可以加入各种不同的背景噪音
- 图片可以改变图片的颜色和形状

# 5. 微调

微调通过使用在大数据上得到的预训练好的模型来初始化模型权重来完成提升精度

使用大模型的初始化参数可以使得训练速度变快，精度变高

**代码实现**

```python
finetune_net = torchvision.models.resnet18(pretrained=True) # 获得预训练模型resnet，并且将模型的参数也拿过来
finetune_net.fc = nn.Linear(finetune_net.fc.in_features, 2) # 取出预训练模型的最后一层（输出层），保留输入的维度，更改输出维度为2
nn.init.xavier_uniform_(finetune_net.fc.weight) # 只对最后的输出层做随机初始化，前面层d依然用预训练模型的参数
```

# 6. NLP

## 6.1 序列模型

如果想预测在 t 时间（或者说顺序为t）的变量xt，我们可以通过前面 1~t-1 的变量进行预测

即 p(x) ~ (x1, x2, ..., xt-1)，由于这 t-1 个变量是不相互独立的，所以需要用条件概率展开

![时序模型条件概率](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\时序模型条件概率.png)

序列模型就是对条件概率建模： p(xt|f(x1, x2, ..., xt-1)) 就是对前面t-1个数据进行建模，来预测xt。

由于标号（要预测的第t个数据）和 样本（前面t-1个数据）是同一个东西，因此这样的模型也称为自回归模型

对前t-1个数据建模的方案：

**马尔科夫假设**

假设当前数据之和过去 t (tao) 个数据点相关，那么问题就简化为了给定固定数量个数据预测一个数据。

就可以使用前面的模型（比如线性回归，MLP）解决

**潜变量（latent variable）模型**

引入潜变量ht来表示过去信息    ht = f(x1, ..., xt-1)，这样xt = p(xt|ht)

![潜变量模型](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\潜变量模型.png)

通过h和x加上模型1得到h‘，通过h’和x加上模型2得到x‘，这样每个模型都跟一个或者两个变量相关，使得计算较为简单

## 6.2 文本预处理

文本预处理就是将文本处理为一个时序序列

处理过程：

1. 读入原文本

   将原文本按行读入，拿到 lines=String[] 格式的数据

2. 将文本行拆分成token（以单词为基本单元为例）

   拆分完之后的格式为 `List<List<word>>`

   ```python
   def tokenize(lines):
       return [line.split() for line in lines] # 将每一行按空格分割成单词
   ```

3. 构造词汇表（vocabulary）

   将字符串类型的token映射到数字索引中

   ```python
   """
   tokens: 要处理的token列表
   min_freq: 设置最小频率，就是说如果有token出现次数小于min_freq，就直接丢掉，标记为unknown token
   reserved_tokens: 保留token
   """
   class Vocab:
       def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
           if tokens is None:
               tokens = []
           if reserved_tokens is None:
               reserved_tokens = []
           # 统计每个token的出现次数，counter是一个字典
           counter = collections.Counter(tokens)
           # 将token按出现次数排序
           self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
           # 创建id和token（list格式） token和id（字典格式）的映射
           self.idx_to_token = ['<unk>'] + reserved_tokens
           self.token_to_idx = {token:idx for idx, token in enumerate(self.idx_to_token)}
           # 按照token出现的频率给每个token编号
           for token, freq in self_token_freqs:
               if freq < min_freq:
                   break
               if token not in self.token_to_idx:
                   self.idx_to_token.append(token)
                   self.token_to_idx[token] = len(self.idx_to_token) - 1
   	
       def __len__(self):
           return len(self.idx_to_token)
       
       # 给定一个token列表，返回序列化后的结果
       def __getitem__(self, tokens):
           # 如果给的是单个token，就直接返回对应的index
           if not isinstance(tokens, (list, tuple)):
               return self.token_to_idx.get(tokens, self.unk) # 找不到给定的token就返回unknown token
           return [self.__getitem__(token) for token in tokens]
       
       # 给定一个 index 序列，返回对应的 token字符串
       def to_tokens(self, indices):
           if not isinstance(indices, (list, tuple)):
               return self.idx_to_token[indices]
           return [self.idx_to_token[index] for index in indices]
   ```


## 6.3 语言模型

整个语言模型的目标就是：给定文本序列x1, ......, xt，估计联合概率 p(x1, ......, xt)

应用包括：

- 做预训练模型，比如BERT，GPT-3

- 生成文本，给定前面几个词，不断使用xt~p(xt|x1, x2, ...,xt-1)来生成后续文本

对于N元语法来说，这实际上是一个统计方法，而不是学习方法

**计数建模**

比如，要求序列为长度为2的文本序列 xx‘ （x和x’都是单词）的出现概率，则

![计数建模](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\计数建模.png)

- n：文本中的总词数
- n(x)：单词x出现的次数
- n(x, x')：连续词对 xx’ 出现的次数

就可以理解为：（单词x出现的概率） 乘上 （在单词x出现的情况下，下一个词是x'的概率（即组成xx‘词对的概率））

或者：将n(x)约分掉，就变成 xx'词对出现的次数 除以 总词数，即为xx’出现的概率

这样往下可以无限扩展到长为3、4、5、6...的文本序列出现概率

### 6.3.1 N-gram

**N元语法**

计数建模的问题是，当选择的序列很长时，可能整个文本中都不会出现一次那样的序列

使用马尔科夫假设可以缓解这个问题

比如对于一个长度为4的文本序列 x1x2 x3 x4，我们不再关注它整体出现的次数，而是每次都只关心其中长度为N的文本序列

- 计数建模：p(x1, x2, x3, x4) = n(x1, x2, x3, x4) / n 

- 一元语法：只关心长度为1的文本序列，也就是只关心当前单词，完全不考虑前文

  ![一元语法](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\一元语法.png)

- 二元语法：关心长度为2的文本序列，也就是对每个单词只关心它前面一个单词

  ![二元语法](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\二元语法.png)

- 三元语法

  ![三元语法](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\三元语法.png)

N元语法的好处是，不论要求多长的文本序列，我都只关心长度为N的文本序列。

比如二元语法我只关心长度为2的文本序列即可，可以把所有两个词的组合的可能情况存下来。比如整个文本长为1000个词，那么两个词在一起的所有可能数就是1000 * 999 ≈ 100,0000，我就将这么多组合 每个组合出现的概率保存下来即可

**代码实现**

获取所有的二元文本序列：

```python
# 把原文本 一个去掉开头 一个去掉结尾 zip在一起，就得到所有出现过的二元组
bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]
# 将这些二元组放入刚刚文本预处理中定义的 Vocab 类
bigram_vocab = Vocab(bigram_tokens)
```

获取所有的三元文本序列：

```python
trigram_tokens = [triple for triple in zip(corpus[:-2], corpus[1:-1], corpus[2:])]
grigram_vocab = Vocab(trigram_tokens)
```

### 6.3.2 数据采样（拆分序列）

如果给定的原始文本序列太长而不能被模型一次性全部处理时，或者说序列太长的话在处理过程中就会积累太多前面的无用信息，因此需要将一个完整的文本序列拆分成多个子序列。

随机生成小批量需要两个参数：

- batch_size批量大小：指定每个小批量中 子序列样本的数目，也就是一个批量中有几个子序列
- num_steps时间步数：每个子序列中的时间步数，也就是每个子序列中包含几个单词



## 6.4 RNN

### 6.4.1 模型

**在语言模型上的应用实例**

![RNN实例](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\RNN实例.png)

1. 起始输入 x0=你 ，通过起始隐变量模型h，得到新的隐变量模型 h1 ，h1 预测出下一个字应该是 `好`
2. 根据本次输入的 x1=好，再加上之前的隐变量模型h1，得到新的模型h2，h2预测出下一个字应该是` ,`
3. 根据本次输入的x2=， 再加上之前的隐变量模型h2，得到新模型h3，h3预测出下一个字应该是 世

也就是输出ot的时候，是根据 更新的隐变量模型ht-1 和 上一个字xt-1 预测并输出的，模型在预测是并不会看到xt是什么。

预测完成之后再拿预测出来的字ot和真实的字xt来计算损失，并更新模型

**RNN模型**

![RNN模型](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\RNN模型.png)

根据更新状态的公式可以看到，当前隐藏状态ht由三部分决定

- 上一个时刻的隐变量模型 （这一项是RNN的关键，如果去掉这一项就退化为了一个MLP）

  Whh表示 h-->h 的权重

- 上一个输入xt-1

  Whx表示 x-->h 的权重

- 偏移bh

- 最前面的fai是激活函数

最后的输出应该是没有激活函数的，直接就是Woh*ht + b0，也就是h-->o的权重+偏移

**梯度裁剪**

对于RNN来说，更容易发生梯度爆炸和梯度消失，因此要应用梯度裁剪，即将每一次计算的梯度固定在某个范围中

**代码实现**

- 将token的索引转化为one-hot编码，转化之后的维度从1变为词表大小

  我们在将文本采样为小批量数据之后，得到的形状是一个二维张量：（批量大小，时间步数），ont_hot操作能将这样一个批量数据转换成三维张量（批量大小，时间步数，词表大小）

  为了方便，我们经常转换输入的维度为（时间步数，批量大小，词表大小）

  ```python
  X = torch.arange(10).reshape((2, 5)) # 模拟一个批量大小为2，时间步数为5的数据
  F.one_hot(X.T, len(vocab)).shape # 将其进行one-hot编码
  # 假设词表大小为28，则最后得到的输入维度为（5，2，28）
  ```

- 初始化模型参数

  模型有三个参数：输入维度Wxh，输出维度Who，和隐藏单元数Whh（也就是隐变量的维度）

  其中输入和输出来自相同的词表，因此它们的维度都是词表的大小（因为已经经过one-hot编码，会将一个词对应的索引转化为one-hot）；而隐藏单元数是一个超参数

  ```python
  def init_params(vocab_size, num_hiddens, device):
      num_inputs = num_outputs = vocab_size # 输入输出维度都是词表大小
      
      # 用于随机创建一个给定维度的矩阵，用于初始化参数
      def normal(shape):
          return torch.randn(size=shape, device=device * 0.01) 
      
      # 初始化隐藏层参数
      W_xh = normal((num_inputs, num_hiddens)) # 输入层到隐藏层
      W_hh = normal((num_hiddens, num_hiddens)) # 隐藏层到隐藏层
      b_h = torch.zeros(num_hiddens, device=device)
      
      # 初始化输出层参数
      W_hq = normal((num_hiddens, num_outputs))
      b_q = torch.zeros(num_outputs, device=device)
      
      # 给所有参数附加梯度
      params = [W_xh, W_hh, b_h, W_hq, b_q]
      for param in prams:
          param.requires_grad_(True)
      return params
  ```

- 初始化隐状态

  隐状态的形状保持不变，一直都是（批量大小，隐藏单元数）

  ```python
  torch.zeros(batch_size, num_hiddens)
  ```

- 计算一个时间步内的隐藏状态和输出

```python
# 计算一个时间步内的隐藏状态和输出
for X in inputs: # inputs就是从x0~xt
    H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h) # 计算新的隐藏状态
    Y = torch.mm(H, W_hq) + b_q # 通过隐藏状态计算当前输出
```

- 使用模型来做预测

```python
# prefix-给定的文本  num_preds-要往后预测多少单词 net-预测使用的网络 vocab-词汇表（单词和序号的映射）
def predict(prefix, num_preds, net, vocab, device):
    state = net.begin_state() # 初始化隐藏状态设置为网络的初始状态
    outputs = [vocab[prefix[0]]]
    # 取出预测的最后一个字符作为下一次的输出
    get_input = lambda: torch.tensor([outputs[-1]], device=device, (1, 1)) 
    # 先根据给定的prefix不断更新隐藏状态
    for y in prefix[1:]:
        _, state = net(get_input(), state)
        outputs.append(vocab[y])
    # 开始向后预测num_preds个词
    for _ in range(num_preds):
        y, state = net(get_input(), state)
        outputs.append(int(y.argmax(dim=1),reshape(1)))
    return ''.join([vocab.idx_to_token[i]] for i in outputs)
```



### 6.4.2 衡量语言模型的标准 困惑度perplexity

困惑度就是平均交叉熵取指数

**平均交叉熵**

预测下一个词是什么实际上也可以看作是一个分类问题，即下一个词有m种可能，输出可能性最高的那一个

分类问题就可以用交叉熵来衡量损失，即预测的结果中 正确的分类 的概率。对于整个模型来说，就是平均交叉熵，即每次预测的交叉熵取平均

![平均交叉熵](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\平均交叉熵.png)

p即为给定前t-1的词的情况下，预测为正确词xt的概率

**困惑度**

由于历史原因，NLP使用困惑度exp(Π)来衡量，就是给平均交叉熵取指数

实际意义就是平均每次预测中的可能选项

- 1就是确信只有这一种可能（最好的情况）
- 2就是本次预测觉得有两个词都比较有可能是下一个词
- n就是本次预测觉得有n个词都比较有可能是下一个词
- 无穷大就是完全没预测到，感觉所有词都有可能（最差情况）

### 6.4.3 代码实现

**RNN层**

1. *创建RNN层*

pytorch中提供了RNN层的实现，需要传入输入输出参数的维度（等于词表的大小）和隐藏层数

```python
batch_size, num_steps = 32, 35
num_hiddens = 256
rnn_layer = nn.RNN(len(vocab), num_hiddens)
```

2. *使用RNN层*

通过一个隐状态和一个输入，就可以使用rnn_layer得到一个新的输出和新的隐状态

- 隐状态的形状：（隐藏层数，批量大小，隐藏单元数），对于现在来说隐藏层数只有一层，后面到深层RNN可以设置多个隐藏层数

- 输入的形状：（时间步数，批量大小，词表大小（one-hot编码））

- 输出的形状：state_new 和 state的形状一样

  这个Y，并不是最后的输出output，它的形状是（时间步数，批量大小，隐藏单元数）实际上是一个中间结果，因此RNN层是没有输出层的，需要自己单独加一个输出层

下面的代码只关注输入输出形状，数据都是随机生成的

```python
state = torch.zeros((1, batch_size, num_hiddens) # 初始化一个初始隐状态
X = torch.rand(size=(num_steps, batch_size, len(vocab))) # 输入维度，one-hot编码过后的
Y, state_new = rnn_layer(X, state)
```

**完整模型**

库中的rnn_layer只包含隐藏的循环层，还需要自己创建一个单独的输出层

output输出层：

1. 第一步是将 Y 从 （时间步数，批量大小，隐藏单元数）压缩为（时间步数*批量大小，隐藏单元数）

2. 第二步是用全连接层计算出最后的结果，形状为（时间步数*批量大小，词表大小）

```python
class RNNModel(nn.Model):
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        # 全连接层，用于输出最后的结果
        self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        
    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size) # 将输入进行one-hot编码
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        # 自己写全连接层计算输出
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state
    
    # 用于初始化隐藏层state
    # 隐藏层形状为（批量大小，隐藏）
    def begin_state(self, device, batch_size=1):
        # 由于目前都是单向单层RNN，因此第一个参数值为1
        return torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device)
    
rnn_layer = rnn.RNN(len(vocab), num_hiddens)
net = RNNModel(rnn_layer, vocal_size=len(vocab))
# 之后就可以用这个网络进行训练或预测

```

## 6.5 GRU 门控循环单元

在一个文本序列中，并不是所有的单词和句子都同等重要。

我们应该特别关注更重要的，遗忘不那么重要的，门控提出了相关机制

- 能关注的机制（更新门）
- 能遗忘的机制（重置门）

**代码实现**

依然是提供了GRU层的实现，需要自己添加一个输出层

```python
num_inputs = vocab_size
gru_layer = nn.GRU(num_inputs, num_hiddens)
net = RNNModel(gru_layer, len(vocab))
```

## 6.6 LSTM 长短期记忆网络

思想也是决定要不要注意或者忘掉某些信息

引入三个门

- 忘记门F：将值朝0减少
- 输入门I：决定是不是忽略掉输入数据
- 输出门O：决定是不是使用隐状态

**代码实现**

```python
lasm_layer = nn.LSTM(num_inputs, num_hiddens)
net = RNNModel(lstm_layer, len(vocab))
```

长短期记忆网络的隐状态需要返回要给额外的记忆元，因此初始化需要更改

```python
def init_lstm_state(batch_size, num_hiddens):
    return (torch.zeros((batch_size, num_hiddens), 
            torch.zeros(batch_size, num_hiddens))
```

## 6.7 深层RNN

![深层RNN](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\深层RNN.png)

就是多加了几个隐藏层，来获得更多的非线性性

现在每个隐藏层同时和左边和下边的隐藏层有关

**代码实现**

用的层依然是之前的层，不过在num_layers中需要指定RNN的层数

```python
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers)
net = RNNModel(lstm_layer, len(vocab))
```

## 6.8 双向循环网络

之前RNN只看过去的词语（也就是上文），双向RNN可以同时根据上下文来预测当前的词语

![双向RNN](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\双向RNN.png)

执行流程：

以O1的生成为例

1. X1作为输入得到H1正向
2. H1正向向右传播......
3. X2作为输入得到H2逆向
4. H2逆向和X1作为输入得到H1逆向
5. H1逆向和H1正向共同作用预测出O1

由于文本生成时看不到之后的词语，因此无法用于文本生成，多用于句子的特征提取（比如文本翻译）

**代码实现**

只需要在创建RNN层时将参数bidirectional置为true

```python
lstm_layer = nn.LSTM(num_inputs, num_hiddens, num_layers, bidirectional=True)
```

如果使用了双向循环网络，要注意最后添加的输出层的输入维度要翻倍

```python
self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)
```

## 6.9 编码器-解码器架构

Encoder-Decoder架构

![Encoder-Decoder](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\Encoder-Decoder.png)

思想是将一个模型分为两块：

- 编码器处理输入

  将input输入处理为一个中间状态（更利于机器学习的中间状态）

- 解码器处理输出

  将中间状态解码成输出

编码器解码器架构算是一种思想，根据Encoder和Decoder的选择不同有很多具体实现，不过大多数选择都是RNN和其变种GRU/LSTM

其中Encoder不需要输出层，只需要将RNN的最后一层state作为输入给到Decoder即可

**用编码器-解码器架构理解CNN**

![Encoder-Decoder-CNN](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\Encoder-Decoder-CNN.png)

- 编码器：将输入的图片变成中间表达形式（即特征提取）
- 解码器：根据提取的特征 解码出想要的结果

**用编码器-解码器架构理解RNN**

![Encoder-Decoder-RNN](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\Encoder-Decoder-RNN.png)

- 编码器：将文本表示成向量
- 解码器：将向量表示成输出

## 6.10 seq2seq

seq2sql指的是一类问题，即输入一个序列，要求输出一个序列。比如机器翻译、语音识别、机器人对话等等

用于解决seq2seq问题的算法基本都属于encoder-decoder架构

编码器和解码器可以任意选择，大多数为RNN及其变种（LSTM、GRU等）

## 6.11 注意力机制

### 6.11.1 心理学的注意力机制

当选择注意点时，有随意和不随意两种

- 随意：以下称为刻意，即有目的 的去关注某样东西
- 不随意：以下称为无意，即不带有明显目的，只是一眼会注意到更显眼的东西

在之前的卷积、全连接、池化层中，都只考虑无意的线索。比如最大池化层，是无意识地拿到矩阵中的最大值。而卷积层会盲目的抽取所有特征。

而注意力机制则会显式的考虑随意线索，即有意识的注意更重要的特征，其中

- 随意线索被称之为查询（query）
- 不随意线索：可以理解为已有的特征
- 值：对应于不随意线索的数据值

注意力机制就是给定一个查询（随意线索），从一堆已有的数据（不随意线索）中找到和查询最相似的那个，并利用对应的值

### 6.11.2 非参注意力池化层

注意力机制实际上在上个世纪的统计学中就用到了，统计学是拿一个公式计算特定的值，所以没有可学习的参数

比如给定一个区域 (xi, yi) i=1,...,n

用不随意的平均池化提取特征就是 直接求yi的平均值，完全无意识无目标

而下面的核回归就是一种有意识的做法

**Nadaraya-Watson核回归**

<img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\Nadaraya-Watson核回归.png" alt="Nadaraya-Watson核回归" style="zoom:50%;" />

对于给定的查询 x，我会计算区域中每个特征（xi）和给定点x的距离（通过K函数进行计算），最终根据每个特征的相对重要性来对 它们对应的值 yi 进行加权求和得到最终的结果

通俗来说就是给定一个新数据，我们倾向通过找和它相似的数据来进行预测

**使用高斯核**

如果K函数使用高斯核

<img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\高斯核.png" alt="高斯核" style="zoom:50%;" />

代入到核回归的公式中就变为

<img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\高斯核代入核回归.png" alt="高斯核代入核回归" style="zoom:50%;" />

最后就相当于，给定一个值x，我会将其和已有的值x1~xn计算距离，并将这些距离进行softmax处理得到对应的概率（可以理解为相似的概率），最终根据这些概率给xi特征 对应的值yi 进行加权

### 6.11.3 有参的注意力池化层

<img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\参数化的注意力机制.png" alt="参数化的注意力机制" style="zoom:50%;" />

就是在之前的公式中加入一个可以学习的参数w

### 6.11.4 注意力分数

注意力分数是 key 和query的相似度，注意力权重是分数的softmax的结果

<img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\注意力分数.png" alt="注意力分数" style="zoom:50%;" />

**计算注意力分数**

给定两个向量k(key)和q(query)，如何计算他们两个的相似性（即计算注意力权重α(k, q)）：

对于两个输入，要先分别乘上参数Wq和Wk得到q和k，然后再对q和k做计算得到注意力分数

- Additive Attention 适用于key 和 query 长度不同

  将query和key合并（相加）起来进入一个单输出隐藏层的MLP

  <img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\AdditiveAttention.png" alt="AdditiveAttention" style="zoom:50%;" />

- Dot-Product Attention key 和 query长度要相同，因为需要做dot-product内积运算

  直接将query和key做内积

  <img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\Dot-ProductAttention.png" alt="Dot-ProductAttention" style="zoom:50%;" />

  

## 6.12 self-attention自注意机制

对于每一个输出，都要考虑所有的输入才能得到，这就是自注意机制（CNN可以看作是Self-attention的特例）

比如对于一个输入和输出维度相同的例子

![self-attention example](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\self-attention example.png)

​                                  

对于每一个输出b，都要在考虑所有输入a1，a2，a3，a4的基础上得到

### 6.12.1 单个输出计算过程

下面以b1的计算过程为例：

1. b1应该由a1得出，所以a1作为query，其他输入作为key（a1同时也作为key和自己计算注意力分数）

   a1乘参数Wq得到q，a1a2a3a4分别乘Wk得到k1 k2 k3 k4

2. q分别和k1 k2 k3 k4计算注意力分数，之后对计算出的注意力分数做softmax，得到每个key对于query的权重α'

   ![self-attention calculate](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\self-attention calculate.png)

3. 根据 α‘ 从给定输入sequence中抽取重要的咨询。也就是根据每个输入的权重（也就是和query的重要程度）决定每个输入对最后输出的重要程度

   ![self-attention calculate2](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\self-attention calculate2.png)

   给每个输入乘上Wv得到对应的v向量，再让v向量和对应的α’相乘，最后相加得到b1

### 6.12.2 整体计算过程

上述计算单个输出的过程对于每个输入a都要进行一次，因此可以将所有输入拼成一个矩阵来一次性完成计算输出的操作

1. 将所有输入向量拼成一个矩阵，分别和参数矩阵Wq、Wk、Wv相乘，得到对应的QKV矩阵

   <img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\self-attention-total1.png" alt="self-attention-total1" style="zoom: 80%;" />

2. 第二步中每个输入对应的query都要去其余输入向量的key进行dot-product内积运算得到对应的注意力分数α

   ![self-attention-total2](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\self-attention-total2.png)

3. 将每个输入对应的α和v矩阵相乘相加得到最终的输出b

   ![self-attention-total3](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\self-attention-total3.png)

**最终所有矩阵运算的过程**

![self-attention-totaltotal](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\self-attention-totaltotal.png)

计算过程中未知的、需要学习的参数为Wq、Wk、Wv

### 6.12.3 Multi-head Self-attention

多头注意力，用于在不同输入之间针对于不同的方面（种类）分别计算相关性 Different types of relevance

**以2 heads为例**

将qi、ki、vi分别进一步计算，得到qi1和qi2（代表两个不同方面的特征），之后qi1和ki1、ki2等分别计算得到相关性

![multi-head self-attention](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\multi-head self-attention.png)

分别计算出bi1和bi2之后，再将这两个结果拼接，再乘上W矩阵得到bi

### 6.12.4 Positional Encoding 考虑位置信息

可以看到在selfattention的计算过程当中，并没有考虑每个输入的位置信息（实际上计算b时不需要按照顺序依次计算b1、b2......，而是可以同时计算）

Positional Encoding的思路就是，对于每个输入ai，再额外加上一个代表位置信息的矩阵ei

<img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\positional encoding.png" alt="positional encoding" style="zoom: 80%;" />

## 6.13 Transformer

Transformer是一个seq2sq的模型

### 6.13.1 Encoder

![TransformerEncoderBlock](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\TransformerEncoderBlock.png)

1. 对于输入向量b，先进行self-attention计算得到输出a
2. 输出a和原输入向量b进行residual运算（将二者相加）
3. 得到的结果再进行Layer Norm运算
4. 得到的结果作为全连接层（FC Fully Connected）的输入，全连接层的输出和输入再相加，再做residual，再做norm运算
5. 一个block块到此结束，得到最终输出

整体结构如下图所示

![TransformerEncoderBlock1](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\TransformerEncoderBlock1.png)

在Encoder中，这样的block会重复多次

Transformer的block有多种改进，这也许是缝模块的方向，替换不同的TransformerBlock

### 6.13.2 AT Decoder

AT Decoder的输入输出形式有点类似于RNN，即每次根据当前输入和之前的输出来决定当前的输出

<img src="D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\AT Decoder.png" alt="AT Decoder" style="zoom: 50%;" />

每次给Decoder一个输入（起始为Begin，之后的输入为上次的输出），Decoder根据当前输入和Encoder的整个输出来预测下一个输出

由于Decoder的输入输出形式，即每次输出只能看到前面的信息，无法看到全部的信息。这种只看前面信息的叫做**Self-attention(Mask)**

### 6.13.3 Encoder-Decoder连接

Decoder中接收来自Encoder输入（也可以说是将Encoder和Decoder连接在一起）的小模块叫做Cross attention

![Cross attention](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\Cross attention.png)

Decoder完整执行过程：

1. 给定Decoder起始输入BEGIN，经过Self-attention(Mask)，再乘Wq矩阵进行Transformer变换，得到q
2. Encoder的输出进行transformer变换得到对应的k、v矩阵
3. Decoder的q矩阵和Encoder输出得到的k矩阵 计算注意力分数，再根据分数和v矩阵得到最终的输出v
4. v经过Decoder下面的结构得到Decoder的输出（也就是Transformer的最终输出）
5. 这个输出再作为Decoder的下一个输入，继续往下预测

Transformer的整体架构如下

![TransformerStructure](D:\CS_Source\myNote-of-ComputerStudy\机器学习\3\TransformerStructure.png)
